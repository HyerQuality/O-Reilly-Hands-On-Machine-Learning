{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb50468-b0ac-4f7e-9fad-72f0af5fda20",
   "metadata": {},
   "source": [
    "<b> Chapter 4 is a technical chapter which will go over the mathematics behind some of the machine learning models that have been used so far.  I will be focusing on the linear algebra definitions and applications as they more succinctly embody the theories and calculations </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21427fd-a99e-40fa-b2ba-64cf906ee565",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7d0da-7a23-4517-9be1-fd0f0c60f613",
   "metadata": {},
   "source": [
    "<b> Simply put linear regression is a weighted average of features and a scalar bias.  The weights are bias are chosen based off the minimization of a cost function.  The most common cost function for linear regressions is Mean Squared Error (MSE) which is rooted (RMSE) when comparing across multiple models.  The reason MSE is used is because the derivative is much simpler to compute and requires significantly less computational power than the rooted version.  Since rooting the function simply scales it, selecting MSE to minimize has the same end result as selected RMSE to minimize.  This is true in general for any cost or reward function and it is encouraged to use the form of the cost function which minimizes computational effort. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ac62a-356f-4853-b450-9ad60e81d47d",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "\n",
    "#### ${\\hat{\\theta}}$ = $(\\mathbf{X^T}\\mathbf{X})^{-1} \\mathbf{X^T} \\mathbf{y}$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "${\\hat{\\theta}}$ is the value of $\\theta$ that minimizes the cost function\n",
    "<br>\n",
    "$\\mathbf{y}$ is the vector of target values\n",
    "<br>\n",
    "$\\mathbf{X}$ is a matrix of features with a bias column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82d41c3-93a2-48f0-a45c-8ab7fedca06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.11885904],\n",
       "       [2.89835696]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Generate some data to validate the above expression'\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # mx + b : 3x + 4 + noise\n",
    "\n",
    "' Introduce the constant bias'\n",
    "X_b = np.c_[np.ones((100, 1)), X] # m x n matrix with column of 1's and column of X values\n",
    "\n",
    "' Apply the normal equation'\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b5e8a-2fa3-4ac2-a43a-90d5437aba31",
   "metadata": {},
   "source": [
    "<b> The bias chosen was 4 and the weight was 3.  The gaussian noise introduced makes it impossible to get these exact values but the derived values are almost perfect.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e937627-9114-44dc-bc48-36dce03a5618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.11885904],\n",
       "       [9.91557295]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Make some predictions'\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2064678e-404e-46a9-801e-2fa2f0d5c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25d36439bb0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "' Plot the raw data and the best fit predictions'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta_best), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ccf1d9-8629-46c2-bfc0-f8c58a01a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  [4.11885904] Coefficient:  [[2.89835696]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.11885904],\n",
       "       [9.91557295]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Performing linear regression with sklearn is very simple'\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print('Bias: ', lin_reg.intercept_, 'Coefficient: ', lin_reg.coef_)\n",
    "\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c25a93-aaa3-431c-9225-86f88a855c8e",
   "metadata": {},
   "source": [
    "<b> A caveat for inverted matrices is that they require a non-zero determinant.  This isn't always the case.  A work-around is the Moore-Penrose Inverse aka the pseudoinverse.  This utilizes Singular Value Decomposition to generate an equivalent matrix which is always invertible.  This method is also more computationally efficient. Sklearn ueses this method natively </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7cfda-4dbb-4ef9-950f-5a0cf8a26f10",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4bddd-c559-4994-b25c-34e4a5442fda",
   "metadata": {},
   "source": [
    "<b> Gradient Descent outperforms sklearn and the Normal Equation when the dataset is extremely large (100,000+ features).  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab888f9-f3d6-4d59-b2fb-3be5f936d97f",
   "metadata": {},
   "source": [
    "<b> General notes about gradient descent </b>\n",
    "* Tweaks parameters iteratively in order to minimize a cost function\n",
    "* Not all cost functions are convex.  As such, one hurdle for gradient descent is overcoming local minima and plateaus\n",
    "* The learning rate is the primary parameter for correctly finding the absolute minima.  Too high and the optimizer will diverge and never find a minima. Too low and it will stop too soon at a local minima\n",
    "* Because a convex function has exactly 1 minima they are ideal. If possible, transform the cost function such that its transformation is convex.\n",
    "* Scaling all features has an enormous impact on how long it takes for the optimizer to find the mininma. \n",
    "* Gradient descent utilizes the properties of partial derivatives.  As such, a less complex derivative is arguably more important than a convex, continuous function.  Both is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92247245-a5f5-4f49-875b-5b38ccfb5a9f",
   "metadata": {},
   "source": [
    "## Gradient Vector of the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1e88d-22af-425a-a0ac-fb0cce053898",
   "metadata": {},
   "source": [
    "#### $\\nabla_{\\boldsymbol\\theta}MSE(\\boldsymbol\\theta) = \\frac{2}{m}\\mathbf{X}^{T}(\\mathbf{X}\\boldsymbol\\theta - \\mathbf{y})$\n",
    "\n",
    "<br>\n",
    "\n",
    "<b> Notice that this formula computes over the full training set X at each step.  The batch size here is 1 and as a result the training time on large datasets will be greatly impacted.  There are other ways to do this involving larger batch sizes which will be discussed later<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea2681-4f81-475d-9831-59f1d06babe9",
   "metadata": {},
   "source": [
    "## Gradient Descent Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519d015-40e4-47e5-a810-65afd9b4c19b",
   "metadata": {},
   "source": [
    "#### $\\boldsymbol\\theta^{(next step)} = \\boldsymbol\\theta - \\eta\\nabla_{\\boldsymbol\\theta}MSE(\\boldsymbol\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a39c430-fa06-4cc3-8f06-0573df641bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.11885904]\n",
      " [2.89835696]] \n",
      " \n",
      " This is exactly what the Normal Equation yielded\n"
     ]
    }
   ],
   "source": [
    "' Validate the above expressions'\n",
    "eta = 0.1  #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta*gradients\n",
    "    \n",
    "print (theta, '\\n', '\\n', 'This is exactly what the Normal Equation yielded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4447c-84dd-4cbd-83c6-a34ff63d9d59",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bb400-25a4-4cba-99a7-a067fae5e936",
   "metadata": {},
   "source": [
    "<b> Opposite to batch gradient descent which uses the entire training set at each epoch, stochastic gradient descent takes one random sample from the training set at each epoch and applies the gradient descent step.  This requires vastly less memory and as such is a better choice for very large training sets.  However, there are some nuances to the process.  Due to the randomness the descent curve will not be smooth and the optimal solution will never be settled upon.  This can help escape local minima but comes at the cost of a less perfect output.  One solution to converging on the optimal output using stochastic gradient descent is to implement a learning schedule in which the learning rate is gradually reduced such that the random jumps at the point of convergence are so small that the output is essentially the same as batch gradient descent. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5c59c9-2dad-476f-a049-3d8fa0d4e28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.06514835],\n",
       "       [2.92730976]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' An SGD example with a simple learning schedule'\n",
    "\n",
    "# Epochs\n",
    "n_epochs = 50\n",
    "\n",
    "# Learning rate schedule\n",
    "t0, t1 = 5, 50 \n",
    "\n",
    "def learning_rate_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# Random initialization\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# SGD\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m = 100 was declared in an earlier code block\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index: random_index + 1]\n",
    "        yi = y[random_index: random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_rate_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8638911-ccd9-4942-9aea-deb7314e5f60",
   "metadata": {},
   "source": [
    "<b> When using SGD it is best practice to shuffle the training set </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ea5f8d-8f0b-41bf-a882-12450b1e2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.14405085] [2.96046418]\n"
     ]
    }
   ],
   "source": [
    "' Implementing SGD with Sklearn'\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel()) # .ravel() is a numpy function which flattens an array into a single 1D array with all the input-array elements and with the same type as it\n",
    "\n",
    "print(sgd_reg.intercept_, sgd_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c23ba8-eeeb-474f-a2cb-8c4832c54345",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919c95-85d0-4888-bc6d-b7387e5cc1b0",
   "metadata": {},
   "source": [
    "<b> Mini-batch gradient desecent is essentially the middle ground between batch and sgd.  It computes gradients on small, random sets of instances called mini-batches. One advantage of SGD is you can get performance boosts from hardward optimzation when using matrix operations and GPUs.  It will likely end up closer to the minimum than SGD as well, although it may be harder to escape from local minima.  For large datasets this is usually the best algorithm. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a376fd0-51d5-418d-9f1a-e13a5fe4fb76",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23315727-d3e7-4bbc-a012-33effb35573a",
   "metadata": {},
   "source": [
    "<b> Suprisingly, you can use linear regression algorithms to model non-linear relationships.  For example, consider a simple parabola.  You can simply augment the square of the feature(s) as additional features into the dataset and viola, you can use linear regression to estimate non-linear relationships <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a046aa-cfd8-4728-9cc9-d29edf3229cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.92312747]), array([-1.92312747,  3.69841925]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' An example of estimating a polynomial function with linear regression'\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "X[0], X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba72094-9608-4d63-84d2-036d40d4d4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.97744395]), array([[0.94651612, 0.51595379]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "\n",
    "# Here we see an intercept close to 2, a second-degree coeffecient close to 0.5, and a first-degree coeffecient close to 1 as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a152b10-b9b8-496c-939d-f4554e82ffb5",
   "metadata": {},
   "source": [
    "<b> Note that the PolynomialFeatures(degree=d) class will transform an array containing n-features into an array containing $\\large\\frac{(n + d)!}{d!n!}$ features.  Be cautious of the combinatorix nature of this augmentation <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f610155-d876-4187-aa31-bb3364d73366",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b8612-0ce0-498b-a5f6-ce7ac2ea6805",
   "metadata": {},
   "source": [
    "<b> If, during cross-valdiation, a model performs well on training data but poor on valdation data it is likley overfitting.  If it performs poorly on both it is likely underfitting. This is one method to test for fitness.  Another is to analyze the learning curve.  Learning curves are plots of the model's performance on the training set and valdiation set as a function of the training set sieze (or the training iteration). <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815cd474-b0c6-43a1-8379-7a0b2e987740",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Observe an underfitting learning curve set'\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    for m in range(1, X_train.shape[0]):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    \n",
    "    plt.axis([0, X_train.shape[0], 0, 3])\n",
    "    plt.plot(np.sqrt(train_errors), 'r-+', linewidth=2, label='train')\n",
    "    plt.plot(np.sqrt(val_errors), 'b-', linewidth=3, label='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2d0e8f-56cb-472c-8670-47ea451a018e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fa3c35-6f15-460f-a704-f48eee05ceca",
   "metadata": {},
   "source": [
    "<b> These learning curves are typical of a model that is underfitting.  Both curves have reached a plateau; they are close and fairly high.  If a model is underfitting the training data, adding more training examples will not help.  You need to use a more complex model or come up with better features </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae9fb41-3366-43cb-855e-9d03cba65555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "' Observe a model that is overfitting'\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    ('lin_reg', LinearRegression()),\n",
    "])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b36ed-ec4a-4642-94c0-45f34e7c6f0b",
   "metadata": {},
   "source": [
    "<b> The error here is lower than the underfit model and it performs much better on the training data.  This example doesn't show well but often you'll see a noticeable gap between the training and validation curve.  These are all characteristic of an overfit model.  One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98957443-685d-4252-807e-de349429e373",
   "metadata": {},
   "source": [
    "### The Bias/Variance Trade-off\n",
    "\n",
    "<b> An important theoretical result of statistics and Machine Learning is the fact that a model's generalization error can be expressed as the sum of three very different errors:</b>\n",
    "    <br>\n",
    "<b><i> Bias <i><b>:\n",
    "    <br>\n",
    "> A high-bias model is most likely to underfit the training data.  This is due to wrong assumptions, like fitting a straight line to a curve.\n",
    "    \n",
    "<b><i> Variance <b><i>:\n",
    "> A model with many degrees of freedom is likely to have high variance and thus overfit the training data.  This is caused by extra sensitivity to small variations in the training data.\n",
    "    <br>\n",
    "    \n",
    "<b><i> Irreducible error <b><i>:\n",
    "> The only way to reduce this part of the error is to clean up the data.  Fix data sources, such as broken sensors, or detect and remove outliers. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ca3eb-4f79-4cd5-9c03-da0927fc4b56",
   "metadata": {},
   "source": [
    "## Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a22d93-b671-4532-aa8c-18ec9f878f49",
   "metadata": {},
   "source": [
    "<b> A good way to reduce overfitting is to regularize the model (i.e. to constrain it).  A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.  For a linear model, regularization is typically achieved by constraining the weights of the model.  Three common methods of regularization are <i> Ridge Regression, Lasso Regression, and Elastic Net <i><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1259c-5b50-4966-ada0-0508321cfbf4",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae600f-9c03-44bb-96b2-3f1231dd69c9",
   "metadata": {},
   "source": [
    "<b> Ridge regression, also known as Tikhonov regularization, is a regularized version of Linear Regression.  A regularization term equal to $\\alpha\\sum_{i=1}^{n} \\theta{\\scriptstyle i}^{2}$ is added to the cost function.  This forces the learning algorithm to keep the model weights as small as possible.  The regularization term should only be added to the cost function during training.  Once the model is trained use the unregularized performance measure to evaluate the model's performance. \n",
    "<br><br>\n",
    "It is quite common for the cost function used during training to be different from the performance measure used for testing. A good training cost function should have optimization-friendly derivatives, while the performance measure used for testing should be as close as possible to the final objective.\n",
    "<br><br>\n",
    "If $\\alpha = 0$ then the Ridge Regression is just Linear Regression.  If $\\alpha$ is very large then all weights end up close to zero and the result is a flat line through the data's mean. The full form for the Ridge Regression cost function is:\n",
    "<br><br>\n",
    "$J(\\boldsymbol\\theta) = MSE(\\boldsymbol\\theta) + \\alpha\\sum_{i=1}^{n} \\theta{\\scriptstyle i}^{2}$\n",
    "<br><br>\n",
    "Note that the bias term $\\theta{\\scriptscriptstyle 0}$ is not regularized.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8919bf-ad8a-4ff9-93b8-b5f949cc0bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.7752796]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Use Ridge Regression with scikit-learn using the closed-form formula.'\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver='cholesky')\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n",
    "\n",
    "# Alternatively you can set the solver hyperparameter to 'sag' to run the Ridge model with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae2f29d-34c9-4fe5-90b4-7df5c2237ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.76861553])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Use Ridge Regression with scikit-learn using gradient descent.'\n",
    "sgd_reg = SGDRegressor(penalty='l2')\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "\n",
    "# The penalty hyperparameter sets the type of regularization term to use.  l2 corresponds to ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca155b75-e0c2-48a9-bc30-71f88b61716e",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a833b-6161-41d3-947d-87e126838b58",
   "metadata": {},
   "source": [
    "<b> Least Absolute Shrinkage and Selection Operator Regression is another regularized version of Linear Regression.  It adds a regularization term to the cost function, but it uses the L1 norm instead of half the square of the L2 norm. The cost function is as follows:\n",
    "<br><br>\n",
    "$J(\\boldsymbol\\theta) = MSE(\\boldsymbol\\theta) + \\alpha\\sum_{i=1}^{n} |\\theta{\\scriptstyle i}|$\n",
    "<br><br>\n",
    "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features.  In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (few non-zero weights).  Compared to Ridge Regression there are two main differences with Lasso.  First, the gradients get smaller as the parameters approach the global optimum, so Gradient Descent naturally slows down which helps convergence.  Second, the optimal parameters get closer and closer to 0 when you increase $\\alpha$, but they never get eliminated entirely.\n",
    "<br><br>\n",
    "The Lasso cost function is not differentiable at $\\theta{\\scriptstyle i}$ = 0 but gradient descent works fine if you use piece-wise the cost function and set the cost function to 0 when $\\theta{\\scriptstyle i}$ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6a4874-9cd1-4a96-90e7-904ede4b9f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.73016106])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Example of Lasso regression'\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "\n",
    "# This is equivalent to setting the SGDRegressor(penality='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ec70bb-366b-4705-8cce-56fc2ec16c06",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570e1d4-82ba-4c7a-aa38-14d10f327622",
   "metadata": {},
   "source": [
    "<b> Elastic Net is a middle ground between Ridge Regression and Lasso Regression.  The regularization term is a simple mix of both Ridge and Lasso's regularization terms, and you can control the mix ratio <i> r </i>.  When r = 0 Elastic Net is equivalent to Ridge Regression and when r = 1 it is equivalent to Lasso Regression.  The cost function is as follows:\n",
    "<br><br>\n",
    "$J(\\boldsymbol\\theta) = MSE(\\boldsymbol\\theta) + r\\alpha\\sum_{i=1}^{n} |\\theta{\\scriptstyle i}| + \\frac{1 - r}{2}\\alpha\\sum_{i=1}^{n} \\theta{\\scriptstyle i}^{2}$\n",
    "<br><br>\n",
    "When should you use which regularization technique?  It is almost always preferable to have at least a little bit of regularization.  Ridge is a good default but if you suspect only a few features are useful you should prefer Lasso or Elastic Net because they tend to reduce the useless features' weights down to zero. In general, Elastic Net is preferred over Lasso because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874e6a08-17fd-4ada-bb98-537b2f50d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.73463765])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Example of Elastic Net'\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  #l1_ratio corresponds to the mix ratio r\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2f076-f955-49cc-892e-aa4fa3e88cd0",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bc3bb-2c32-4fe3-ac95-d388453c755a",
   "metadata": {},
   "source": [
    "<b>An elegant and simple regularization technique is early stopping.  This applies to iterative models like Gradient Descent.  By observing the loss curves of both the training set and validation set and ending the training once the validation loss begins to increase you can simply optimize your model to avoid over-fitting.  For more sporadic loss curves you can run until the validation loss is certainly increasing then roll back the model parameters to the point where the validation error was at a minimum <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86f8882-d742-46dd-9d52-2463158f2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Basic implementation of early stopping'\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# prepare the data\n",
    "poly_scaler = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(\n",
    "    max_iter=1,\n",
    "    tol= -np.inf,\n",
    "    warm_start=True, # When the fit() method is called it continues training where it left off vs. restarting from scratch\n",
    "    penalty=None,\n",
    "    learning_rate='constant',\n",
    "    eta0=0.0005\n",
    ")\n",
    "\n",
    "minimum_val_error = float('inf')\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train.ravel())\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29c4bfa3-e324-498d-ad6d-6663ff007431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06055222, 0.05300337, 0.05015628, 0.04847403, 0.04156685,\n",
       "        0.04283305, 0.03566116, 0.03818836, 0.03158398, 0.03469195,\n",
       "        0.02874959, 0.03211082, 0.02675661, 0.03020251, 0.02533499,\n",
       "        0.02878509, 0.02430638, 0.02773159, 0.02355336, 0.02695372,\n",
       "        0.02299772, 0.02638865, 0.02258627, 0.02599038, 0.02228195,\n",
       "        0.02572417, 0.02205827, 0.0255632 , 0.02189588, 0.02548631,\n",
       "        0.02178036, 0.02547659, 0.02170079, 0.02552039, 0.02164882,\n",
       "        0.02560661, 0.02161803, 0.02572618, 0.02160343, 0.02587166,\n",
       "        0.02160115, 0.02603696, 0.02160818, 0.0262171 , 0.0216222 ,\n",
       "        0.02640795, 0.02164141, 0.02660619, 0.02166444, 0.02680907,\n",
       "        0.02169023, 0.02701437, 0.021718  , 0.0272203 , 0.02174717,\n",
       "        0.02742541, 0.02177731, 0.02762855, 0.02180813, 0.02782882,\n",
       "        0.02183941, 0.0280255 , 0.02187104, 0.02821807, 0.02190294,\n",
       "        0.02840613, 0.02193508, 0.02858938, 0.02196746, 0.02876763,\n",
       "        0.02200009, 0.02894076, 0.02203301, 0.02910873, 0.02206627,\n",
       "        0.02927151, 0.02209992, 0.02942915, 0.022134  , 0.02958171,\n",
       "        0.02216856, 0.02972927, 0.02220365, 0.02987195, 0.02223932,\n",
       "        0.03000986, 0.0222756 , 0.03014315, 0.02231253, 0.03027194]),\n",
       " array([0.14660043]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train_poly_scaled, y_train.ravel())\n",
    "best_model.coef_, best_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b55cb-5503-4006-a33b-110f4799af31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
