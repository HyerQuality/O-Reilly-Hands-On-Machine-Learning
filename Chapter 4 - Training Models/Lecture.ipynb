{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb50468-b0ac-4f7e-9fad-72f0af5fda20",
   "metadata": {},
   "source": [
    "<b> Chapter 4 is a technical chapter which will go over the mathematics behind some of the machine learning models that have been used so far.  I will be focusing on the linear algebra definitions and applications as they more succinctly embody the theories and calculations </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21427fd-a99e-40fa-b2ba-64cf906ee565",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7d0da-7a23-4517-9be1-fd0f0c60f613",
   "metadata": {},
   "source": [
    "<b> Simply put linear regression is a weighted average of features and a scalar bias.  The weights are bias are chosen based off the minimization of a cost function.  The most common cost function for linear regressions is Mean Squared Error (MSE) which is rooted (RMSE) when comparing across multiple models.  The reason MSE is used is because the derivative is much simpler to compute and requires significantly less computational power than the rooted version.  Since rooting the function simply scales it, selecting MSE to minimize has the same end result as selected RMSE to minimize.  This is true in general for any cost or reward function and it is encouraged to use the form of the cost function which minimizes computational effort. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046ac62a-356f-4853-b450-9ad60e81d47d",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "\n",
    "#### ${\\hat{\\theta}}$ = $(\\mathbf{X^T}\\mathbf{X})^{-1} \\mathbf{X^T} \\mathbf{y}$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "${\\hat{\\theta}}$ is the value of $\\theta$ that minimizes the cost function\n",
    "<br>\n",
    "$\\mathbf{y}$ is the vector of target values\n",
    "<br>\n",
    "$\\mathbf{X}$ is a matrix of features with a bias column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82d41c3-93a2-48f0-a45c-8ab7fedca06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.91900882],\n",
       "       [3.27450507]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Generate some data to validate the above expression'\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # mx + b : 3x + 4 + noise\n",
    "\n",
    "' Introduce the constant bias'\n",
    "X_b = np.c_[np.ones((100, 1)), X] # m x n matrix with column of 1's and column of X values\n",
    "\n",
    "' Apply the normal equation'\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b5e8a-2fa3-4ac2-a43a-90d5437aba31",
   "metadata": {},
   "source": [
    "<b> The bias chosen was 4 and the weight was 3.  The gaussian noise introduced makes it impossible to get these exact values but the derived values are almost perfect.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e937627-9114-44dc-bc48-36dce03a5618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.91900882],\n",
       "       [10.46801896]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Make some predictions'\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2064678e-404e-46a9-801e-2fa2f0d5c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b378979b80>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "' Plot the raw data and the best fit predictions'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta_best), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ccf1d9-8629-46c2-bfc0-f8c58a01a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  [3.91900882] Coefficient:  [[3.27450507]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.91900882],\n",
       "       [10.46801896]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Performing linear regression with sklearn is very simple'\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print('Bias: ', lin_reg.intercept_, 'Coefficient: ', lin_reg.coef_)\n",
    "\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c25a93-aaa3-431c-9225-86f88a855c8e",
   "metadata": {},
   "source": [
    "<b> A caveat for inverted matrices is that they require a non-zero determinant.  This isn't always the case.  A work-around is the Moore-Penrose Inverse aka the pseudoinverse.  This utilizes Singular Value Decomposition to generate an equivalent matrix which is always invertible.  This method is also more computationally efficient. Sklearn ueses this method natively </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7cfda-4dbb-4ef9-950f-5a0cf8a26f10",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4bddd-c559-4994-b25c-34e4a5442fda",
   "metadata": {},
   "source": [
    "<b> Gradient Descent outperforms sklearn and the Normal Equation when the dataset is extremely large (100,000+ features).  </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab888f9-f3d6-4d59-b2fb-3be5f936d97f",
   "metadata": {},
   "source": [
    "<b> General notes about gradient descent </b>\n",
    "* Tweaks parameters iteratively in order to minimize a cost function\n",
    "* Not all cost functions are convex.  As such, one hurdle for gradient descent is overcoming local minima and plateaus\n",
    "* The learning rate is the primary parameter for correctly finding the absolute minima.  Too high and the optimizer will diverge and never find a minima. Too low and it will stop too soon at a local minima\n",
    "* Because a convex function has exactly 1 minima they are ideal. If possible, transform the cost function such that its transformation is convex.\n",
    "* Scaling all features has an enormous impact on how long it takes for the optimizer to find the mininma. \n",
    "* Gradient descent utilizes the properties of partial derivatives.  As such, a less complex derivative is arguably more important than a convex, continuous function.  Both is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92247245-a5f5-4f49-875b-5b38ccfb5a9f",
   "metadata": {},
   "source": [
    "## Gradient Vector of the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1e88d-22af-425a-a0ac-fb0cce053898",
   "metadata": {},
   "source": [
    "#### $\\nabla_{\\boldsymbol\\theta}MSE(\\boldsymbol\\theta) = \\frac{2}{m}\\mathbf{X}^{T}(\\mathbf{X}\\boldsymbol\\theta - \\mathbf{y})$\n",
    "\n",
    "<br>\n",
    "\n",
    "<b> Notice that this formula computes over the full training set X at each step.  The batch size here is 1 and as a result the training time on large datasets will be greatly impacted.  There are other ways to do this involving larger batch sizes which will be discussed later<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea2681-4f81-475d-9831-59f1d06babe9",
   "metadata": {},
   "source": [
    "## Gradient Descent Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519d015-40e4-47e5-a810-65afd9b4c19b",
   "metadata": {},
   "source": [
    "#### $\\boldsymbol\\theta^{(next step)} = \\boldsymbol\\theta - \\eta\\nabla_{\\boldsymbol\\theta}MSE(\\boldsymbol\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a39c430-fa06-4cc3-8f06-0573df641bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.91900882]\n",
      " [3.27450507]] \n",
      " \n",
      " This is exactly what the Normal Equation yielded\n"
     ]
    }
   ],
   "source": [
    "' Validate the above expressions'\n",
    "eta = 0.1  #learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta*gradients\n",
    "    \n",
    "print (theta, '\\n', '\\n', 'This is exactly what the Normal Equation yielded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4447c-84dd-4cbd-83c6-a34ff63d9d59",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3bb400-25a4-4cba-99a7-a067fae5e936",
   "metadata": {},
   "source": [
    "<b> Opposite to batch gradient descent which uses the entire training set at each epoch, stochastic gradient descent takes one random sample from the training set at each epoch and applies the gradient descent step.  This requires vastly less memory and as such is a better choice for very large training sets.  However, there are some nuances to the process.  Due to the randomness the descent curve will not be smooth and the optimal solution will never be settled upon.  This can help escape local minima but comes at the cost of a less perfect output.  One solution to converging on the optimal output using stochastic gradient descent is to implement a learning schedule in which the learning rate is gradually reduced such that the random jumps at the point of convergence are so small that the output is essentially the same as batch gradient descent. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5c59c9-2dad-476f-a049-3d8fa0d4e28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.91357171],\n",
       "       [3.30173625]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' An SGD example with a simple learning schedule'\n",
    "\n",
    "# Epochs\n",
    "n_epochs = 50\n",
    "\n",
    "# Learning rate schedule\n",
    "t0, t1 = 5, 50 \n",
    "\n",
    "def learning_rate_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# Random initialization\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# SGD\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):  # m = 100 was declared in an earlier code block\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index: random_index + 1]\n",
    "        yi = y[random_index: random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_rate_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8638911-ccd9-4942-9aea-deb7314e5f60",
   "metadata": {},
   "source": [
    "<b> When using SGD it is best practice to shuffle the training set </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ea5f8d-8f0b-41bf-a882-12450b1e2829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.85426458] [3.22932641]\n"
     ]
    }
   ],
   "source": [
    "' Implementing SGD with Sklearn'\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel()) # .ravel() is a numpy function which flattens an array into a single 1D array with all the input-array elements and with the same type as it\n",
    "\n",
    "print(sgd_reg.intercept_, sgd_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c23ba8-eeeb-474f-a2cb-8c4832c54345",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919c95-85d0-4888-bc6d-b7387e5cc1b0",
   "metadata": {},
   "source": [
    "<b> Mini-batch gradient desecent is essentially the middle ground between batch and sgd.  It computes gradients on small, random sets of instances called mini-batches. One advantage of SGD is you can get performance boosts from hardward optimzation when using matrix operations and GPUs.  It will likely end up closer to the minimum than SGD as well, although it may be harder to escape from local minima.  For large datasets this is usually the best algorithm. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a376fd0-51d5-418d-9f1a-e13a5fe4fb76",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23315727-d3e7-4bbc-a012-33effb35573a",
   "metadata": {},
   "source": [
    "<b> Suprisingly, you can use linear regression algorithms to model non-linear relationships.  For example, consider a simple parabola.  You can simply augment the square of the feature(s) as additional features into the dataset and viola, you can use linear regression to estimate non-linear relationships <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1a046aa-cfd8-4728-9cc9-d29edf3229cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.85321989]), array([-0.85321989,  0.72798418]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' An example of estimating a polynomial function with linear regression'\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "X[0], X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba72094-9608-4d63-84d2-036d40d4d4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.13977325]), array([[1.0366274 , 0.50755395]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "\n",
    "# Here we see an intercept close to 2, a second-degree coeffecient close to 0.5, and a first-degree coeffecient close to 1 as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a152b10-b9b8-496c-939d-f4554e82ffb5",
   "metadata": {},
   "source": [
    "<b> Note that the PolynomialFeatures(degree=d) class will transform an array containing n-features into an array containing $\\large\\frac{(n + d)!}{d!n!}$ features.  Be cautious of the combinatorix nature of this augmentation <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f610155-d876-4187-aa31-bb3364d73366",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a475c28-5d1f-4d7c-be7d-731b2bf4eeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cd474-b0c6-43a1-8379-7a0b2e987740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d0e8f-56cb-472c-8670-47ea451a018e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab23bf-0046-4ac0-a021-fff452ee6431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9fb41-3366-43cb-855e-9d03cba65555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
