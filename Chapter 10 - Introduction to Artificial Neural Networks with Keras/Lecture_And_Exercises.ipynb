{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b70ba3-bdd8-44a8-88b2-5cd632fbbe75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c1fb2-13d3-4874-8bf7-f0d15fc91460",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36304c-e6df-4a5d-870d-f4a956b3e20d",
   "metadata": {},
   "source": [
    "*Artificial Nerual Networks* (ANNs) are Machine Learning models inspired by the networks of biological neurons found in our brains.  ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks such as classifying billions of images, powering speech recognition services, recommending the best videos to watch to hundreds of millions of users every day, or learning to beat the world champion at a game of Go.\n",
    "\n",
    "The first part of this chapter introduces artificial neural networks.  In the second part, we will look at how to implement neural networks using the popular Keras API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6790a-e40d-4918-8d95-811490cae7d0",
   "metadata": {},
   "source": [
    "## From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8e0e0-3025-49c4-a5fc-eba8bdb03116",
   "metadata": {},
   "source": [
    "The early successes of ANNs led to widespread belief that we would soon be conversing with truly intelligent machines.  When it became clear in the 1960's that this promise would go unfulfilled, funding flew elsewhere, and ANNs entered a long winter.  In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in *cnnectionism* (the study of neural networks).  But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as Support Vector Machines.  These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs, and this time is different.\n",
    "* There are now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "* The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time.\n",
    "* The training algorithms have been improved.\n",
    "* ANNs seem to have entered a virtuous cirle of funding and progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276296-1296-4911-a4d9-6c2311f9e061",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca2a95-d509-45d0-8013-5c266022aa6f",
   "metadata": {},
   "source": [
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt.  It is based on a *threshold logic unit* (TLU). For a TLU, the inputs and outputs are numbers, and each input connection is associated with a weight.  The TLU computes a weighted sum of its inputs, then applies a *step function* to that sum and outputs the results.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.  When all the neurons in a layer are connected to every neuron in the previous layer, the layer is called a *fully connected layer*, or a *dense layer*.\n",
    "\n",
    "Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once.\n",
    "\n",
    "<c> Equation 10-2: Computing the outputs of a fully connected layer </c>\n",
    "$$ h_{W, b}(X) = \\phi(XW + b) $$\n",
    "\n",
    "So how is a Perceptron trained? \"Cells that fire together, wire together.\" Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connections that help reduce the error.  More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions.  For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would ahve contributed to the correct prediction.  \n",
    "\n",
    "**The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers).**\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that implements a single-TLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a1171-8787-4eee-b98c-910112da8752",
   "metadata": {},
   "source": [
    "#### Example 1: Scikit-Learns Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02695eec-266a-414e-bfe0-399da24a9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74792a2-b38a-4233-bea2-c5b015bcaed3",
   "metadata": {},
   "source": [
    "You may have noticed that the Perceptron learning algorithm strongly resembles Stochastic Gradient Descent.  In fact, Scikit-Learn's Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss = 'perceptron', learning_rate = 'constant', eta0 = 1 (the learning rate), and penalty = None (no regularization).\n",
    "\n",
    "Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability.  This is one reason to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "There are a number of significant weaknesses of Perceptrons, in particular that they are incapable of solving some trivial problems.  It turns out that some of the limitations of Perceptrons an be eliminated by stacking multiple Perceptrons.  The resulting ANN is called a *Multilayer Perceptron* (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa583da-d07a-4ce9-8371-d0c91b615b24",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f77643-166f-40b3-9179-ddc1ca9dba94",
   "metadata": {},
   "source": [
    "An MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs, called the *output layer*.  The layers close to the input layers are usually called the *lower layers* and the ones close to the outputs are usually called the *upper layers*.  Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "The signal flows only in one direction (from the inputs to the oupts), so this architecture is an example of a *feedforward neural network* (FNN).\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers (dozens or hundreds), it is called a *deep neural network* (DNN).  For many years researchers struggled to find a way to train MLPs, without success, until 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking paper that introduced the *backpropagation* training algorithm, which is still used today.\n",
    "\n",
    "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter.  In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error.  Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Automatically computing gradients is called *automatic differentiation* or *autodiff*.  There are various autodiff techniques, with different pros and cons.  The one used by backpropagation is called *reverse-moode autodiff*.  It is fase and precise, and is well suited when the function to differentiate has many variables and few outputs.\n",
    "\n",
    "Let's run through the algorithm in a bit more detail:\n",
    "* It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times.  Each pass is called an *epoch*\n",
    "* Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the *forward pass*: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "* Next, the algorithm measures the network's output error (i.e. it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "* Then it computes how much each output connection contributed to the error. This is done analytically by applying the *chain rule* (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "* The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer.  As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
    "* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "**This algorithm is so important that it's worth summarizing it again: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)**\n",
    "\n",
    "***It is important to initialize all the hidden layers' connection weights randomly, or else training will fail.***\n",
    "\n",
    "In order for this algorithm to work properly, its authors made a key change to the MLP's architecture: they replaced the step function with the logistic (sigmoid) function:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} $$\n",
    "\n",
    "The backpropagation algorithm works well with many other activation functions.  Here are two other popular choices:\n",
    "1. **The hyperbolic tanger function: tanh(z)**: $2\\sigma(2z) - 1$\n",
    "<br>Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from -1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer's output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "2. **The Rectified Linear Unit function: ReLU(z)**: $max(0, z)$\n",
    "<br>The ReLU function is continuous but unfortunatley not differentiable at z=0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
    "\n",
    "Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. So if you don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can't solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86031eec-79e9-4ee4-8f9e-5c9a26005638",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255039-12e6-4a50-856a-0a7129a3d08d",
   "metadata": {},
   "source": [
    "First, MLPs can be used for regression tasks. If you want to predict a single value (like the value of a house), then you just need a single output neuron: its output is the predicted value. For multivariate regression (predicting multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D cooridinates, so you need two output neurons.\n",
    "\n",
    "**In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values.** If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Alternatively, you can use the *softplus* activation function, which is a smooth variant of ReLU: $softplus(z) = log(1 + \\exp(z))$\n",
    "\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can us ethe Huber loss, which is a combination of both.\n",
    "\n",
    "The Huber loss is quadratic when the error is smaller than a threshold $\\delta$ (typically 1) but linear when the error is larger than $\\delta$. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38bad854-01f6-4371-bce5-759b19a1caae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Typical Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input Neurons</th>\n",
       "      <td>One per intput feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layers</th>\n",
       "      <td>Depends on the problem, typically 1 to 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurons per Layer</th>\n",
       "      <td>Depends on the problem, typically 10 to 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Neurons</th>\n",
       "      <td>1 per prediction dimension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Activation</th>\n",
       "      <td>ReLU or SELU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Activation</th>\n",
       "      <td>None, or ReLU/softplus. Generally tailored for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>MSE, MAE or Huber if outliers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Typical Value\n",
       "Input Neurons                                 One per intput feature\n",
       "Hidden Layers               Depends on the problem, typically 1 to 5\n",
       "Neurons per Layer        Depends on the problem, typically 10 to 100\n",
       "Output Neurons                            1 per prediction dimension\n",
       "Hidden Activation                                       ReLU or SELU\n",
       "Output Activation  None, or ReLU/softplus. Generally tailored for...\n",
       "Loss Function                          MSE, MAE or Huber if outliers"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Typical regression MLP architecture\n",
    "pd.DataFrame.from_dict(data={\n",
    "    'Input Neurons': 'One per intput feature',\n",
    "    'Hidden Layers': 'Depends on the problem, typically 1 to 5',\n",
    "    'Neurons per Layer': 'Depends on the problem, typically 10 to 100',\n",
    "    'Output Neurons': '1 per prediction dimension',\n",
    "    'Hidden Activation': 'ReLU or SELU',\n",
    "    'Output Activation': 'None, or ReLU/softplus. Generally tailored for desired output.',\n",
    "    'Loss Function': 'MSE, MAE or Huber if outliers'\n",
    "}, orient='index').rename(columns={0: 'Typical Value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428968e-1cbf-4c2e-9fec-b864856bb097",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d448b1f-e775-446c-8363-d81acbfbdba8",
   "metadata": {},
   "source": [
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class.\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification tasks. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do no necessarily add up to 1.\n",
    "\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g. classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activiation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and they they add up to 1 (which is required if the classses are exclusive). This is called multiclass classification.\n",
    "\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3a57c9-eb7f-49e2-b13e-a7dbda21131e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Classification</th>\n",
       "      <th>Multilabel Binary Classification</th>\n",
       "      <th>Multiclass Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input and Hidden Layers</th>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Output Neurons</th>\n",
       "      <td>1</td>\n",
       "      <td>1 per label</td>\n",
       "      <td>1 per class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Layer Activation</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>Softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Binary Classification  \\\n",
       "Input and Hidden Layers     Same as regression   \n",
       "Number of Output Neurons                     1   \n",
       "Output Layer Activation               Logistic   \n",
       "Loss Function                    Cross-Entropy   \n",
       "\n",
       "                         Multilabel Binary Classification  \\\n",
       "Input and Hidden Layers                Same as regression   \n",
       "Number of Output Neurons                      1 per label   \n",
       "Output Layer Activation                          Logistic   \n",
       "Loss Function                               Cross-Entropy   \n",
       "\n",
       "                         Multiclass Classification  \n",
       "Input and Hidden Layers         Same as regression  \n",
       "Number of Output Neurons               1 per class  \n",
       "Output Layer Activation                    Softmax  \n",
       "Loss Function                        Cross-Entropy  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typical classification MLP architecture\n",
    "pd.DataFrame.from_dict({\n",
    "    'Input and Hidden Layers': ['Same as regression', 'Same as regression', 'Same as regression'],\n",
    "    'Number of Output Neurons': ['1', '1 per label', '1 per class'],\n",
    "    'Output Layer Activation': ['Logistic', 'Logistic', 'Softmax'],\n",
    "    'Loss Function': ['Cross-Entropy', 'Cross-Entropy', 'Cross-Entropy']\n",
    "}, orient='index').rename(columns={0: 'Binary Classification', 1: 'Multilabel Binary Classification', 2: 'Multiclass Classification'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12fe7b-7870-483d-82c1-e8a7b28deb6f",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676858a2-3031-42c6-9787-d4bbbdf184df",
   "metadata": {},
   "source": [
    "At present, you can choose from 3 popular open source Deep Learning libraries:\n",
    "1. TensorFlow\n",
    "2. Microsoft Cognitive Toolkit\n",
    "3. Theano\n",
    "\n",
    "As of 2016 Keras can be run on:\n",
    "1. Apache MXNet\n",
    "2. Apple's Core ML\n",
    "3. JavaScript\n",
    "4. TypeScript\n",
    "5. PlaidML\n",
    "\n",
    "Moreover, TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage of offering some very useful extra features like TensorFlow's Data API which makes it easy to load and preprocess data efficiently.\n",
    "\n",
    "**The most populare Deep Learning library after Keras and TensorFlow is Facebook's PyTorch. The good news is that its API is quite similar to Keras's, so once you know Keras, it is not difficult to switch to PyTorch, if you ever want to.** PyTorch's popularity grew exponentially in 2018, largely thanks to its simplicity and excellent documenatation, which were not TensorFlow 1.x's main strengths. However, TensorFlow 2 is arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and its developers have greatly simplified and cleaned up the rest of the API. Similarly, PyTorch's main weaknesses (e.g. limited portability and on computation graph analysis) have been largely addressed in PyTorch 1.0. Healthy competition is beneficial to everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f219-547d-4de9-b5ba-e2b88859e781",
   "metadata": {},
   "source": [
    "## Installing TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "892d512e-e5fe-477d-b166-3b516634908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f1eba2-bf16-4aca-ad19-79b3ecfbb8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.8.0', '2.8.0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7250c-4a87-4ccf-b484-ae04ab2063c5",
   "metadata": {},
   "source": [
    "For GPU support, at the time of this writing you need to install tensorflow-gpu instead of tensorflow, but the TensorFlow team is working on having a single library that will support both CPU-only and GPU-equipped systems. You will need to install extra libraries for GPU support (see http://tensorflow.org/install for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08600ab7-c44d-4442-96d9-065e848d9845",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bcd3b-0a1c-40eb-b418-da9b865aea0a",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05841662-7dd7-4dfe-9297-fdf5761f50db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Keras dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Check dimensions and datatypes\n",
    "print(X_train_full.shape, X_train_full.dtype)\n",
    "\n",
    "# Split the training set into a validation and training set\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12a37c-7d72-41ce-b4fd-2128ab4cc8ff",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d903317a-6efb-416f-9304-29b173c2b192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,149,010\n",
      "Trainable params: 1,149,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798f669-9a92-4083-ab96-4d7e89e6fdc5",
   "metadata": {},
   "source": [
    "Let's go through this code line by line:\n",
    "1. The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "2. Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn't include the batch size, only the shape of the instances. Alternatively, you could add a keras.Layers.InputLayer as the first layer, setting input_shape=[28, 28].\n",
    "3. Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activiation function. **Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs.** It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "4. The we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "5. Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc032b-ff07-49f9-8656-b23e97af33bd",
   "metadata": {},
   "source": [
    "**Note that Dense layers often have a *lot* of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data**\n",
    "\n",
    "You can easily get a model's list of layers, to fetch a layer by its index, or you can fetch it by name. All the parameters of a layer can be accessed using its get_weights() and set_weights() methods. For a Dense layer, this includes both the connection weights and the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48c9de87-b81a-49d1-8915-daaf5714b959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: dense_17. Existing layers are: ['flatten', 'dense', 'dense_1', 'dense_2', 'dense_3', 'dense_4'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mlayers, model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdense_17\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2828\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[1;34m(self, name, index)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name:\n\u001b[0;32m   2827\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[1;32m-> 2828\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Existing layers are: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2829\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlayer\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2830\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProvide either a layer name or layer index at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2831\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`get_layer`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No such layer: dense_17. Existing layers are: ['flatten', 'dense', 'dense_1', 'dense_2', 'dense_3', 'dense_4']."
     ]
    }
   ],
   "source": [
    "model.layers, model.layers[1].name, model.get_layer('dense_17') is model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71d9ee-b90c-4f7e-b291-f3c12698ef11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "weights.shape, biases.shape, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19c11-771a-451e-8382-a20c3825e99b",
   "metadata": {},
   "source": [
    "The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. **Until the model is really built, the layers will not have any weights, and you will not be able to do certain things such as print the model summary or save the model.** So if you know the input shape when creating the model, it is best to specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c197eb-4248-4f09-8ff8-7532d8083f40",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9cbe-dc1c-4468-9134-9f44c7c952f7",
   "metadata": {},
   "source": [
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756a27e-5d3c-4787-ac13-aef2f10ef28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8492c25-35cc-4e7d-8cc7-5f7316f7afb6",
   "metadata": {},
   "source": [
    "This code requires some explanation. **First, we use the 'sparse_categorical_crossentropy' loss because we have sparse labels (i.e. for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0, 0, 1, 0]) to represent class 2), then we would need to use the 'categorical_crossentropy' loss instead.**\n",
    "\n",
    "If we were doing binary classification (with one or more binary labels), then we would use the 'sigmoid' (i.e. logistic) activiation function in the output layer instead of the 'softmax' activiation function, and we would use the 'binary_crossentropy' loss.\n",
    "\n",
    "If you want to convert sparse labels (i.e. class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way around, use the np.argmax() function with axis=1.\n",
    "\n",
    "When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(learning_rate=???) to set the learning rate, rather than optimizer='sgd', which defaults to learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c95c3-46ca-4926-a2ac-b20f7252a7fc",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7178-e731-4495-b9ab-420c6f359489",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained. For this we simply need to call its fit() method. We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train. We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch. If the performance on the training set is much better than the validation set, your model is probably overfitting the training set (or there is a bug such as a data mismatch between the training set and the validation set).\n",
    "\n",
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation.\n",
    "\n",
    "**If the training set was very skewed, with some classes being overrepresented and others underrepresnted, it would be useful to set the class_weight argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes.**\n",
    "\n",
    "If you need per-instance weights, set the sample_weight argument (it supersedes class_weight). Per-instance weights could be useful if some instances were labeled by experts while others were labelled using a crowdsourcing platform: you might want to give more weight to the former.\n",
    "\n",
    "The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a pandas DataFrame and call its plot() method, you get the learning curves for the model.\n",
    "\n",
    "**If your training or validation data does not match the expected shape, you will get an exception. This is perhaps the most common error, so you should get familiar with the error message. The message is actually quite clear: for example, if you try to train this model with an array containing flattened images it will throw this error.**\n",
    "\n",
    "If the validation curves are close to the training curves, it means there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that's not the case: indeed, the validation error is computed at the *end* of each epoch, while the training error is computed using a running mean *during* each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see the that the training and validations curve overlap almost perfectly at the beginning of training.\n",
    "\n",
    "**When plotting the training curve, it should be shifted by half an epoch to the left.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578895e-708d-41cb-89b9-4a529198aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549fd8e-cf03-424b-8523-9e45468686d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame from model history\n",
    "learning_curves = pd.DataFrame.from_dict(history.history, orient='columns')\n",
    "\n",
    "# Shift training loss back by 1 epoch for betting alignment (this is shifted left by 0.5 epochs becuase the training loss is a rolling mean during the epoch)\n",
    "learning_curves.loc[:, 'loss'] = learning_curves.loc[:, 'loss'].shift(-1)\n",
    "\n",
    "# Plot\n",
    "learning_curves.plot(ylim=(0,1), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44463a0-a3da-40c3-8959-282e301ac00c",
   "metadata": {},
   "source": [
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. **You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue trianing.** It's as simple as calling the fit() method again, since Keras just continues training where you left off.\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn't help, try another optimizer (and always return the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning the model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size.  \n",
    "\n",
    "Once you are satisfied with your model's validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this by using the evaluate() method. It is common to get slightly lower performance on the test set than the validation set, because the hyperparameters are tuned on the validation set, not the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1f3ec-82f3-440c-921d-43b79d74f00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c5c23-c9ec-4d4b-8de3-3bc4a25072a9",
   "metadata": {},
   "source": [
    "### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3df9-1540-47c2-a3f2-17b3409cc2f6",
   "metadata": {},
   "source": [
    "Next, we can use the model's predict() method to make predictions on new instances. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the predict_classes() method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fe7b1-1a9b-4a3b-818a-e6f5b2077160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced6416-8f8e-4195-a6b6-81691419097d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred, np.array(class_names)[y_pred], y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c3179-eff8-4326-9b9c-67c5ab5b2271",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3a1ec-3cfb-449f-aa2f-9b64e81c8a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "' Lets switch to the California housing problem and tackle it using a regression neural network'\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the raw data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split data in train, val and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full\n",
    ")\n",
    "\n",
    "# Scale the data. Remember, only fit the scaler to the train data.  Transform the val and test data according to this fit.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Construct the TF model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01)\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Make predictions\n",
    "X_new = X_test[:3] # Pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b42c19-e8fd-4dd1-98f3-c0890ddfbbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse_test, y_pred, y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ca00c-961f-4e65-b68b-fdab06130e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(np.concatenate((y_pred, np.zeros((y_pred.shape[0], 7))), axis=1))[:, 0] * 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d733c74-bb03-4494-8daa-a879e445e83f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(np.concatenate((y_test[:3].reshape(-1, 1), np.zeros((y_test[:3].shape[0], 7))), axis=1))[:, 0] * 1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d91354-2dfc-4dcf-bdf9-80e882ed512a",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06c060-340b-4d0d-92f4-3ea291f528db",
   "metadata": {},
   "source": [
    "One example of a nonsequential neural network is a *Wide & Deep* neural network. It connects all or part of the inputs directly to the output layer. **This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).** In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by the sequence of transformations. Let's build such a neural network to tackle the California housing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfdf1a-4691-425c-906a-109d934d01b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913572c-1adb-4c13-8b56-1d0021223396",
   "metadata": {},
   "source": [
    "Let's go through each line of this code:\n",
    "1. First, we need to create an Input object. This is a specification of the kind of input the model will get, including its shape and dtype. A model may actually have multiple inputs, as we will see shortly.\n",
    "2. Next, we create a Dense layer with 30 neurons, using the ReLU activation function. **As soon as it is created, notice that we call it like a function, passing it the input.** This is why this is called the Functional API. **Note that we are just telling Keras how it should connect the layers together**, no actual data is being processed yet. \n",
    "3. We then create a second hidden layer, and again we use it as a function. **Note that we pass it the output of the first hidden layer.**\n",
    "4. Next, we create a Concatenate layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer. You may prefer the keras.layers.concatenate() function, which creates a Concatenate layer and immeidately calls it with the given inputs.\n",
    "5. Then we create the output layer, with a single neuron and no activiation function, and we call it like a function, passing it the result of the concatenation.\n",
    "6. Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. In this case, when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input. Alternatively, you can pass a dictionary mapping the input names to the input values, like {'wide_input': X_train_A, 'deep_input': X_train_B}. This is especially useful when there are many inputs, to avoid getting the order wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01213a2c-b7a1-4908-9583-ff519a436535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-3)\n",
    ")\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit(\n",
    "    (X_train_A, X_train_B),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=((X_valid_A, X_valid_B), y_valid)\n",
    ")\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "mse_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c291d-d8d8-4d29-9900-c9e11a5b1bbe",
   "metadata": {},
   "source": [
    "There are many use cases in which you may want to have multiple outputs:\n",
    "1. The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object's center, as well as its width and height) and a classification task.\n",
    "2. Similarly, you may have multiple independent tasks based on the same data. **Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are usefull across tasks.** For example, you could perform *multitask classification* on pictures of faces, using one output to classify the person's facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.\n",
    "3. Another use case is as a regularization technique (i.e. a training contraint whose objective is to reduce overfitting and thus improve the model's ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture to ensure that hte underlying part of the newtwork learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model's list of outputs. Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses. By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output (as it is just used for regularization), so we want to give the main output's loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model. Also, now when we train the model, we need to provide labels for each output.\n",
    "\n",
    "When we evaluate the model, Keras will return the total loss, as well as the individual losses. Similarly, the predict() method will return predictions for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224921c-7981-4150-ad2c-baeb50d9281b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B],\n",
    "    [y_test, y_test]\n",
    ")\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict(\n",
    "    [X_test_A[:3], X_test_B[:3]]\n",
    ")\n",
    "\n",
    "total_loss, main_loss, aux_loss, y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d257b2-1ba4-4b9e-92d3-c32e5a685d77",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f040844-cd38-48b6-9c40-398084febfbc",
   "metadata": {},
   "source": [
    "Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages:\n",
    "1. The model can easily be saved, cloned and shared\n",
    "2. Its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early.\n",
    "3. It's fairly easy to debug, since the whole model is a static graph of layers.\n",
    "\n",
    "But the flip side is just that: it's static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you.\n",
    "\n",
    "Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. The big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations, etc. This makes it a great API for researchers experimenting with new ideas.\n",
    "\n",
    "This extra flexibility does come at a cost: your model's architecture is hidden within the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. **So unless you really need that extra flexibility, you should probably stick to the Sequential and Functional API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6a212-2f59-4e81-af56-dbf7a83deedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g, name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs: tuple):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb15599-03f0-4b9f-9387-a9a78f88ac44",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2ef8e-281c-4df1-8180-4b4a5abd8460",
   "metadata": {},
   "source": [
    "Saving a trained Keras model is as simple as it gets. Keras will use the HDF5 format to save both the model's architecture (including every layer's hyperparameters) and the values of all the model parameters for every layer (e.g. connection weights and biases).. It also saves the optimizer (including its hyperparameters and any state it may have). Loading the model is just as easy.\n",
    "\n",
    "**This will work when using the Sequential API and Functional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything else yourself**\n",
    "\n",
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during the training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? **Use callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f24a5e-d979-4400-8db6-1e1c39c6c31a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")\n",
    "\n",
    "model.save('my_keras_model.h5')\n",
    "loaded_model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe76381-f4c6-4c6d-b88d-9a626e5d7926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(loaded_model.get_weights()[0] == model.get_weights()[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b5868-c43e-4137-bcb2-0e0778caeb43",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724208a-4aa0-45a0-a75b-d31790d573ac",
   "metadata": {},
   "source": [
    "The fit() method accepts a *callbacks* argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints on your model at regular intervals during training, by default at the end of each epoch. **Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ec518-c363-43ec-8d01-829949b057fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'my_keras_model.h5',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "history = keras.models.load_model('my_keras_model.h5') # roll back to best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e10610-8664-4a65-938d-6654dc0bdb78",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791aad15-00bd-447b-831a-18d37b8074b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=100,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85f324-e3d5-4376-8901-1de1091a2537",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g. detect over-fitting).\n",
    "\n",
    "Callbacks can also be used during evaluation and predictions, should you ever need them. For evaluation, you should implement on on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end(). These are called by evaluate().  For prediction you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end(). These are called by predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96140fa-37a0-4f06-a819-bf72bcada0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {:2f}'.format(logs['val_loss'] / logs['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd83c1-441a-423b-b4d2-ce77a78bbdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "custom_cb = PrintValTrainRatioCallback()\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=100,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[early_stopping_cb, custom_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85495a8-9cc8-4921-b9cc-0902db600e0e",
   "metadata": {},
   "source": [
    "## Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c64cf-a784-43cd-a9c0-fea099b73d51",
   "metadata": {},
   "source": [
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more. This tool is installed automatically when you install TensorFlow.\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called *event files*. Each binary data record is called a *summary*. This allows you to visualize live data (with a short delay). In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae95590-d731-4e6e-9fb0-37f5a03430c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d_-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=100,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[early_stopping_cb, tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff488c6-b079-4ccd-88ca-947baf254d4c",
   "metadata": {},
   "source": [
    "Next you need to start the TensorBoard server. One way to do this is by running a command in a terminal. If you installed TensorFlow within a virtualenv, you should activate it. Next, run the following command at the root of the project\n",
    "\n",
    "tensorboard --logdir=./my_logs --port=6006\n",
    "\n",
    "If your sehll cannot find the *tensorboard* script, then you must update your PATH environment variable so that it contains the directory in which the script was installed. Once the server is up, you can open a web browser and go to http://localhost:6006\n",
    "\n",
    "Alternatively, you can use TensorBoard directly within Jupyter, by running the following commands. \n",
    "\n",
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. The following code creates a SummaryWriter using the create_file_writer() function and it uses this writer as a context to log scalers, histograms, images, audio, and text, all of which can then be visualized using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2abb5-304a-4589-899e-84a723327aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4d401-a1a7-487c-9621-87b5f16941ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar('my_scalar', np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram('my_hist', data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32x32 RGB images\n",
    "        tf.summary.image('my_images', images * step / 1000, step=step)\n",
    "        texts = ['The step is ' + str(step), 'Its square is ' + str(step**2)]\n",
    "        tf.summary.text('my_text', texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce001a8-4c61-414c-924b-3244a8d62c75",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567f6cc-271a-4951-b7c4-20dc44dbe9e5",
   "metadata": {},
   "source": [
    "How do you know what combination of hyperparameters is the best for your task? One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation). We can use GridSearchCV or RandomizedSearchCV to explore the hyperparameter space. To do this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters.\n",
    "\n",
    "It is good practice to provide reasonable defaults to as many hyperparameters as you can, as Scikit-Learn does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c3a09-2811-4b2e-8164-c4174606f82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1e57e-ea52-42f3-9484-672d9a9c9d89",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, let's create a KerasRegressor based on this build_model() function. The KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameters when creating it, it will use the default hyperparameters we defined in build_model(). Now we can use this object like a regular Scikit-Learn regressor: we can train it using its fit() method, then evaluate it using its score() method, and use it to make predictions using its predict() method, as you can see in the following code.\n",
    "\n",
    "Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e. higher should be better). Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5ea49-e880-414b-9176-be24c6cc7713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "keras_reg = KerasRegressor(build_model)\n",
    "keras_reg.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=5)],\n",
    "    verbose=0\n",
    ")\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "print(mse_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f3b4d-fcba-46ca-bff2-1d4d5e2d6b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_reg = KerasRegressor(build_model, n_hidden=1, n_neurons=30, learning_rate=3e-3) # For this wrapper you need to initialize the parameters you're testing\n",
    "param_distribs = {\n",
    "    'n_hidden': np.arange(0, 3, 1),\n",
    "    'n_neurons': np.arange(1, 100, 1),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "opt_keras_reg = KerasRegressor(build_model(**rnd_search_cv.best_params_))\n",
    "opt_keras_reg.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n",
    "    verbose=0\n",
    ")\n",
    "mse_test = opt_keras_reg.score(X_test, y_test)\n",
    "y_pred = opt_keras_reg.predict(X_new)\n",
    "print(mse_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e2e47-c094-4cda-afcc-6a150c80edfb",
   "metadata": {},
   "source": [
    "This is identical to what we did in Chapter 2, except here we pass extra parameters to the fit() method, and they get relayed to the underlying Keras models. Note that RandomizedSearchCV uses K-fold cross-validation, so it does not use X_valid and y_valid, which are only used for early stopping. The exploration may last many hours, depending on the hardware, the size of the dataset, the complexity of the model, and the avlues of n_iter and cv. When its over, you can access the best parameters found, the best score, and the trained Keras model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70eb74-d7d2-4253-8156-d0a6b6a7ca9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model_\n",
    "rnd_search_cv.best_params_, rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827817b8-3b91-419a-8a93-d7ccd57a8ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('my_keras_model.h5')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214936ba-e34c-4bc9-9866-a964ba015cc0",
   "metadata": {},
   "source": [
    "You can now save this model, evaluate it on the test set, and, if you are satisfied with its performance, deploy it to production. Using randomized search is not too hard, and it works well for many fairly simple problems. When training is slow, however (e.g. for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space. You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranges of values centered on the best ones found during the first run, and so on. This approach is very time consuming.\n",
    "\n",
    "Fortunately, there are many techniques to explore a search space much more efficiently than randomly. Such techniques take care of the 'zooming' process for you and lead to much better solutions in much less time. Here are some python libraries you can use to optimize hyperparameters:\n",
    "1. Hyperopt (https://github.com/hyperopt/hyperopt)\n",
    "<br>A popular library for optimizing over all sorts of complex search spaces (including real values, such as the learning rate, and discrete values, such as the number of layers).\n",
    "\n",
    "2. Hyperas (https://github.com/maxpumperla/hyperas), kopt (https://github.com/Avsecz/kopt), Talos (https://github.com/autonomio/talos)\n",
    "<br>Useful libraries for optimizing hyperparameters for Keras models (the first two are based on Hyperopt)\n",
    "\n",
    "3. Keras Tuner (https://homl.info/kerastuner)\n",
    "<br>**An easy-to-use hyperparameter optimization library by Google for Keras models, with a hosted service for visualization and analysis**\n",
    "\n",
    "4. Scikit-Optimize (skopt) (https://scikit-optimize.github.io/)\n",
    "<br>A general-purpose optimization library. The BayesSearchCV class performs Bayesian optimization using an interface similar to GridSearchCV\n",
    "\n",
    "5. Sprearmint (https://github.com/JasperSnoek/spearmint)\n",
    "<br>A Bayesian optimization library.\n",
    "\n",
    "6. Hyperband(https://github.com/zygmuntz/hyperband)\n",
    "<br>A fast hyperparameter tuning library baesd on the recent Hyperband paper (https://home.info/hyperband) by Lisha Li et al.\n",
    "\n",
    "7. Sklearn-Deap (https://github.com/rsteca/sklearn-deap)\n",
    "<br>A hyperparameter optimization library based on evolutionary algorithms, with a GridSearchCV like interface.\n",
    "\n",
    "Hyperparameter tuning is still an active area of research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da9882-5cf1-431a-a3f6-9a437a9225ec",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cabbd6a-a28b-4c8d-a335-4ead0c42fb6c",
   "metadata": {},
   "source": [
    "An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, deep networks have a much higher *parameter efficiency* than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance wit the same amount of training data.\n",
    "\n",
    "Real-world data is often structured in such a hierarchical way (symmetry, fractals, subsets, etc), and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g. line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g. squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g. faces).\n",
    "\n",
    "For exapmle, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; this is called *transfer learning*.\n",
    "\n",
    "In summary, for many problems you can start with just one or two hidden layers and the neural network will work just fine. **For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set.** Very complex tasks,such as large image classification or speech recognition, typically require networks with doezens of layers (or even hundreds, but not fully connected ones, as we will see in Chapter 14), and they need a huge amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae24c8-71a9-46b0-b369-223d27d82978",
   "metadata": {},
   "source": [
    "## Number of Neurons per Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef4c0a5-5ef0-4cfa-b4b1-558e0d8482d6",
   "metadata": {},
   "source": [
    "**In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer**.\n",
    "\n",
    "The number of neurons in the input and output layers is determined by the type of input and output your task requires. As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer. The rational being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200 neurons, and the third with 100. **However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer.** That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. **But in practice, it's often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdd648-e30f-4c5b-bfc2-2c57868e7dfc",
   "metadata": {},
   "source": [
    "## Learning Rate, Batch Size, and Other Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04702ee6-3d31-4583-8b9c-03e5cf866b27",
   "metadata": {},
   "source": [
    "The number of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, as well as tips on how to set them:\n",
    "\n",
    "1. Learning Rate\n",
    "<br>**The learning rate is arguably the most important hyperparameter.** In general, the optimal learning rate is about half of the maximum learning rate (i.e. the learning rate above which the training algorithm diverges, as we saw in Chapter 4). One way to find a good learning rate is to train the model for a few hundreds iterations, starting with a very low learning rate (e.g. 1e-6) and gradually increasing it up to a very large value (e.g. 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g. by exp(log(1e7) / 500) to go from 1e-6 to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a b it lower than the point at which the loss starts to climb (typically about 10x lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate. We will look at more learning rate techniques in Chapter 11. **The optimal learning rate depends on other hyperparameters, especially the batch size, so if you modify any hyperparameter make sure to update the learning rate as well.**\n",
    "\n",
    "2. Optimizer\n",
    "<br>Choosing a better optimizer than plain old Mini-batch Gradient Desecent (and tuning its hyperparameters) is also quite important. We will see several advanced optimizers in Chapter 11.\n",
    "\n",
    "3. Batch Size\n",
    "<br>The batch size can have a significant impact on your model's performance and training time. **The main benefit of using large batch sizes is that hardware accelerators like GPU's can process them efficiently** (see Chapter 19), so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. There's a catch, though: **in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size.** In April 2018, Yann LeCun even tweeted \"Friends don't let friends use mini-batches larger than 32,\" **citing a 2018 paper (https://homl.info/smallbatch) by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time.** Other papers point in the opposite direction, however; in 2017, papers by Elad Hoffer et al. (https://homl.info/largebatch) and Priya Goyal et al. (https:homl.info/largebatch2) showed that it was possible to use very large batch sizes (up to 8,192) using various techniques such as warming up the learning rate (i.e. starting training with a small learning rate, then ramping it up, as we will see in Chapter 11). This led to a very short training time, without any generalization gap. **So one strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch instead.**\n",
    "\n",
    "4. Activation Function\n",
    "<br>We discussed how to choose the activation function earlier in this chapter: **in general, the ReLU activation function will be a good default for all hidden layers.** For the output layer, it really depends on your task.\n",
    "\n",
    "5. Number of Iterations\n",
    "<br>In most cases, the number of training iterations does not actually need to be tweaked; just use early stopping instead.\n",
    "\n",
    "For more best practices regarding tuning neural network hyperparameters, check out the excellent 2018 paper (https://homl.info/1cycle) by Leslie Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6b786-4418-4c91-b6e2-77a23b6bbd93",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2baf6-8c50-4f9a-ad7c-1318b7098126",
   "metadata": {},
   "source": [
    "1. This excerise has the reader go to https://playground.tensorflow.org/ and familiarize with basic Sequential networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14cc5f-aa74-491e-a363-25e4c2233c68",
   "metadata": {},
   "source": [
    "2. Draw an ANN using the original artificial neurons that computes A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cdeff-5e2c-47ea-980a-a752e2788c6c",
   "metadata": {},
   "source": [
    "**My answer** <br>\n",
    "I don't feel this is worth putting effort into. This can be quickly searched if it is required in real life and doesn't add to the practical understanding of what the chapter teaches, in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dded51-0c17-4357-8e20-22134c22f14a",
   "metadata": {},
   "source": [
    "3. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e. a single layer of threshold logic units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fca5ee-b63a-4724-b7b0-59d3a9ca7a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d304cb-ca3f-42ce-adf3-688ce92fe5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7635732-c3d2-48c0-a0b9-371bd6d58e8e",
   "metadata": {},
   "source": [
    "4. Why was the logistic activiation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361965b1-a0a8-4089-9090-c2f0f89421b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b5f2f-39d8-45a3-ac12-8f83d4b6048f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c108929-aee9-4f2e-8be9-6edf081182a7",
   "metadata": {},
   "source": [
    "5. Name 3 popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef923f44-8fe8-4db3-a639-5508dd419270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e6907-4456-4af3-8de9-24607b095d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d0e817-179f-4bd7-93e0-5969bda162ef",
   "metadata": {},
   "source": [
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.<br>\n",
    "\n",
    "    a. What is the shape of the input matrix X? <br>\n",
    "    b. What are the shapes of the hidden layer's weight vector $W_h$ and its bias vector $b_h$?<br>\n",
    "    c. What are the shapes of the output layer's weight vector $W_o$ and its bias vector $b_o$?<br>\n",
    "    d. What is the shape of the network's output matrix Y?<br>\n",
    "    e. Write the equation that computes the networks output matrix Y as a function of X, $W_h$, $b_h$, $W_o$, and $b_o$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f1f36-e742-488c-aaaa-12ae18b4eefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e34736-a096-46f1-a769-d3cea6a11df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "969ed7eb-bd2a-46f4-9333-d0bccdd9456e",
   "metadata": {},
   "source": [
    "7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instaed you want to tackle MNIST, how many neurons do you need in the output layer, and which activiation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625b4c4-82b2-45e8-a2be-eb25586e0d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b982987-0e99-473d-80a9-5dc8c1395b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c97a1f-9639-4419-8423-ec8e67e37d22",
   "metadata": {},
   "source": [
    "8. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08394ce-796a-4649-8d2c-c6458f537169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036fc35-dd4d-4318-8f89-f0ba2a545638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9eb40d6-75c2-44f9-b809-d4bb9bda46ad",
   "metadata": {},
   "source": [
    "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f466548-e836-478c-9cbb-5dac3e56325a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7430b12-6b60-46ec-a39f-058365857bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee7e16d8-f466-4785-aee7-5702932c5f28",
   "metadata": {},
   "source": [
    "10. Train a deep MLP on the MNIST dataset (you can load it using keras.datasets.mnist.load_data()). See if you can get over 98% precision. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e. by growing the learning rate exponentially, plotting the error, and finding the point where the error shoots up). Try adding all the bells and whistles:<br><br>\n",
    "    a. Save checkpoints<br>\n",
    "    b. Early stopping<br>\n",
    "    c. Plot learning curves using TensorBoard<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa8283-d358-41d8-b8fc-0bc506d89349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1dab62-ee69-47f6-ac96-da2f1e19d064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985d978-7505-41e4-978f-40c40bbd09bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4804b-06b3-4cec-9cec-571ceb0d1acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec81fd-6247-4ce4-96ce-74836a733798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f704a-4d49-48c2-9b8b-48a1ba131e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0ffa6-3522-4cd6-a2d5-6b8c9f8f3ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0a55b-9627-4e12-acfb-7143f9fb1dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
