{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "05b70ba3-bdd8-44a8-88b2-5cd632fbbe75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c1fb2-13d3-4874-8bf7-f0d15fc91460",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36304c-e6df-4a5d-870d-f4a956b3e20d",
   "metadata": {},
   "source": [
    "*Artificial Nerual Networks* (ANNs) are Machine Learning models inspired by the networks of biological neurons found in our brains.  ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks such as classifying billions of images, powering speech recognition services, recommending the best videos to watch to hundreds of millions of users every day, or learning to beat the world champion at a game of Go.\n",
    "\n",
    "The first part of this chapter introduces artificial neural networks.  In the second part, we will look at how to implement neural networks using the popular Keras API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6790a-e40d-4918-8d95-811490cae7d0",
   "metadata": {},
   "source": [
    "## From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8e0e0-3025-49c4-a5fc-eba8bdb03116",
   "metadata": {},
   "source": [
    "The early successes of ANNs led to widespread belief that we would soon be conversing with truly intelligent machines.  When it became clear in the 1960's that this promise would go unfulfilled, funding flew elsewhere, and ANNs entered a long winter.  In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in *cnnectionism* (the study of neural networks).  But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as Support Vector Machines.  These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs, and this time is different.\n",
    "* There are now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "* The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time.\n",
    "* The training algorithms have been improved.\n",
    "* ANNs seem to have entered a virtuous cirle of funding and progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276296-1296-4911-a4d9-6c2311f9e061",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca2a95-d509-45d0-8013-5c266022aa6f",
   "metadata": {},
   "source": [
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt.  It is based on a *threshold logic unit* (TLU). For a TLU, the inputs and outputs are numbers, and each input connection is associated with a weight.  The TLU computes a weighted sum of its inputs, then applies a *step function* to that sum and outputs the results.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.  When all the neurons in a layer are connected to every neuron in the previous layer, the layer is called a *fully connected layer*, or a *dense layer*.\n",
    "\n",
    "Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once.\n",
    "\n",
    "<c> Equation 10-2: Computing the outputs of a fully connected layer </c>\n",
    "$$ h_{W, b}(X) = \\phi(XW + b) $$\n",
    "\n",
    "So how is a Perceptron trained? \"Cells that fire together, wire together.\" Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connections that help reduce the error.  More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions.  For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would ahve contributed to the correct prediction.  \n",
    "\n",
    "**The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers).**\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that implements a single-TLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a1171-8787-4eee-b98c-910112da8752",
   "metadata": {},
   "source": [
    "#### Example 1: Scikit-Learns Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02695eec-266a-414e-bfe0-399da24a9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74792a2-b38a-4233-bea2-c5b015bcaed3",
   "metadata": {},
   "source": [
    "You may have noticed that the Perceptron learning algorithm strongly resembles Stochastic Gradient Descent.  In fact, Scikit-Learn's Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss = 'perceptron', learning_rate = 'constant', eta0 = 1 (the learning rate), and penalty = None (no regularization).\n",
    "\n",
    "Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability.  This is one reason to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "There are a number of significant weaknesses of Perceptrons, in particular that they are incapable of solving some trivial problems.  It turns out that some of the limitations of Perceptrons an be eliminated by stacking multiple Perceptrons.  The resulting ANN is called a *Multilayer Perceptron* (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa583da-d07a-4ce9-8371-d0c91b615b24",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f77643-166f-40b3-9179-ddc1ca9dba94",
   "metadata": {},
   "source": [
    "An MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs, called the *output layer*.  The layers close to the input layers are usually called the *lower layers* and the ones close to the outputs are usually called the *upper layers*.  Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "The signal flows only in one direction (from the inputs to the oupts), so this architecture is an example of a *feedforward neural network* (FNN).\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers (dozens or hundreds), it is called a *deep neural network* (DNN).  For many years researchers struggled to find a way to train MLPs, without success, until 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking paper that introduced the *backpropagation* training algorithm, which is still used today.\n",
    "\n",
    "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter.  In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error.  Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Automatically computing gradients is called *automatic differentiation* or *autodiff*.  There are various autodiff techniques, with different pros and cons.  The one used by backpropagation is called *reverse-moode autodiff*.  It is fase and precise, and is well suited when the function to differentiate has many variables and few outputs.\n",
    "\n",
    "Let's run through the algorithm in a bit more detail:\n",
    "* It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times.  Each pass is called an *epoch*\n",
    "* Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the *forward pass*: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "* Next, the algorithm measures the network's output error (i.e. it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "* Then it computes how much each output connection contributed to the error. This is done analytically by applying the *chain rule* (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "* The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer.  As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
    "* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "**This algorithm is so important that it's worth summarizing it again: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)**\n",
    "\n",
    "***It is important to initialize all the hidden layers' connection weights randomly, or else training will fail.***\n",
    "\n",
    "In order for this algorithm to work properly, its authors made a key change to the MLP's architecture: they replaced the step function with the logistic (sigmoid) function:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} $$\n",
    "\n",
    "The backpropagation algorithm works well with many other activation functions.  Here are two other popular choices:\n",
    "1. **The hyperbolic tanger function: tanh(z)**: $2\\sigma(2z) - 1$\n",
    "<br>Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from -1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer's output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "2. **The Rectified Linear Unit function: ReLU(z)**: $max(0, z)$\n",
    "<br>The ReLU function is continuous but unfortunatley not differentiable at z=0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
    "\n",
    "Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. So if you don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can't solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86031eec-79e9-4ee4-8f9e-5c9a26005638",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255039-12e6-4a50-856a-0a7129a3d08d",
   "metadata": {},
   "source": [
    "First, MLPs can be used for regression tasks. If you want to predict a single value (like the value of a house), then you just need a single output neuron: its output is the predicted value. For multivariate regression (predicting multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D cooridinates, so you need two output neurons.\n",
    "\n",
    "**In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values.** If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Alternatively, you can use the *softplus* activation function, which is a smooth variant of ReLU: $softplus(z) = log(1 + \\exp(z))$\n",
    "\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can us ethe Huber loss, which is a combination of both.\n",
    "\n",
    "The Huber loss is quadratic when the error is smaller than a threshold $\\delta$ (typically 1) but linear when the error is larger than $\\delta$. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bad854-01f6-4371-bce5-759b19a1caae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Typical Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input Neurons</th>\n",
       "      <td>One per intput feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layers</th>\n",
       "      <td>Depends on the problem, typically 1 to 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurons per Layer</th>\n",
       "      <td>Depends on the problem, typically 10 to 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Neurons</th>\n",
       "      <td>1 per prediction dimension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Activation</th>\n",
       "      <td>ReLU or SELU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Activation</th>\n",
       "      <td>None, or ReLU/softplus. Generally tailored for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>MSE, MAE or Huber if outliers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Typical Value\n",
       "Input Neurons                                 One per intput feature\n",
       "Hidden Layers               Depends on the problem, typically 1 to 5\n",
       "Neurons per Layer        Depends on the problem, typically 10 to 100\n",
       "Output Neurons                            1 per prediction dimension\n",
       "Hidden Activation                                       ReLU or SELU\n",
       "Output Activation  None, or ReLU/softplus. Generally tailored for...\n",
       "Loss Function                          MSE, MAE or Huber if outliers"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Typical regression MLP architecture\n",
    "pd.DataFrame.from_dict(data={\n",
    "    'Input Neurons': 'One per intput feature',\n",
    "    'Hidden Layers': 'Depends on the problem, typically 1 to 5',\n",
    "    'Neurons per Layer': 'Depends on the problem, typically 10 to 100',\n",
    "    'Output Neurons': '1 per prediction dimension',\n",
    "    'Hidden Activation': 'ReLU or SELU',\n",
    "    'Output Activation': 'None, or ReLU/softplus. Generally tailored for desired output.',\n",
    "    'Loss Function': 'MSE, MAE or Huber if outliers'\n",
    "}, orient='index').rename(columns={0: 'Typical Value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428968e-1cbf-4c2e-9fec-b864856bb097",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d448b1f-e775-446c-8363-d81acbfbdba8",
   "metadata": {},
   "source": [
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class.\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification tasks. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do no necessarily add up to 1.\n",
    "\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g. classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activiation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and they they add up to 1 (which is required if the classses are exclusive). This is called multiclass classification.\n",
    "\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc3a57c9-eb7f-49e2-b13e-a7dbda21131e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Classification</th>\n",
       "      <th>Multilabel Binary Classification</th>\n",
       "      <th>Multiclass Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input and Hidden Layers</th>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Output Neurons</th>\n",
       "      <td>1</td>\n",
       "      <td>1 per label</td>\n",
       "      <td>1 per class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Layer Activation</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>Softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Binary Classification  \\\n",
       "Input and Hidden Layers     Same as regression   \n",
       "Number of Output Neurons                     1   \n",
       "Output Layer Activation               Logistic   \n",
       "Loss Function                    Cross-Entropy   \n",
       "\n",
       "                         Multilabel Binary Classification  \\\n",
       "Input and Hidden Layers                Same as regression   \n",
       "Number of Output Neurons                      1 per label   \n",
       "Output Layer Activation                          Logistic   \n",
       "Loss Function                               Cross-Entropy   \n",
       "\n",
       "                         Multiclass Classification  \n",
       "Input and Hidden Layers         Same as regression  \n",
       "Number of Output Neurons               1 per class  \n",
       "Output Layer Activation                    Softmax  \n",
       "Loss Function                        Cross-Entropy  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typical classification MLP architecture\n",
    "pd.DataFrame.from_dict({\n",
    "    'Input and Hidden Layers': ['Same as regression', 'Same as regression', 'Same as regression'],\n",
    "    'Number of Output Neurons': ['1', '1 per label', '1 per class'],\n",
    "    'Output Layer Activation': ['Logistic', 'Logistic', 'Softmax'],\n",
    "    'Loss Function': ['Cross-Entropy', 'Cross-Entropy', 'Cross-Entropy']\n",
    "}, orient='index').rename(columns={0: 'Binary Classification', 1: 'Multilabel Binary Classification', 2: 'Multiclass Classification'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12fe7b-7870-483d-82c1-e8a7b28deb6f",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676858a2-3031-42c6-9787-d4bbbdf184df",
   "metadata": {},
   "source": [
    "At present, you can choose from 3 popular open source Deep Learning libraries:\n",
    "1. TensorFlow\n",
    "2. Microsoft Cognitive Toolkit\n",
    "3. Theano\n",
    "\n",
    "As of 2016 Keras can be run on:\n",
    "1. Apache MXNet\n",
    "2. Apple's Core ML\n",
    "3. JavaScript\n",
    "4. TypeScript\n",
    "5. PlaidML\n",
    "\n",
    "Moreover, TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage of offering some very useful extra features like TensorFlow's Data API which makes it easy to load and preprocess data efficiently.\n",
    "\n",
    "**The most populare Deep Learning library after Keras and TensorFlow is Facebook's PyTorch. The good news is that its API is quite similar to Keras's, so once you know Keras, it is not difficult to switch to PyTorch, if you ever want to.** PyTorch's popularity grew exponentially in 2018, largely thanks to its simplicity and excellent documenatation, which were not TensorFlow 1.x's main strengths. However, TensorFlow 2 is arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and its developers have greatly simplified and cleaned up the rest of the API. Similarly, PyTorch's main weaknesses (e.g. limited portability and on computation graph analysis) have been largely addressed in PyTorch 1.0. Healthy competition is beneficial to everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f219-547d-4de9-b5ba-e2b88859e781",
   "metadata": {},
   "source": [
    "## Installing TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "892d512e-e5fe-477d-b166-3b516634908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f1eba2-bf16-4aca-ad19-79b3ecfbb8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.8.0', '2.8.0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7250c-4a87-4ccf-b484-ae04ab2063c5",
   "metadata": {},
   "source": [
    "For GPU support, at the time of this writing you need to install tensorflow-gpu instead of tensorflow, but the TensorFlow team is working on having a single library that will support both CPU-only and GPU-equipped systems. You will need to install extra libraries for GPU support (see http://tensorflow.org/install for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08600ab7-c44d-4442-96d9-065e848d9845",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bcd3b-0a1c-40eb-b418-da9b865aea0a",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05841662-7dd7-4dfe-9297-fdf5761f50db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Keras dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Check dimensions and datatypes\n",
    "print(X_train_full.shape, X_train_full.dtype)\n",
    "\n",
    "# Split the training set into a validation and training set\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12a37c-7d72-41ce-b4fd-2128ab4cc8ff",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d903317a-6efb-416f-9304-29b173c2b192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,149,010\n",
      "Trainable params: 1,149,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798f669-9a92-4083-ab96-4d7e89e6fdc5",
   "metadata": {},
   "source": [
    "Let's go through this code line by line:\n",
    "1. The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "2. Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn't include the batch size, only the shape of the instances. Alternatively, you could add a keras.Layers.InputLayer as the first layer, setting input_shape=[28, 28].\n",
    "3. Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activiation function. **Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs.** It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "4. The we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "5. Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc032b-ff07-49f9-8656-b23e97af33bd",
   "metadata": {},
   "source": [
    "**Note that Dense layers often have a *lot* of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data**\n",
    "\n",
    "You can easily get a model's list of layers, to fetch a layer by its index, or you can fetch it by name. All the parameters of a layer can be accessed using its get_weights() and set_weights() methods. For a Dense layer, this includes both the connection weights and the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "48c9de87-b81a-49d1-8915-daaf5714b959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<keras.layers.core.flatten.Flatten at 0x260302df310>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302dff70>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302f74f0>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302f7eb0>,\n",
       "  <keras.layers.core.dense.Dense at 0x2602d35c1f0>,\n",
       "  <keras.layers.core.dense.Dense at 0x2602d3bc3d0>],\n",
       " 'dense_17',\n",
       " True)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers, model.layers[1].name, model.get_layer('dense_17') is model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bd71d9ee-b90c-4f7e-b291-f3c12698ef11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 500),\n",
       " (500,),\n",
       " array([[-0.02918262,  0.02894263,  0.05410552, ...,  0.02060419,\n",
       "         -0.01661743, -0.03945428],\n",
       "        [ 0.03310787, -0.02599242, -0.05342195, ..., -0.02981296,\n",
       "          0.03183615, -0.01892894],\n",
       "        [ 0.04201207, -0.03836633,  0.05245259, ..., -0.04116692,\n",
       "          0.0665056 ,  0.00915689],\n",
       "        ...,\n",
       "        [-0.0621868 , -0.00052582,  0.02977216, ..., -0.02170236,\n",
       "          0.06045925,  0.03534312],\n",
       "        [-0.00262833, -0.0545004 , -0.06271002, ...,  0.01006421,\n",
       "          0.03277665,  0.04097281],\n",
       "        [-0.05442855,  0.03501238,  0.00235659, ...,  0.01903313,\n",
       "         -0.00377643,  0.02144849]], dtype=float32))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "weights.shape, biases.shape, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19c11-771a-451e-8382-a20c3825e99b",
   "metadata": {},
   "source": [
    "The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. **Until the model is really built, the layers will not have any weights, and you will not be able to do certain things such as print the model summary or save the model.** So if you know the input shape when creating the model, it is best to specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c197eb-4248-4f09-8ff8-7532d8083f40",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9cbe-dc1c-4468-9134-9f44c7c952f7",
   "metadata": {},
   "source": [
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2756a27e-5d3c-4787-ac13-aef2f10ef28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8492c25-35cc-4e7d-8cc7-5f7316f7afb6",
   "metadata": {},
   "source": [
    "This code requires some explanation. **First, we use the 'sparse_categorical_crossentropy' loss because we have sparse labels (i.e. for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0, 0, 1, 0]) to represent class 2), then we would need to use the 'categorical_crossentropy' loss instead.**\n",
    "\n",
    "If we were doing binary classification (with one or more binary labels), then we would use the 'sigmoid' (i.e. logistic) activiation function in the output layer instead of the 'softmax' activiation function, and we would use the 'binary_crossentropy' loss.\n",
    "\n",
    "If you want to convert sparse labels (i.e. class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way around, use the np.argmax() function with axis=1.\n",
    "\n",
    "When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(learning_rate=???) to set the learning rate, rather than optimizer='sgd', which defaults to learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c95c3-46ca-4926-a2ac-b20f7252a7fc",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7178-e731-4495-b9ab-420c6f359489",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained. For this we simply need to call its fit() method. We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train. We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch. If the performance on the training set is much better than the validation set, your model is probably overfitting the training set (or there is a bug such as a data mismatch between the training set and the validation set).\n",
    "\n",
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation.\n",
    "\n",
    "**If the training set was very skewed, with some classes being overrepresented and others underrepresnted, it would be useful to set the class_weight argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes.**\n",
    "\n",
    "If you need per-instance weights, set the sample_weight argument (it supersedes class_weight). Per-instance weights could be useful if some instances were labeled by experts while others were labelled using a crowdsourcing platform: you might want to give more weight to the former.\n",
    "\n",
    "The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a pandas DataFrame and call its plot() method, you get the learning curves for the model.\n",
    "\n",
    "**If your training or validation data does not match the expected shape, you will get an exception. This is perhaps the most common error, so you should get familiar with the error message. The message is actually quite clear: for example, if you try to train this model with an array containing flattened images it will throw this error.**\n",
    "\n",
    "If the validation curves are close to the training curves, it means there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that's not the case: indeed, the validation error is computed at the *end* of each epoch, while the training error is computed using a running mean *during* each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see the that the training and validations curve overlap almost perfectly at the beginning of training.\n",
    "\n",
    "**When plotting the training curve, it should be shifted by half an epoch to the left.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a578895e-708d-41cb-89b9-4a529198aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7138 - accuracy: 0.7617 - val_loss: 0.4749 - val_accuracy: 0.8376\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4608 - accuracy: 0.8372 - val_loss: 0.4273 - val_accuracy: 0.8528\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4099 - accuracy: 0.8545 - val_loss: 0.4109 - val_accuracy: 0.8540\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3790 - accuracy: 0.8657 - val_loss: 0.3559 - val_accuracy: 0.8748\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3565 - accuracy: 0.8707 - val_loss: 0.3576 - val_accuracy: 0.8706\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3379 - accuracy: 0.8763 - val_loss: 0.3642 - val_accuracy: 0.8662\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3239 - accuracy: 0.8808 - val_loss: 0.3380 - val_accuracy: 0.8784\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.3117 - accuracy: 0.8869 - val_loss: 0.3377 - val_accuracy: 0.8764\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2981 - accuracy: 0.8909 - val_loss: 0.3246 - val_accuracy: 0.8826\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2889 - accuracy: 0.8945 - val_loss: 0.3478 - val_accuracy: 0.8702\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2780 - accuracy: 0.8980 - val_loss: 0.3161 - val_accuracy: 0.8868\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2700 - accuracy: 0.9011 - val_loss: 0.3122 - val_accuracy: 0.8872\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2624 - accuracy: 0.9041 - val_loss: 0.3174 - val_accuracy: 0.8834\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2514 - accuracy: 0.9071 - val_loss: 0.3147 - val_accuracy: 0.8866\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2446 - accuracy: 0.9100 - val_loss: 0.3158 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2372 - accuracy: 0.9124 - val_loss: 0.3233 - val_accuracy: 0.8788\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2299 - accuracy: 0.9154 - val_loss: 0.2919 - val_accuracy: 0.8996\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2226 - accuracy: 0.9189 - val_loss: 0.2863 - val_accuracy: 0.8962\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2178 - accuracy: 0.9199 - val_loss: 0.2921 - val_accuracy: 0.8948\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2097 - accuracy: 0.9233 - val_loss: 0.2981 - val_accuracy: 0.8904\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2042 - accuracy: 0.9239 - val_loss: 0.3028 - val_accuracy: 0.8928\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1991 - accuracy: 0.9260 - val_loss: 0.2949 - val_accuracy: 0.8942\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1926 - accuracy: 0.9296 - val_loss: 0.2936 - val_accuracy: 0.8928\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1877 - accuracy: 0.9314 - val_loss: 0.2908 - val_accuracy: 0.8950\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1804 - accuracy: 0.9342 - val_loss: 0.3117 - val_accuracy: 0.8886\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1761 - accuracy: 0.9352 - val_loss: 0.3133 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1711 - accuracy: 0.9370 - val_loss: 0.2956 - val_accuracy: 0.8934\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1657 - accuracy: 0.9389 - val_loss: 0.2970 - val_accuracy: 0.8974\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1609 - accuracy: 0.9409 - val_loss: 0.4269 - val_accuracy: 0.8628\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1575 - accuracy: 0.9412 - val_loss: 0.2928 - val_accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a549fd8e-cf03-424b-8523-9e45468686d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame from model history\n",
    "learning_curves = pd.DataFrame.from_dict(history.history, orient='columns')\n",
    "\n",
    "# Shift training loss back by 1 epoch for betting alignment (this is shifted left by 0.5 epochs becuase the training loss is a rolling mean during the epoch)\n",
    "learning_curves.loc[:, 'loss'] = learning_curves.loc[:, 'loss'].shift(-1)\n",
    "\n",
    "# Plot\n",
    "learning_curves.plot(ylim=(0,1), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44463a0-a3da-40c3-8959-282e301ac00c",
   "metadata": {},
   "source": [
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. **You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue trianing.** It's as simple as calling the fit() method again, since Keras just continues training where you left off.\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn't help, try another optimizer (and always return the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning the model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size.  \n",
    "\n",
    "Once you are satisfied with your model's validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this by using the evaluate() method. It is common to get slightly lower performance on the test set than the validation set, because the hyperparameters are tuned on the validation set, not the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "0cb1f3ec-82f3-440c-921d-43b79d74f00f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 62.7664 - accuracy: 0.8324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[62.76642990112305, 0.8324000239372253]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c5c23-c9ec-4d4b-8de3-3bc4a25072a9",
   "metadata": {},
   "source": [
    "### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3df9-1540-47c2-a3f2-17b3409cc2f6",
   "metadata": {},
   "source": [
    "Next, we can use the model's predict() method to make predictions on new instances. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the predict_classes() method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3f1fe7b1-1a9b-4a3b-818a-e6f5b2077160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eced6416-8f8e-4195-a6b6-81691419097d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9, 2, 1], dtype=int64),\n",
       " array(['Ankle Boot', 'Pullover', 'Trouser'], dtype='<U11'),\n",
       " array([9, 2, 1], dtype=uint8))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred, np.array(class_names)[y_pred], y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c3179-eff8-4326-9b9c-67c5ab5b2271",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f7d3a1ec-3cfb-449f-aa2f-9b64e81c8a27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.8523 - val_loss: 0.6023\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.4936\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4728 - val_loss: 0.4767\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4565 - val_loss: 0.4575\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4447\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4495\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4831 - val_loss: 0.4397\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.4501\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4243\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4222\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4102\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.4051\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.4002\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.3954\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3872 - val_loss: 0.4024\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.4033\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.3901\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3883\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3750 - val_loss: 0.3861\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3893\n",
      "162/162 [==============================] - 0s 904us/step - loss: 0.3744\n"
     ]
    }
   ],
   "source": [
    "' Lets switch to the California housing problem and tackle it using a regression neural network'\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the raw data\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Split data in train, val and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full\n",
    ")\n",
    "\n",
    "# Scale the data. Remember, only fit the scaler to the train data.  Transform the val and test data according to this fit.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Construct the TF model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01)\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Make predictions\n",
    "X_new = X_test[:3] # Pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "d2b42c19-e8fd-4dd1-98f3-c0890ddfbbd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37440621852874756,\n",
       " array([[1.6933646],\n",
       "        [1.6310713],\n",
       "        [2.6947432]], dtype=float32),\n",
       " array([1.394, 0.844, 2.209]))"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test, y_pred, y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "ce1ca00c-961f-4e65-b68b-fdab06130e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([708108.86415547, 696313.63961629, 897719.7271625 ])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(np.concatenate((y_pred, np.zeros((y_pred.shape[0], 7))), axis=1))[:, 0] * 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "8d733c74-bb03-4494-8daa-a879e445e83f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([651424.2219965 , 547281.8114281 , 805744.33947514])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.inverse_transform(np.concatenate((y_test[:3].reshape(-1, 1), np.zeros((y_test[:3].shape[0], 7))), axis=1))[:, 0] * 1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d91354-2dfc-4dcf-bdf9-80e882ed512a",
   "metadata": {},
   "source": [
    "## Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06c060-340b-4d0d-92f4-3ea291f528db",
   "metadata": {},
   "source": [
    "One example of a nonsequential neural network is a *Wide & Deep* neural network. It connects all or part of the inputs directly to the output layer. **This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).** In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by the sequence of transformations. Let's build such a neural network to tackle the California housing problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "64cfdf1a-4691-425c-906a-109d934d01b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913572c-1adb-4c13-8b56-1d0021223396",
   "metadata": {},
   "source": [
    "Let's go through each line of this code:\n",
    "1. First, we need to create an Input object. This is a specification of the kind of input the model will get, including its shape and dtype. A model may actually have multiple inputs, as we will see shortly.\n",
    "2. Next, we create a Dense layer with 30 neurons, using the ReLU activation function. **As soon as it is created, notice that we call it like a function, passing it the input.** This is why this is called the Functional API. **Note that we are just telling Keras how it should connect the layers together**, no actual data is being processed yet. \n",
    "3. We then create a second hidden layer, and again we use it as a function. **Note that we pass it the output of the first hidden layer.**\n",
    "4. Next, we create a Concatenate layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer. You may prefer the keras.layers.concatenate() function, which creates a Concatenate layer and immeidately calls it with the given inputs.\n",
    "5. Then we create the output layer, with a single neuron and no activiation function, and we call it like a function, passing it the result of the concatenation.\n",
    "6. Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path? In this case, one solution is to use multiple inputs. In this case, when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input. Alternatively, you can pass a dictionary mapping the input names to the input values, like {'wide_input': X_train_A, 'deep_input': X_train_B}. This is especially useful when there are many inputs, to avoid getting the order wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "01213a2c-b7a1-4908-9583-ff519a436535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8511 - val_loss: 0.9101\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7772 - val_loss: 0.7231\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6645 - val_loss: 0.6482\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6038\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5734 - val_loss: 0.5760\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5513 - val_loss: 0.5558\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5344 - val_loss: 0.5409\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5338\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 0.5200\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.5132\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4999 - val_loss: 0.5073\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4951 - val_loss: 0.5031\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4913 - val_loss: 0.4976\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4880 - val_loss: 0.4954\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4837 - val_loss: 0.4913\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4806 - val_loss: 0.4867\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4781 - val_loss: 0.4836\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4768 - val_loss: 0.4803\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4727 - val_loss: 0.4774\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4716 - val_loss: 0.4750\n",
      "162/162 [==============================] - 0s 810us/step - loss: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4722248911857605,\n",
       " array([[2.0295992],\n",
       "        [1.4974513],\n",
       "        [2.7036755]], dtype=float32))"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-3)\n",
    ")\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit(\n",
    "    (X_train_A, X_train_B),\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    validation_data=((X_valid_A, X_valid_B), y_valid)\n",
    ")\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "mse_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c291d-d8d8-4d29-9900-c9e11a5b1bbe",
   "metadata": {},
   "source": [
    "There are many use cases in which you may want to have multiple outputs:\n",
    "1. The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object's center, as well as its width and height) and a classification task.\n",
    "2. Similarly, you may have multiple independent tasks based on the same data. **Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are usefull across tasks.** For example, you could perform *multitask classification* on pictures of faces, using one output to classify the person's facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.\n",
    "3. Another use case is as a regularization technique (i.e. a training contraint whose objective is to reduce overfitting and thus improve the model's ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture to ensure that hte underlying part of the newtwork learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model's list of outputs. Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses. By default, Keras will compute all these losses and simply add them up to get the final loss used for training. We care much more about the main output than about the auxiliary output (as it is just used for regularization), so we want to give the main output's loss a much greater weight. Fortunately, it is possible to set all the loss weights when compiling the model. Also, now when we train the model, we need to provide labels for each output.\n",
    "\n",
    "When we evaluate the model, Keras will return the total loss, as well as the individual losses. Similarly, the predict() method will return predictions for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f224921c-7981-4150-ad2c-baeb50d9281b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8863 - main_output_loss: 0.7505 - aux_output_loss: 2.1083 - val_loss: 0.6177 - val_main_output_loss: 0.5678 - val_aux_output_loss: 1.0665\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5387 - main_output_loss: 0.4987 - aux_output_loss: 0.8988 - val_loss: 0.5469 - val_main_output_loss: 0.5176 - val_aux_output_loss: 0.8104\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5775 - main_output_loss: 0.5575 - aux_output_loss: 0.7576 - val_loss: 0.4954 - val_main_output_loss: 0.4678 - val_aux_output_loss: 0.7444\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4833 - main_output_loss: 0.4604 - aux_output_loss: 0.6898 - val_loss: 0.4643 - val_main_output_loss: 0.4394 - val_aux_output_loss: 0.6886\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4621 - main_output_loss: 0.4411 - aux_output_loss: 0.6509 - val_loss: 0.4506 - val_main_output_loss: 0.4277 - val_aux_output_loss: 0.6573\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4365 - main_output_loss: 0.4170 - aux_output_loss: 0.6121 - val_loss: 0.4589 - val_main_output_loss: 0.4413 - val_aux_output_loss: 0.6174\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4866 - main_output_loss: 0.4666 - aux_output_loss: 0.6667 - val_loss: 0.4815 - val_main_output_loss: 0.4508 - val_aux_output_loss: 0.7584\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4298 - main_output_loss: 0.4112 - aux_output_loss: 0.5968 - val_loss: 0.4342 - val_main_output_loss: 0.4188 - val_aux_output_loss: 0.5727\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4030 - main_output_loss: 0.3873 - aux_output_loss: 0.5445 - val_loss: 0.4248 - val_main_output_loss: 0.4096 - val_aux_output_loss: 0.5618\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3965 - main_output_loss: 0.3818 - aux_output_loss: 0.5288 - val_loss: 0.4003 - val_main_output_loss: 0.3828 - val_aux_output_loss: 0.5579\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3816 - main_output_loss: 0.3668 - aux_output_loss: 0.5151 - val_loss: 0.4103 - val_main_output_loss: 0.3926 - val_aux_output_loss: 0.5699\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3764 - main_output_loss: 0.3622 - aux_output_loss: 0.5050 - val_loss: 0.3812 - val_main_output_loss: 0.3659 - val_aux_output_loss: 0.5189\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3679 - main_output_loss: 0.3541 - aux_output_loss: 0.4923 - val_loss: 0.4045 - val_main_output_loss: 0.3902 - val_aux_output_loss: 0.5333\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - main_output_loss: 0.3524 - aux_output_loss: 0.4838 - val_loss: 0.3646 - val_main_output_loss: 0.3504 - val_aux_output_loss: 0.4925\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3605 - main_output_loss: 0.3478 - aux_output_loss: 0.4751 - val_loss: 0.3651 - val_main_output_loss: 0.3506 - val_aux_output_loss: 0.4957\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3585 - main_output_loss: 0.3463 - aux_output_loss: 0.4675 - val_loss: 0.3768 - val_main_output_loss: 0.3627 - val_aux_output_loss: 0.5036\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - main_output_loss: 0.3441 - aux_output_loss: 0.4639 - val_loss: 0.3634 - val_main_output_loss: 0.3509 - val_aux_output_loss: 0.4755\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3527 - main_output_loss: 0.3411 - aux_output_loss: 0.4571 - val_loss: 0.3877 - val_main_output_loss: 0.3760 - val_aux_output_loss: 0.4931\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3571 - main_output_loss: 0.3460 - aux_output_loss: 0.4568 - val_loss: 0.3544 - val_main_output_loss: 0.3421 - val_aux_output_loss: 0.4649\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - main_output_loss: 0.3398 - aux_output_loss: 0.4514 - val_loss: 0.3578 - val_main_output_loss: 0.3452 - val_aux_output_loss: 0.4709\n",
      "162/162 [==============================] - 0s 915us/step - loss: 0.3747 - main_output_loss: 0.3627 - aux_output_loss: 0.4826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3746812343597412,\n",
       " 0.36269134283065796,\n",
       " 0.4825904667377472,\n",
       " array([[1.5375495],\n",
       "        [1.0377332],\n",
       "        [2.485611 ]], dtype=float32),\n",
       " array([[1.5604802],\n",
       "        [1.1815712],\n",
       "        [2.4568267]], dtype=float32))"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B],\n",
    "    [y_test, y_test]\n",
    ")\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict(\n",
    "    [X_test_A[:3], X_test_B[:3]]\n",
    ")\n",
    "\n",
    "total_loss, main_loss, aux_loss, y_pred_main, y_pred_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d257b2-1ba4-4b9e-92d3-c32e5a685d77",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f040844-cd38-48b6-9c40-398084febfbc",
   "metadata": {},
   "source": [
    "Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages:\n",
    "1. The model can easily be saved, cloned and shared\n",
    "2. Its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early.\n",
    "3. It's fairly easy to debug, since the whole model is a static graph of layers.\n",
    "\n",
    "But the flip side is just that: it's static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you.\n",
    "\n",
    "Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. The big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations, etc. This makes it a great API for researchers experimenting with new ideas.\n",
    "\n",
    "This extra flexibility does come at a cost: your model's architecture is hidden within the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. **So unless you really need that extra flexibility, you should probably stick to the Sequential and Functional API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "29d6a212-2f59-4e81-af56-dbf7a83deedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g, name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs: tuple):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb15599-03f0-4b9f-9387-a9a78f88ac44",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2ef8e-281c-4df1-8180-4b4a5abd8460",
   "metadata": {},
   "source": [
    "Saving a trained Keras model is as simple as it gets. Keras will use the HDF5 format to save both the model's architecture (including every layer's hyperparameters) and the values of all the model parameters for every layer (e.g. connection weights and biases).. It also saves the optimizer (including its hyperparameters and any state it may have). Loading the model is just as easy.\n",
    "\n",
    "**This will work when using the Sequential API and Functional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything else yourself**\n",
    "\n",
    "But what if training lasts several hours? This is quite common, especially when training on large datasets. In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during the training, to avoid losing everything if your computer crashes. But how can you tell the fit() method to save checkpoints? **Use callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f0f24a5e-d979-4400-8db6-1e1c39c6c31a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8638 - main_output_loss: 0.7871 - aux_output_loss: 1.5546 - val_loss: 0.5497 - val_main_output_loss: 0.5025 - val_aux_output_loss: 0.9743\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4999 - main_output_loss: 0.4649 - aux_output_loss: 0.8148 - val_loss: 0.4837 - val_main_output_loss: 0.4558 - val_aux_output_loss: 0.7341\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4581 - main_output_loss: 0.4338 - aux_output_loss: 0.6765 - val_loss: 0.4524 - val_main_output_loss: 0.4297 - val_aux_output_loss: 0.6566\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4543 - main_output_loss: 0.4346 - aux_output_loss: 0.6317 - val_loss: 0.4433 - val_main_output_loss: 0.4231 - val_aux_output_loss: 0.6248\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4327 - main_output_loss: 0.4140 - aux_output_loss: 0.6007 - val_loss: 0.4293 - val_main_output_loss: 0.4113 - val_aux_output_loss: 0.5911\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4187 - main_output_loss: 0.4013 - aux_output_loss: 0.5757 - val_loss: 0.4255 - val_main_output_loss: 0.4088 - val_aux_output_loss: 0.5762\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4089 - main_output_loss: 0.3926 - aux_output_loss: 0.5560 - val_loss: 0.4026 - val_main_output_loss: 0.3851 - val_aux_output_loss: 0.5606\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3998 - main_output_loss: 0.3843 - aux_output_loss: 0.5393 - val_loss: 0.3995 - val_main_output_loss: 0.3842 - val_aux_output_loss: 0.5371\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3875 - main_output_loss: 0.3725 - aux_output_loss: 0.5225 - val_loss: 0.3900 - val_main_output_loss: 0.3742 - val_aux_output_loss: 0.5327\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3808 - main_output_loss: 0.3666 - aux_output_loss: 0.5083 - val_loss: 0.3999 - val_main_output_loss: 0.3848 - val_aux_output_loss: 0.5358\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3752 - main_output_loss: 0.3619 - aux_output_loss: 0.4955 - val_loss: 0.3938 - val_main_output_loss: 0.3790 - val_aux_output_loss: 0.5267\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - main_output_loss: 0.3581 - aux_output_loss: 0.4861 - val_loss: 0.3753 - val_main_output_loss: 0.3622 - val_aux_output_loss: 0.4933\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3673 - main_output_loss: 0.3552 - aux_output_loss: 0.4764 - val_loss: 0.3604 - val_main_output_loss: 0.3475 - val_aux_output_loss: 0.4764\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3604 - main_output_loss: 0.3487 - aux_output_loss: 0.4659 - val_loss: 0.3783 - val_main_output_loss: 0.3655 - val_aux_output_loss: 0.4939\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3703 - main_output_loss: 0.3596 - aux_output_loss: 0.4665 - val_loss: 0.3477 - val_main_output_loss: 0.3353 - val_aux_output_loss: 0.4589\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - main_output_loss: 0.3438 - aux_output_loss: 0.4531 - val_loss: 0.3584 - val_main_output_loss: 0.3468 - val_aux_output_loss: 0.4621\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3557 - main_output_loss: 0.3455 - aux_output_loss: 0.4481 - val_loss: 0.4179 - val_main_output_loss: 0.4068 - val_aux_output_loss: 0.5171\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - main_output_loss: 0.3425 - aux_output_loss: 0.4440 - val_loss: 0.3540 - val_main_output_loss: 0.3437 - val_aux_output_loss: 0.4473\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3469 - main_output_loss: 0.3370 - aux_output_loss: 0.4354 - val_loss: 0.4178 - val_main_output_loss: 0.4133 - val_aux_output_loss: 0.4580\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3457 - main_output_loss: 0.3363 - aux_output_loss: 0.4295 - val_loss: 0.3742 - val_main_output_loss: 0.3652 - val_aux_output_loss: 0.4556\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")\n",
    "\n",
    "model.save('my_keras_model.h5')\n",
    "loaded_model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "5fe76381-f4c6-4c6d-b88d-9a626e5d7926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loaded_model.get_weights()[0] == model.get_weights()[0]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b5868-c43e-4137-bcb2-0e0778caeb43",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724208a-4aa0-45a0-a75b-d31790d573ac",
   "metadata": {},
   "source": [
    "The fit() method accepts a *callbacks* argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints on your model at regular intervals during training, by default at the end of each epoch. **Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last model saved after training, and this will be the best model on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "773ec518-c363-43ec-8d01-829949b057fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7379 - main_output_loss: 0.6565 - aux_output_loss: 1.4699 - val_loss: 0.6843 - val_main_output_loss: 0.6493 - val_aux_output_loss: 0.9998\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5215 - main_output_loss: 0.4846 - aux_output_loss: 0.8538 - val_loss: 0.6326 - val_main_output_loss: 0.6110 - val_aux_output_loss: 0.8272\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4987 - main_output_loss: 0.4715 - aux_output_loss: 0.7436 - val_loss: 0.4583 - val_main_output_loss: 0.4323 - val_aux_output_loss: 0.6917\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4548 - main_output_loss: 0.4325 - aux_output_loss: 0.6548 - val_loss: 0.4411 - val_main_output_loss: 0.4204 - val_aux_output_loss: 0.6273\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4393 - main_output_loss: 0.4218 - aux_output_loss: 0.5966 - val_loss: 0.4360 - val_main_output_loss: 0.4174 - val_aux_output_loss: 0.6032\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4208 - main_output_loss: 0.4049 - aux_output_loss: 0.5641 - val_loss: 0.4118 - val_main_output_loss: 0.3946 - val_aux_output_loss: 0.5664\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4399 - main_output_loss: 0.4282 - aux_output_loss: 0.5448 - val_loss: 0.5312 - val_main_output_loss: 0.5159 - val_aux_output_loss: 0.6694\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4145 - main_output_loss: 0.4018 - aux_output_loss: 0.5292 - val_loss: 0.3904 - val_main_output_loss: 0.3762 - val_aux_output_loss: 0.5183\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3959 - main_output_loss: 0.3843 - aux_output_loss: 0.4999 - val_loss: 0.4063 - val_main_output_loss: 0.3921 - val_aux_output_loss: 0.5335\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4120 - main_output_loss: 0.4034 - aux_output_loss: 0.4889 - val_loss: 0.3797 - val_main_output_loss: 0.3669 - val_aux_output_loss: 0.4946\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3758 - main_output_loss: 0.3649 - aux_output_loss: 0.4735 - val_loss: 0.4231 - val_main_output_loss: 0.4088 - val_aux_output_loss: 0.5525\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3724 - main_output_loss: 0.3623 - aux_output_loss: 0.4638 - val_loss: 0.3799 - val_main_output_loss: 0.3684 - val_aux_output_loss: 0.4838\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3654 - main_output_loss: 0.3557 - aux_output_loss: 0.4530 - val_loss: 0.3747 - val_main_output_loss: 0.3649 - val_aux_output_loss: 0.4620\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3707 - main_output_loss: 0.3619 - aux_output_loss: 0.4499 - val_loss: 0.3792 - val_main_output_loss: 0.3675 - val_aux_output_loss: 0.4844\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3578 - main_output_loss: 0.3488 - aux_output_loss: 0.4386 - val_loss: 0.3576 - val_main_output_loss: 0.3466 - val_aux_output_loss: 0.4563\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3559 - main_output_loss: 0.3471 - aux_output_loss: 0.4359 - val_loss: 0.3512 - val_main_output_loss: 0.3411 - val_aux_output_loss: 0.4421\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3618 - main_output_loss: 0.3541 - aux_output_loss: 0.4312 - val_loss: 0.3744 - val_main_output_loss: 0.3647 - val_aux_output_loss: 0.4609\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3533 - main_output_loss: 0.3449 - aux_output_loss: 0.4292 - val_loss: 0.3487 - val_main_output_loss: 0.3375 - val_aux_output_loss: 0.4490\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3460 - main_output_loss: 0.3379 - aux_output_loss: 0.4185 - val_loss: 0.3566 - val_main_output_loss: 0.3478 - val_aux_output_loss: 0.4357\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - main_output_loss: 0.3422 - aux_output_loss: 0.4199 - val_loss: 0.3522 - val_main_output_loss: 0.3416 - val_aux_output_loss: 0.4472\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'my_keras_model.h5',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "history = keras.models.load_model('my_keras_model.h5') # roll back to best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e10610-8664-4a65-938d-6654dc0bdb78",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "791aad15-00bd-447b-831a-18d37b8074b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8121 - main_output_loss: 0.7091 - aux_output_loss: 1.7387 - val_loss: 0.8932 - val_main_output_loss: 0.8670 - val_aux_output_loss: 1.1288\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5760 - main_output_loss: 0.5419 - aux_output_loss: 0.8824 - val_loss: 0.6026 - val_main_output_loss: 0.5793 - val_aux_output_loss: 0.8122\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4925 - main_output_loss: 0.4679 - aux_output_loss: 0.7138 - val_loss: 0.4723 - val_main_output_loss: 0.4476 - val_aux_output_loss: 0.6953\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4631 - main_output_loss: 0.4422 - aux_output_loss: 0.6511 - val_loss: 0.4523 - val_main_output_loss: 0.4304 - val_aux_output_loss: 0.6496\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4436 - main_output_loss: 0.4247 - aux_output_loss: 0.6140 - val_loss: 0.4378 - val_main_output_loss: 0.4167 - val_aux_output_loss: 0.6274\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4282 - main_output_loss: 0.4108 - aux_output_loss: 0.5846 - val_loss: 0.4357 - val_main_output_loss: 0.4181 - val_aux_output_loss: 0.5943\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4151 - main_output_loss: 0.3991 - aux_output_loss: 0.5594 - val_loss: 0.4185 - val_main_output_loss: 0.4021 - val_aux_output_loss: 0.5665\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4027 - main_output_loss: 0.3877 - aux_output_loss: 0.5383 - val_loss: 0.3980 - val_main_output_loss: 0.3815 - val_aux_output_loss: 0.5466\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3936 - main_output_loss: 0.3794 - aux_output_loss: 0.5212 - val_loss: 0.3997 - val_main_output_loss: 0.3834 - val_aux_output_loss: 0.5459\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - main_output_loss: 0.3747 - aux_output_loss: 0.5049 - val_loss: 0.3846 - val_main_output_loss: 0.3703 - val_aux_output_loss: 0.5133\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3789 - main_output_loss: 0.3664 - aux_output_loss: 0.4909 - val_loss: 0.3664 - val_main_output_loss: 0.3519 - val_aux_output_loss: 0.4974\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3756 - main_output_loss: 0.3639 - aux_output_loss: 0.4808 - val_loss: 0.4147 - val_main_output_loss: 0.4046 - val_aux_output_loss: 0.5058\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3704 - main_output_loss: 0.3595 - aux_output_loss: 0.4683 - val_loss: 0.3856 - val_main_output_loss: 0.3752 - val_aux_output_loss: 0.4793\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3674 - main_output_loss: 0.3572 - aux_output_loss: 0.4594 - val_loss: 0.4060 - val_main_output_loss: 0.3960 - val_aux_output_loss: 0.4959\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3645 - main_output_loss: 0.3547 - aux_output_loss: 0.4528 - val_loss: 0.4062 - val_main_output_loss: 0.3971 - val_aux_output_loss: 0.4881\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3542 - main_output_loss: 0.3442 - aux_output_loss: 0.4446 - val_loss: 0.4161 - val_main_output_loss: 0.4093 - val_aux_output_loss: 0.4775\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3510 - main_output_loss: 0.3414 - aux_output_loss: 0.4377 - val_loss: 0.3682 - val_main_output_loss: 0.3564 - val_aux_output_loss: 0.4750\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3571 - main_output_loss: 0.3484 - aux_output_loss: 0.4356 - val_loss: 0.3465 - val_main_output_loss: 0.3362 - val_aux_output_loss: 0.4393\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3464 - main_output_loss: 0.3374 - aux_output_loss: 0.4267 - val_loss: 0.3343 - val_main_output_loss: 0.3234 - val_aux_output_loss: 0.4324\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3449 - main_output_loss: 0.3362 - aux_output_loss: 0.4229 - val_loss: 0.3495 - val_main_output_loss: 0.3399 - val_aux_output_loss: 0.4356\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3427 - main_output_loss: 0.3342 - aux_output_loss: 0.4186 - val_loss: 0.3612 - val_main_output_loss: 0.3519 - val_aux_output_loss: 0.4447\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3409 - main_output_loss: 0.3327 - aux_output_loss: 0.4147 - val_loss: 0.3418 - val_main_output_loss: 0.3315 - val_aux_output_loss: 0.4348\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3392 - main_output_loss: 0.3312 - aux_output_loss: 0.4121 - val_loss: 0.3309 - val_main_output_loss: 0.3216 - val_aux_output_loss: 0.4150\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3366 - main_output_loss: 0.3287 - aux_output_loss: 0.4085 - val_loss: 0.3655 - val_main_output_loss: 0.3563 - val_aux_output_loss: 0.4480\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3360 - main_output_loss: 0.3281 - aux_output_loss: 0.4070 - val_loss: 0.3338 - val_main_output_loss: 0.3249 - val_aux_output_loss: 0.4134\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3327 - main_output_loss: 0.3250 - aux_output_loss: 0.4015 - val_loss: 0.3726 - val_main_output_loss: 0.3640 - val_aux_output_loss: 0.4499\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - main_output_loss: 0.3242 - aux_output_loss: 0.4014 - val_loss: 0.3853 - val_main_output_loss: 0.3783 - val_aux_output_loss: 0.4483\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3305 - main_output_loss: 0.3231 - aux_output_loss: 0.3970 - val_loss: 0.3243 - val_main_output_loss: 0.3157 - val_aux_output_loss: 0.4016\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3293 - main_output_loss: 0.3217 - aux_output_loss: 0.3972 - val_loss: 0.3734 - val_main_output_loss: 0.3651 - val_aux_output_loss: 0.4483\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - main_output_loss: 0.3242 - aux_output_loss: 0.3973 - val_loss: 0.3327 - val_main_output_loss: 0.3240 - val_aux_output_loss: 0.4105\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3296 - main_output_loss: 0.3224 - aux_output_loss: 0.3945 - val_loss: 0.3429 - val_main_output_loss: 0.3352 - val_aux_output_loss: 0.4115\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3268 - main_output_loss: 0.3196 - aux_output_loss: 0.3913 - val_loss: 0.4056 - val_main_output_loss: 0.3976 - val_aux_output_loss: 0.4773\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3254 - main_output_loss: 0.3183 - aux_output_loss: 0.3891 - val_loss: 0.3257 - val_main_output_loss: 0.3177 - val_aux_output_loss: 0.3975\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3231 - main_output_loss: 0.3160 - aux_output_loss: 0.3872 - val_loss: 0.3355 - val_main_output_loss: 0.3270 - val_aux_output_loss: 0.4119\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3254 - main_output_loss: 0.3183 - aux_output_loss: 0.3894 - val_loss: 0.3233 - val_main_output_loss: 0.3156 - val_aux_output_loss: 0.3931\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3230 - main_output_loss: 0.3159 - aux_output_loss: 0.3866 - val_loss: 0.3171 - val_main_output_loss: 0.3094 - val_aux_output_loss: 0.3864\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3222 - main_output_loss: 0.3150 - aux_output_loss: 0.3865 - val_loss: 0.3258 - val_main_output_loss: 0.3189 - val_aux_output_loss: 0.3879\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3192 - main_output_loss: 0.3121 - aux_output_loss: 0.3826 - val_loss: 0.3425 - val_main_output_loss: 0.3360 - val_aux_output_loss: 0.4010\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3197 - main_output_loss: 0.3127 - aux_output_loss: 0.3829 - val_loss: 0.3365 - val_main_output_loss: 0.3303 - val_aux_output_loss: 0.3923\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3175 - main_output_loss: 0.3108 - aux_output_loss: 0.3781 - val_loss: 0.3631 - val_main_output_loss: 0.3595 - val_aux_output_loss: 0.3956\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3202 - main_output_loss: 0.3137 - aux_output_loss: 0.3793 - val_loss: 0.3461 - val_main_output_loss: 0.3389 - val_aux_output_loss: 0.4114\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - main_output_loss: 0.3137 - aux_output_loss: 0.3775 - val_loss: 0.3274 - val_main_output_loss: 0.3203 - val_aux_output_loss: 0.3913\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3165 - main_output_loss: 0.3099 - aux_output_loss: 0.3755 - val_loss: 0.3103 - val_main_output_loss: 0.3029 - val_aux_output_loss: 0.3763\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3152 - main_output_loss: 0.3087 - aux_output_loss: 0.3744 - val_loss: 0.3129 - val_main_output_loss: 0.3053 - val_aux_output_loss: 0.3811\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3145 - main_output_loss: 0.3079 - aux_output_loss: 0.3746 - val_loss: 0.3163 - val_main_output_loss: 0.3096 - val_aux_output_loss: 0.3764\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3142 - main_output_loss: 0.3076 - aux_output_loss: 0.3731 - val_loss: 0.3128 - val_main_output_loss: 0.3056 - val_aux_output_loss: 0.3778\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3136 - main_output_loss: 0.3071 - aux_output_loss: 0.3722 - val_loss: 0.3846 - val_main_output_loss: 0.3819 - val_aux_output_loss: 0.4093\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3135 - main_output_loss: 0.3072 - aux_output_loss: 0.3709 - val_loss: 0.3176 - val_main_output_loss: 0.3114 - val_aux_output_loss: 0.3741\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3131 - main_output_loss: 0.3067 - aux_output_loss: 0.3714 - val_loss: 0.3152 - val_main_output_loss: 0.3086 - val_aux_output_loss: 0.3746\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3120 - main_output_loss: 0.3057 - aux_output_loss: 0.3692 - val_loss: 0.3170 - val_main_output_loss: 0.3102 - val_aux_output_loss: 0.3785\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3091 - main_output_loss: 0.3026 - aux_output_loss: 0.3672 - val_loss: 0.3114 - val_main_output_loss: 0.3044 - val_aux_output_loss: 0.3744\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3102 - main_output_loss: 0.3038 - aux_output_loss: 0.3678 - val_loss: 0.3261 - val_main_output_loss: 0.3190 - val_aux_output_loss: 0.3906\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3087 - main_output_loss: 0.3023 - aux_output_loss: 0.3662 - val_loss: 0.3285 - val_main_output_loss: 0.3223 - val_aux_output_loss: 0.3846\n"
     ]
    }
   ],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=100,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85f324-e3d5-4376-8901-1de1091a2537",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g. detect over-fitting).\n",
    "\n",
    "Callbacks can also be used during evaluation and predictions, should you ever need them. For evaluation, you should implement on on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end(). These are called by evaluate().  For prediction you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end(). These are called by predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "b96140fa-37a0-4f06-a819-bf72bcada0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {:2f}'.format(logs['val_loss'] / logs['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd83c1-441a-423b-b4d2-ce77a78bbdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(\n",
    "    loss=['mse', 'mse'],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.02)\n",
    ")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "custom_cb = \n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_A, X_train_B],\n",
    "    [y_train, y_train],\n",
    "    epochs=100,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]),\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
