{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65c1fb2-13d3-4874-8bf7-f0d15fc91460",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36304c-e6df-4a5d-870d-f4a956b3e20d",
   "metadata": {},
   "source": [
    "*Artificial Nerual Networks* (ANNs) are Machine Learning models inspired by the networks of biological neurons found in our brains.  ANNs are at the very core of Deep Learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex Machine Learning tasks such as classifying billions of images, powering speech recognition services, recommending the best videos to watch to hundreds of millions of users every day, or learning to beat the world champion at a game of Go.\n",
    "\n",
    "The first part of this chapter introduces artificial neural networks.  In the second part, we will look at how to implement neural networks using the popular Keras API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6790a-e40d-4918-8d95-811490cae7d0",
   "metadata": {},
   "source": [
    "## From Biological to Artificial Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8e0e0-3025-49c4-a5fc-eba8bdb03116",
   "metadata": {},
   "source": [
    "The early successes of ANNs led to widespread belief that we would soon be conversing with truly intelligent machines.  When it became clear in the 1960's that this promise would go unfulfilled, funding flew elsewhere, and ANNs entered a long winter.  In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in *cnnectionism* (the study of neural networks).  But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as Support Vector Machines.  These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs, and this time is different.\n",
    "* There are now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "* The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time.\n",
    "* The training algorithms have been improved.\n",
    "* ANNs seem to have entered a virtuous cirle of funding and progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e276296-1296-4911-a4d9-6c2311f9e061",
   "metadata": {},
   "source": [
    "## The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca2a95-d509-45d0-8013-5c266022aa6f",
   "metadata": {},
   "source": [
    "The *Perceptron* is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt.  It is based on a *threshold logic unit* (TLU). For a TLU, the inputs and outputs are numbers, and each input connection is associated with a weight.  The TLU computes a weighted sum of its inputs, then applies a *step function* to that sum and outputs the results.\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.  When all the neurons in a layer are connected to every neuron in the previous layer, the layer is called a *fully connected layer*, or a *dense layer*.\n",
    "\n",
    "Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently compute the outputs of a layer of artificial neurons for several instances at once.\n",
    "\n",
    "<c> Equation 10-2: Computing the outputs of a fully connected layer </c>\n",
    "$$ h_{W, b}(X) = \\phi(XW + b) $$\n",
    "\n",
    "So how is a Perceptron trained? \"Cells that fire together, wire together.\" Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the Perceptron learning rule reinforces connections that help reduce the error.  More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions.  For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would ahve contributed to the correct prediction.  \n",
    "\n",
    "**The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns (just like Logistic Regression classifiers).**\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that implements a single-TLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a1171-8787-4eee-b98c-910112da8752",
   "metadata": {},
   "source": [
    "#### Example 1: Scikit-Learns Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02695eec-266a-414e-bfe0-399da24a9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74792a2-b38a-4233-bea2-c5b015bcaed3",
   "metadata": {},
   "source": [
    "You may have noticed that the Perceptron learning algorithm strongly resembles Stochastic Gradient Descent.  In fact, Scikit-Learn's Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss = 'perceptron', learning_rate = 'constant', eta0 = 1 (the learning rate), and penalty = None (no regularization).\n",
    "\n",
    "Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class probability.  This is one reason to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "There are a number of significant weaknesses of Perceptrons, in particular that they are incapable of solving some trivial problems.  It turns out that some of the limitations of Perceptrons an be eliminated by stacking multiple Perceptrons.  The resulting ANN is called a *Multilayer Perceptron* (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa583da-d07a-4ce9-8371-d0c91b615b24",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f77643-166f-40b3-9179-ddc1ca9dba94",
   "metadata": {},
   "source": [
    "An MLP is composed of one (passthrough) *input layer*, one or more layers of TLUs, called *hidden layers*, and one final layer of TLUs, called the *output layer*.  The layers close to the input layers are usually called the *lower layers* and the ones close to the outputs are usually called the *upper layers*.  Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "The signal flows only in one direction (from the inputs to the oupts), so this architecture is an example of a *feedforward neural network* (FNN).\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers (dozens or hundreds), it is called a *deep neural network* (DNN).  For many years researchers struggled to find a way to train MLPs, without success, until 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking paper that introduced the *backpropagation* training algorithm, which is still used today.\n",
    "\n",
    "In short, it is Gradient Descent using an efficient technique for computing the gradients automatically: in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single model parameter.  In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error.  Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
    "\n",
    "Automatically computing gradients is called *automatic differentiation* or *autodiff*.  There are various autodiff techniques, with different pros and cons.  The one used by backpropagation is called *reverse-moode autodiff*.  It is fase and precise, and is well suited when the function to differentiate has many variables and few outputs.\n",
    "\n",
    "Let's run through the algorithm in a bit more detail:\n",
    "* It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times.  Each pass is called an *epoch*\n",
    "* Each mini-batch is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the *forward pass*: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "* Next, the algorithm measures the network's output error (i.e. it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "* Then it computes how much each output connection contributed to the error. This is done analytically by applying the *chain rule* (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "* The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer.  As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network.\n",
    "* Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "**This algorithm is so important that it's worth summarizing it again: for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step)**\n",
    "\n",
    "***It is important to initialize all the hidden layers' connection weights randomly, or else training will fail.***\n",
    "\n",
    "In order for this algorithm to work properly, its authors made a key change to the MLP's architecture: they replaced the step function with the logistic (sigmoid) function:\n",
    "$$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} $$\n",
    "\n",
    "The backpropagation algorithm works well with many other activation functions.  Here are two other popular choices:\n",
    "1. **The hyperbolic tanger function: tanh(z)**: $2\\sigma(2z) - 1$\n",
    "<br>Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from -1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer's output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "2. **The Rectified Linear Unit function: ReLU(z)**: $max(0, z)$\n",
    "<br>The ReLU function is continuous but unfortunatley not differentiable at z=0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default. Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.\n",
    "\n",
    "Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. So if you don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can't solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86031eec-79e9-4ee4-8f9e-5c9a26005638",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7255039-12e6-4a50-856a-0a7129a3d08d",
   "metadata": {},
   "source": [
    "First, MLPs can be used for regression tasks. If you want to predict a single value (like the value of a house), then you just need a single output neuron: its output is the predicted value. For multivariate regression (predicting multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D cooridinates, so you need two output neurons.\n",
    "\n",
    "**In general, when building an MLP for regression, you do not want to use any activation function for the output neurons, so they are free to output any range of values.** If you want to guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer. Alternatively, you can use the *softplus* activation function, which is a smooth variant of ReLU: $softplus(z) = log(1 + \\exp(z))$\n",
    "\n",
    "The loss function to use during training is typically the mean squared error, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you can us ethe Huber loss, which is a combination of both.\n",
    "\n",
    "The Huber loss is quadratic when the error is smaller than a threshold $\\delta$ (typically 1) but linear when the error is larger than $\\delta$. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bad854-01f6-4371-bce5-759b19a1caae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Typical Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input Neurons</th>\n",
       "      <td>One per intput feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Layers</th>\n",
       "      <td>Depends on the problem, typically 1 to 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurons per Layer</th>\n",
       "      <td>Depends on the problem, typically 10 to 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Neurons</th>\n",
       "      <td>1 per prediction dimension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hidden Activation</th>\n",
       "      <td>ReLU or SELU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Activation</th>\n",
       "      <td>None, or ReLU/softplus. Generally tailored for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>MSE, MAE or Huber if outliers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Typical Value\n",
       "Input Neurons                                 One per intput feature\n",
       "Hidden Layers               Depends on the problem, typically 1 to 5\n",
       "Neurons per Layer        Depends on the problem, typically 10 to 100\n",
       "Output Neurons                            1 per prediction dimension\n",
       "Hidden Activation                                       ReLU or SELU\n",
       "Output Activation  None, or ReLU/softplus. Generally tailored for...\n",
       "Loss Function                          MSE, MAE or Huber if outliers"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Typical regression MLP architecture\n",
    "pd.DataFrame.from_dict(data={\n",
    "    'Input Neurons': 'One per intput feature',\n",
    "    'Hidden Layers': 'Depends on the problem, typically 1 to 5',\n",
    "    'Neurons per Layer': 'Depends on the problem, typically 10 to 100',\n",
    "    'Output Neurons': '1 per prediction dimension',\n",
    "    'Hidden Activation': 'ReLU or SELU',\n",
    "    'Output Activation': 'None, or ReLU/softplus. Generally tailored for desired output.',\n",
    "    'Loss Function': 'MSE, MAE or Huber if outliers'\n",
    "}, orient='index').rename(columns={0: 'Typical Value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428968e-1cbf-4c2e-9fec-b864856bb097",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d448b1f-e775-446c-8363-d81acbfbdba8",
   "metadata": {},
   "source": [
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class.\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification tasks. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do no necessarily add up to 1.\n",
    "\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g. classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activiation function for the whole output layer. The softmax function will ensure that all the estimated probabilities are between 0 and 1 and they they add up to 1 (which is required if the classses are exclusive). This is called multiclass classification.\n",
    "\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (also called the log loss) is generally a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc3a57c9-eb7f-49e2-b13e-a7dbda21131e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Binary Classification</th>\n",
       "      <th>Multilabel Binary Classification</th>\n",
       "      <th>Multiclass Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Input and Hidden Layers</th>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "      <td>Same as regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Output Neurons</th>\n",
       "      <td>1</td>\n",
       "      <td>1 per label</td>\n",
       "      <td>1 per class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Output Layer Activation</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>Logistic</td>\n",
       "      <td>Softmax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Function</th>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "      <td>Cross-Entropy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Binary Classification  \\\n",
       "Input and Hidden Layers     Same as regression   \n",
       "Number of Output Neurons                     1   \n",
       "Output Layer Activation               Logistic   \n",
       "Loss Function                    Cross-Entropy   \n",
       "\n",
       "                         Multilabel Binary Classification  \\\n",
       "Input and Hidden Layers                Same as regression   \n",
       "Number of Output Neurons                      1 per label   \n",
       "Output Layer Activation                          Logistic   \n",
       "Loss Function                               Cross-Entropy   \n",
       "\n",
       "                         Multiclass Classification  \n",
       "Input and Hidden Layers         Same as regression  \n",
       "Number of Output Neurons               1 per class  \n",
       "Output Layer Activation                    Softmax  \n",
       "Loss Function                        Cross-Entropy  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typical classification MLP architecture\n",
    "pd.DataFrame.from_dict({\n",
    "    'Input and Hidden Layers': ['Same as regression', 'Same as regression', 'Same as regression'],\n",
    "    'Number of Output Neurons': ['1', '1 per label', '1 per class'],\n",
    "    'Output Layer Activation': ['Logistic', 'Logistic', 'Softmax'],\n",
    "    'Loss Function': ['Cross-Entropy', 'Cross-Entropy', 'Cross-Entropy']\n",
    "}, orient='index').rename(columns={0: 'Binary Classification', 1: 'Multilabel Binary Classification', 2: 'Multiclass Classification'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12fe7b-7870-483d-82c1-e8a7b28deb6f",
   "metadata": {},
   "source": [
    "## Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676858a2-3031-42c6-9787-d4bbbdf184df",
   "metadata": {},
   "source": [
    "At present, you can choose from 3 popular open source Deep Learning libraries:\n",
    "1. TensorFlow\n",
    "2. Microsoft Cognitive Toolkit\n",
    "3. Theano\n",
    "\n",
    "As of 2016 Keras can be run on:\n",
    "1. Apache MXNet\n",
    "2. Apple's Core ML\n",
    "3. JavaScript\n",
    "4. TypeScript\n",
    "5. PlaidML\n",
    "\n",
    "Moreover, TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage of offering some very useful extra features like TensorFlow's Data API which makes it easy to load and preprocess data efficiently.\n",
    "\n",
    "**The most populare Deep Learning library after Keras and TensorFlow is Facebook's PyTorch. The good news is that its API is quite similar to Keras's, so once you know Keras, it is not difficult to switch to PyTorch, if you ever want to.** PyTorch's popularity grew exponentially in 2018, largely thanks to its simplicity and excellent documenatation, which were not TensorFlow 1.x's main strengths. However, TensorFlow 2 is arguably just as simple as PyTorch, as it has adopted Keras as its official high-level API and its developers have greatly simplified and cleaned up the rest of the API. Similarly, PyTorch's main weaknesses (e.g. limited portability and on computation graph analysis) have been largely addressed in PyTorch 1.0. Healthy competition is beneficial to everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f219-547d-4de9-b5ba-e2b88859e781",
   "metadata": {},
   "source": [
    "## Installing TensorFlow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "892d512e-e5fe-477d-b166-3b516634908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f1eba2-bf16-4aca-ad19-79b3ecfbb8da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.8.0', '2.8.0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7250c-4a87-4ccf-b484-ae04ab2063c5",
   "metadata": {},
   "source": [
    "For GPU support, at the time of this writing you need to install tensorflow-gpu instead of tensorflow, but the TensorFlow team is working on having a single library that will support both CPU-only and GPU-equipped systems. You will need to install extra libraries for GPU support (see http://tensorflow.org/install for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08600ab7-c44d-4442-96d9-065e848d9845",
   "metadata": {},
   "source": [
    "## Building an Image Classifier Using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238bcd3b-0a1c-40eb-b418-da9b865aea0a",
   "metadata": {},
   "source": [
    "### Using Keras to load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05841662-7dd7-4dfe-9297-fdf5761f50db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Keras dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Check dimensions and datatypes\n",
    "print(X_train_full.shape, X_train_full.dtype)\n",
    "\n",
    "# Split the training set into a validation and training set\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12a37c-7d72-41ce-b4fd-2128ab4cc8ff",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d903317a-6efb-416f-9304-29b173c2b192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 500)               392500    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 500)               250500    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,149,010\n",
      "Trainable params: 1,149,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798f669-9a92-4083-ab96-4d7e89e6fdc5",
   "metadata": {},
   "source": [
    "Let's go through this code line by line:\n",
    "1. The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "2. Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 1). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn't include the batch size, only the shape of the instances. Alternatively, you could add a keras.Layers.InputLayer as the first layer, setting input_shape=[28, 28].\n",
    "3. Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activiation function. **Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs.** It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "4. The we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "5. Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc032b-ff07-49f9-8656-b23e97af33bd",
   "metadata": {},
   "source": [
    "**Note that Dense layers often have a *lot* of parameters. This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data**\n",
    "\n",
    "You can easily get a model's list of layers, to fetch a layer by its index, or you can fetch it by name. All the parameters of a layer can be accessed using its get_weights() and set_weights() methods. For a Dense layer, this includes both the connection weights and the bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "48c9de87-b81a-49d1-8915-daaf5714b959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<keras.layers.core.flatten.Flatten at 0x260302df310>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302dff70>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302f74f0>,\n",
       "  <keras.layers.core.dense.Dense at 0x260302f7eb0>,\n",
       "  <keras.layers.core.dense.Dense at 0x2602d35c1f0>,\n",
       "  <keras.layers.core.dense.Dense at 0x2602d3bc3d0>],\n",
       " 'dense_17',\n",
       " True)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers, model.layers[1].name, model.get_layer('dense_17') is model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bd71d9ee-b90c-4f7e-b291-f3c12698ef11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 500),\n",
       " (500,),\n",
       " array([[-0.02918262,  0.02894263,  0.05410552, ...,  0.02060419,\n",
       "         -0.01661743, -0.03945428],\n",
       "        [ 0.03310787, -0.02599242, -0.05342195, ..., -0.02981296,\n",
       "          0.03183615, -0.01892894],\n",
       "        [ 0.04201207, -0.03836633,  0.05245259, ..., -0.04116692,\n",
       "          0.0665056 ,  0.00915689],\n",
       "        ...,\n",
       "        [-0.0621868 , -0.00052582,  0.02977216, ..., -0.02170236,\n",
       "          0.06045925,  0.03534312],\n",
       "        [-0.00262833, -0.0545004 , -0.06271002, ...,  0.01006421,\n",
       "          0.03277665,  0.04097281],\n",
       "        [-0.05442855,  0.03501238,  0.00235659, ...,  0.01903313,\n",
       "         -0.00377643,  0.02144849]], dtype=float32))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "weights.shape, biases.shape, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e19c11-771a-451e-8382-a20c3825e99b",
   "metadata": {},
   "source": [
    "The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. **Until the model is really built, the layers will not have any weights, and you will not be able to do certain things such as print the model summary or save the model.** So if you know the input shape when creating the model, it is best to specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c197eb-4248-4f09-8ff8-7532d8083f40",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba9cbe-dc1c-4468-9134-9f44c7c952f7",
   "metadata": {},
   "source": [
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2756a27e-5d3c-4787-ac13-aef2f10ef28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8492c25-35cc-4e7d-8cc7-5f7316f7afb6",
   "metadata": {},
   "source": [
    "This code requires some explanation. **First, we use the 'sparse_categorical_crossentropy' loss because we have sparse labels (i.e. for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g. [0, 0, 1, 0]) to represent class 2), then we would need to use the 'categorical_crossentropy' loss instead.**\n",
    "\n",
    "If we were doing binary classification (with one or more binary labels), then we would use the 'sigmoid' (i.e. logistic) activiation function in the output layer instead of the 'softmax' activiation function, and we would use the 'binary_crossentropy' loss.\n",
    "\n",
    "If you want to convert sparse labels (i.e. class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way around, use the np.argmax() function with axis=1.\n",
    "\n",
    "When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(learning_rate=???) to set the learning rate, rather than optimizer='sgd', which defaults to learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c95c3-46ca-4926-a2ac-b20f7252a7fc",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa7178-e731-4495-b9ab-420c6f359489",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained. For this we simply need to call its fit() method. We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train. We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch. If the performance on the training set is much better than the validation set, your model is probably overfitting the training set (or there is a bug such as a data mismatch between the training set and the validation set).\n",
    "\n",
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation.\n",
    "\n",
    "**If the training set was very skewed, with some classes being overrepresented and others underrepresnted, it would be useful to set the class_weight argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes.**\n",
    "\n",
    "If you need per-instance weights, set the sample_weight argument (it supersedes class_weight). Per-instance weights could be useful if some instances were labeled by experts while others were labelled using a crowdsourcing platform: you might want to give more weight to the former.\n",
    "\n",
    "The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a pandas DataFrame and call its plot() method, you get the learning curves for the model.\n",
    "\n",
    "**If your training or validation data does not match the expected shape, you will get an exception. This is perhaps the most common error, so you should get familiar with the error message. The message is actually quite clear: for example, if you try to train this model with an array containing flattened images it will throw this error.**\n",
    "\n",
    "If the validation curves are close to the training curves, it means there is not too much overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training. But that's not the case: indeed, the validation error is computed at the *end* of each epoch, while the training error is computed using a running mean *during* each epoch. So the training curve should be shifted by half an epoch to the left. If you do that, you will see the that the training and validations curve overlap almost perfectly at the beginning of training.\n",
    "\n",
    "**When plotting the training curve, it should be shifted by half an epoch to the left.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578895e-708d-41cb-89b9-4a529198aeb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7138 - accuracy: 0.7617 - val_loss: 0.4749 - val_accuracy: 0.8376\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4608 - accuracy: 0.8372 - val_loss: 0.4273 - val_accuracy: 0.8528\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4099 - accuracy: 0.8545 - val_loss: 0.4109 - val_accuracy: 0.8540\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3790 - accuracy: 0.8657 - val_loss: 0.3559 - val_accuracy: 0.8748\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3565 - accuracy: 0.8707 - val_loss: 0.3576 - val_accuracy: 0.8706\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3379 - accuracy: 0.8763 - val_loss: 0.3642 - val_accuracy: 0.8662\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3239 - accuracy: 0.8808 - val_loss: 0.3380 - val_accuracy: 0.8784\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.3117 - accuracy: 0.8869 - val_loss: 0.3377 - val_accuracy: 0.8764\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2981 - accuracy: 0.8909 - val_loss: 0.3246 - val_accuracy: 0.8826\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2889 - accuracy: 0.8945 - val_loss: 0.3478 - val_accuracy: 0.8702\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2780 - accuracy: 0.8980 - val_loss: 0.3161 - val_accuracy: 0.8868\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2700 - accuracy: 0.9011 - val_loss: 0.3122 - val_accuracy: 0.8872\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2624 - accuracy: 0.9041 - val_loss: 0.3174 - val_accuracy: 0.8834\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2514 - accuracy: 0.9071 - val_loss: 0.3147 - val_accuracy: 0.8866\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2446 - accuracy: 0.9100 - val_loss: 0.3158 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2372 - accuracy: 0.9124 - val_loss: 0.3233 - val_accuracy: 0.8788\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2299 - accuracy: 0.9154 - val_loss: 0.2919 - val_accuracy: 0.8996\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2226 - accuracy: 0.9189 - val_loss: 0.2863 - val_accuracy: 0.8962\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2178 - accuracy: 0.9199 - val_loss: 0.2921 - val_accuracy: 0.8948\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2097 - accuracy: 0.9233 - val_loss: 0.2981 - val_accuracy: 0.8904\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2042 - accuracy: 0.9239 - val_loss: 0.3028 - val_accuracy: 0.8928\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1991 - accuracy: 0.9260 - val_loss: 0.2949 - val_accuracy: 0.8942\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1926 - accuracy: 0.9296 - val_loss: 0.2936 - val_accuracy: 0.8928\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1877 - accuracy: 0.9314 - val_loss: 0.2908 - val_accuracy: 0.8950\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1804 - accuracy: 0.9342 - val_loss: 0.3117 - val_accuracy: 0.8886\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1761 - accuracy: 0.9352 - val_loss: 0.3133 - val_accuracy: 0.8906\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1711 - accuracy: 0.9370 - val_loss: 0.2956 - val_accuracy: 0.8934\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1657 - accuracy: 0.9389 - val_loss: 0.2970 - val_accuracy: 0.8974\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1609 - accuracy: 0.9409 - val_loss: 0.4269 - val_accuracy: 0.8628\n",
      "Epoch 30/30\n",
      "1072/1719 [=================>............] - ETA: 3s - loss: 0.1539 - accuracy: 0.9425"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549fd8e-cf03-424b-8523-9e45468686d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame from model history\n",
    "learning_curves = pd.DataFrame.from_dict(history.history, orient='columns')\n",
    "\n",
    "# Shift training loss back by 1 epoch for betting alignment (this is shifted left by 0.5 epochs becuase the training loss is a rolling mean during the epoch)\n",
    "learning_curves.loc[:, 'loss'] = learning_curves.loc[:, 'loss'].shift(-1)\n",
    "\n",
    "# Plot\n",
    "learning_curves.plot(ylim=(0,1), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44463a0-a3da-40c3-8959-282e301ac00c",
   "metadata": {},
   "source": [
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. **You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue trianing.** It's as simple as calling the fit() method again, since Keras just continues training where you left off.\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn't help, try another optimizer (and always return the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning the model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size.  \n",
    "\n",
    "Once you are satisfied with your model's validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this by using the evaluate() method. It is common to get slightly lower performance on the test set than the validation set, because the hyperparameters are tuned on the validation set, not the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1f3ec-82f3-440c-921d-43b79d74f00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16c5c23-c9ec-4d4b-8de3-3bc4a25072a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3df9-1540-47c2-a3f2-17b3409cc2f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56fbb695-cde5-4dd7-8d64-c64a68d14757",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
