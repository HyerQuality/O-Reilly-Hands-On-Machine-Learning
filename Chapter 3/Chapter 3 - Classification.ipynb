{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4f74af-caef-4ecb-bc25-7b117d8fbc58",
   "metadata": {},
   "source": [
    "### <b> This chapter focuses on the MNIST dataset. A dataset that is commonly referred to as the \"hello world\" of Machine Learning </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e6141c-d080-4d23-a591-fbd768156ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Import the MNIST dataset'\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01645ead-6505-4b49-8e6c-56bb9c7af422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "' Inspect the arrays'\n",
    "X, y = mnist['data'], mnist['target']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7313c55-abb6-4d6f-81fe-fd7f2e615a47",
   "metadata": {},
   "source": [
    "<b> The arrays are 70,000 rows with 784 features.  By default they are flattened 28x28 arrays of pixel intensity values which represent an image. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ea66cf-0704-4687-b7af-84f1de3dbcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target value:  5\n"
     ]
    }
   ],
   "source": [
    "' Observe the image represented in the first item of the data array'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "\n",
    "digit = X.to_numpy()[0].reshape(28,28)\n",
    "plt.imshow(digit, cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('Target value: ', y.to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca1f4fc-6b87-4912-bcb7-9f04792bb799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of 0        5\n",
       "1        0\n",
       "2        4\n",
       "3        1\n",
       "4        9\n",
       "        ..\n",
       "69995    2\n",
       "69996    3\n",
       "69997    4\n",
       "69998    5\n",
       "69999    6\n",
       "Name: class, Length: 70000, dtype: uint8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Convert the labels from strings to integers'\n",
    "import numpy as np\n",
    "y = y.astype(np.uint8)\n",
    "y.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b2099-8081-4acf-b700-94a2fe3c0289",
   "metadata": {},
   "source": [
    "<b> By design the MNIST dataset is already split into a train and a test set.  The first 60,000 samples represent a shuffled training set and the remaining 10,000 are the test set. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68b7c2d-e5d3-4bee-920d-864ee46805dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X.iloc[:60000], X.iloc[60000:], y.iloc[:60000], y.iloc[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a3a01-41c3-4177-a308-d851219ff40a",
   "metadata": {},
   "source": [
    "<b> Training a Binary Classifier </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c0cdcc-0067-4a96-830b-618715f99e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "59995    False\n",
       "59996    False\n",
       "59997     True\n",
       "59998    False\n",
       "59999    False\n",
       "Name: class, Length: 60000, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' To begin, simplify the problem to identify only a single digit. 5 and not-5.'\n",
    "\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "y_train_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ec21d-e01c-46d1-9008-511442119df3",
   "metadata": {},
   "source": [
    " #### A quick note about the SGD classifer ###\n",
    "    \n",
    "The Stochastic Gradient Descent (SGD) classifer deals with training instances independently, one at a time. This makes it well suited for both online learning and very large datasets. It relies on randomness during training and for replicable results its important to set a random_state parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c4b8e6-9e1b-4567-afc0-bb0cdd2bbb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Train a SGD classifer'\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "sgd_clf.predict([digit.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8d8ee-9199-4041-bdf3-ae1050741bc7",
   "metadata": {},
   "source": [
    "<b> Evaluating a classifier is trickier than evaluating a regressor. As such a large section of this chapter will be devoted to methods used in evaluting classifiers </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27019210-b836-4772-a02e-845ddac27c0a",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87651b5f-86ac-4b29-bfb5-948857754fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9669\n",
      "0.91625\n",
      "0.96785\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Sometimes you might want more control over the cross_validation process than what sklearn provides.  Understanding whats going on under the hood is important. \n",
    "    The following is an example of a loop which does roughly the same thing as sklearns cross_val_score() function\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train_5.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train_5.iloc[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))\n",
    "    \n",
    "# The StratifiedKFold class performs stratified sampling to produce folds that contain the representative ratio of each class.\n",
    "# At each iteration the code creates a clone of the classifer, trains that clone on the training folds, then makes predictions on the test fold.\n",
    "# Finally it displays the ratio of correct predictions as the final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242d742e-f06a-4883-9f8b-911b4c4ddc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95035, 0.96035, 0.9604 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Compare the manual cross validation loop with the results from sklearn cross_val_scoare'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70a5ea-b10f-4904-a414-28f660c9b1ad",
   "metadata": {},
   "source": [
    "<b> As always if the accuracy of the model seems to good to be true it likely is.  In this case the reason the accuracy is so high is that the composition of 5's in the dataset represent about 10% of the data.  As such, the model learns very quickkly that classifying everything as \"not-5\" yields very good results. This is a reason why accuracy is often not the preferred metric for classification performance, especially with skewed datasets where some values appear more often than others. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773316a4-c719-400e-a466-364574fd28cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91125, 0.90855, 0.90915])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Compare the model results with that of a classifier which only returns False'\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    \n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1a843-489b-4a2e-a04c-6e0347f2646c",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83437d7b-6c69-4097-bb9f-b59bcb74469a",
   "metadata": {},
   "source": [
    "<b>Confustion matrices compare the predictions of a classifier into True Positives, True Negatives, False Positives, and  False Negatives. They are a much better tool for measuring a classifiers predictive ability than accuracy alone.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5329dee3-ce69-44a3-933d-f6cd6690d27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[53892,   687],\n",
       "       [ 1891,  3530]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Construct a confustion matrix using sklearns cross_val_predict and confustion_matrix functions'\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(estimator=sgd_clf, X=X_train, y=y_train_5, cv=3, n_jobs= -1)\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1052c-9bba-4afd-8de8-3234e9fdbdda",
   "metadata": {},
   "source": [
    "<b> The True values lie along the diagonal.  As with all square matrices the False values can be interpreted from the elements above or below the diagonal as their values will be symmetrical.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62b626-f43f-47da-92bd-a0291e3d62e4",
   "metadata": {},
   "source": [
    "<b> While the confusion matrix is a powerful tool and is superior than a simple accuracy score, a combination of <i> precision </i> and <i> recall </i> can provide a more concise metric for measuring the performance of the classifier </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb026ff-5232-4c8f-895f-7f948007cf3a",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6b059-7e20-400a-8bd3-9dea0db885a7",
   "metadata": {},
   "source": [
    "$$ Precision = \\frac{True Positives}{(True Positives + False Positives)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f249ff-3f38-4067-a4e1-36d04a5cce44",
   "metadata": {},
   "source": [
    "$$ Recall = \\frac{True Positives}{(True Positives + False Negatives)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4051079-50ff-4449-a69e-12e529b64767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8370879772350012 0.6511713705958311\n"
     ]
    }
   ],
   "source": [
    "' Apply precision and recall to the binary classifier'\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_train_5, y_train_pred)\n",
    "recall = recall_score(y_train_5, y_train_pred)\n",
    "print (precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2b06a-fa87-4610-b688-14d300087b6b",
   "metadata": {},
   "source": [
    "<b> This classifer predicts an image of a 5 correctly 83.7% of the time, and out of all of the 5s it only detects 65.1% of them. Having very high values of precision and recall is very difficult in practice and often you need to choose which one is more important for your application.  A useful metric for comparing predicitve power of various classifiers is the $F_{1}$ score.  The $F_{1}$ score favors classifiers with similar precision and recall and as mentioned before that isn't always what you want. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f7b8f-16ee-4eb1-940f-0d58328470c5",
   "metadata": {},
   "source": [
    "$$ F_{1} = \\frac{True Positives}{True Positives + \\frac{(False Negatives + False Positives)}{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0358569-32aa-4724-b285-3f3a8daf397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325171197343846"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' Apply the F1 score to the binary classifier'\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced6eee-1cb7-4668-a275-a6484282d048",
   "metadata": {},
   "source": [
    "#### Precision / Recall Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d9171-8216-4a04-a0e9-a824089c0c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
