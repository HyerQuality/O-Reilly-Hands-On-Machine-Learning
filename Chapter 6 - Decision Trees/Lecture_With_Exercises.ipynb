{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "011ecf96-40a9-4e98-8c90-2bfaa92e7013",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928a7e3-127b-47bb-abe4-dc6d5a68ed13",
   "metadata": {},
   "source": [
    "<i> Decision Trees </i> are powerful algorithms capable of fitting complex datasets.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c465d8-837d-4c79-8a32-953424b41016",
   "metadata": {},
   "source": [
    "#### Example 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d01c9b-0a1f-4b1d-8d21-441135e7bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ac577-1ae7-45d7-9f2c-764edf21bcad",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378fa02-8064-40a1-a66e-e4add335b8a4",
   "metadata": {},
   "source": [
    "Let's see how the tree makes predictions.  You start at a <i> root node </i>.  This node asks whether the flower's petal length is smaller than 2.45 cm.  If it is, then you move down the root's left child node.  In this case, it is a <i> leaf node, </i> so it does not ask any questions: simpoy look at the predicted class for that node.  \n",
    "\n",
    "Now suppose you find another flower, and this time the petal length is greater than 2.45 cm.  You must move down the root's right child node, which is not a leaf node, so the node asks another question: is the petal width smaller than 1.75 cm?  If it is, then your flower is most likely an <i> Iris versicolor. </i> If not, it is likely an <i> Iris virginica.</i>\n",
    "\n",
    "<b> One of the many qualities of Decision Trees is that they require very little data preparation.  In fact, they don't require feature scaling or centering at all. </b>\n",
    "\n",
    "A nodes <i> gini </i> attribute measures its <b> impurity: </b> a node is \"pure\" (gini = 0) if all training instances it applies to belong to the same class.  Example 6-1 shows how the training algorithm computes the gini score $G_i$ of the $i^{th}$ node.\n",
    "\n",
    "<center>Equation 6-1 - Gini Impurity\n",
    "    \n",
    "$$G_i = 1 - \\sum_{k=1}^{n} p_i, k^{2}$$\n",
    "    \n",
    "<center> Where $p_i, k$ is the ratio of class <i>k</i> instances among the training instances of the $i^{th}$ node.</center><br>\n",
    "\n",
    "<b> Sklearn uses the CART algorithm, which produces only <i> binary trees: </i> nonleaf nodes always have two children (questions only have yes or no answers).  However, other algorithms such as the ID3 can product Decision Trees with nodes that have more than two children. <b>\n",
    "    \n",
    "Decision Trees are intuitive, and their decisions are easy to interpret.  Such models are often called <i> white box models.</i>  In contrast, as we will see, Random Forests or neural networks are generally considered <i>black box models</i>.  They make great predictions, and you can easily check the calculations that they performed to make these predictions; nevertheless, it is usually hard to explain in simple terms why the predictions were made.  For example, if a nerual network says that a particular person appears on a picture, it is hard to know what contributed to the prediction: did the model recognize that person's eyes? Their mouth? Their nose? Their shoes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6272c0-1850-4e81-8a2b-600a03ac5da6",
   "metadata": {},
   "source": [
    "### Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc45ff-66ff-4be6-a3a2-3cfae1e86861",
   "metadata": {},
   "source": [
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class <i> k. </i>   You can access this from the sklearn class through the predict_proba class method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c405708-8533-499e-9f1e-a49ac6666670",
   "metadata": {},
   "source": [
    "### The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7258f-be55-42b9-adee-087c7b982eba",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the <i> Classification and Regression Tree (CART) </i> algorithm to train Decision Trees (also called \"growing\" trees).  The algorithm works by first splitting the training set into two subsets using a single feature <i> k </i> and a threshold $t_k$.\n",
    "\n",
    "How does it choose <i> k </i> and $t_k$?  It searches for the pair (k, $t_k$) that produces the purest subsets (weighted by their size).  Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively.  It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity.  A few other hyperparameters control additional stopping conditions (min_samples_split, min_samples_leaf, min_weight_fraction_leaf and max_leaf_nodes).\n",
    "\n",
    "As you can see, the CART algorithm is a <i> greedy algorithm. </i> A greedy algorithm often produces a solution that's reasonably good but not guaranteed to optimal.  Unfortunately, finding the optimal tree is known to be an <i> NP-Complete </i> problem: it requires O(exp(m)) time, making the problem intractable even for small training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca85be2-2f11-4386-a3e7-854402c3744d",
   "metadata": {},
   "source": [
    "### Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b64808-8469-4594-b65e-971a4dd4dd97",
   "metadata": {},
   "source": [
    "Decision Trees generally are approximately balanced, so traversing the Decision Tree requires going through roughly O($log_2$(m)) nodes. Prediction complexity is O($log_2$(m)), independent of the number of features.  Training complexity is O(n x m$log_2$(m))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f5da8-84bc-4e7e-97cc-5787b1fe78fe",
   "metadata": {},
   "source": [
    "### Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639837a6-25ca-44c2-ace0-741708464f3d",
   "metadata": {},
   "source": [
    "By default, the Gini impurity measure is used, but you can select the <i>entropy</i> impurity measure instead by setting the <b>criterion</b> hyperparameter to \"entropy\".  In Machine Learning, entropy is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.  Equation 6-3 shows the definition of the entropy of the $i^{th}$ node.\n",
    "\n",
    "<center>Equation 6-3 - Entropy\n",
    "    \n",
    "$$H_i = - \\sum_{k=1}^{n} p_i, k\\log _{2}(p_i, k)$$\n",
    "    \n",
    "<center> Where $p_i, k$ != 0.</center><br>\n",
    "    \n",
    "So, should you use Gini impurity or entropy?  The truth is <b> most of the time it doesn't make a big difference: they lead to similar trees. </b>  Gini impurity is slightly faster to compute, so it is a good default.  However, <b>when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d647d-cee3-441c-8989-5263c2f618dd",
   "metadata": {},
   "source": [
    "### Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0fb0-01a5-4a9c-99f3-b394fd2a7c68",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about the training data.  If left unconstrained, the tree structure will adapt itself to the training data, fitting it very closely; most likely overfitting.  Such a model is often called a <i>nonparametric model</i>, not because it does not have any parameters but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data.\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree's freedom during training.  As you know by now, this is called regularization.  The regularization hyperparameters depend on the algorithm used.  In Scikit-Learn, this is controlled by the <b>max_depth</b> hyperparameter.  Reducing <b>max_depth</b> will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the Decision Tree: <b>min_samples_split</b> (the minimum number of samples a node must ahve before it can be split), <b>min_samples_leaf</b> (the minimum number of samples a leaf node must have), <b>min_weight_fraction_leaf</b> (same as <b>min_samples_leaf</b> but expressed as a fraction of the total number of weighted instances), <b>max_leaf_nodes</b> (the maximum number of leaf nodes) and <b>max_features</b> (the maximum number of features that are evaluated for splitting at each node).  Increasing <i>min</i> hyperparameters or reducing <i>max</i> hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be2177-9e15-4b79-b970-a1dcfb9904f5",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bb66e-92d2-44a1-936c-ba53fc10da5a",
   "metadata": {},
   "source": [
    "Decision Trees are also capable of performing regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e96fb-4ac9-493c-aa93-2152d77528b1",
   "metadata": {},
   "source": [
    "#### Example 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab14fc80-9d4d-4b6d-9df2-0ef9b5b71929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171ce7d-8e96-447c-b5b4-8c1abe16eb90",
   "metadata": {},
   "source": [
    "For example, suppose you want to make a prediction for a new instance with $x_1$ = 0.6.  you traverse the tree starting at the root, and you eventually reach the leaf node that predicts a value 0f 0.111.  This prediction is the average target value of the subset of training instances associated with this leaf node, and it results in a mean squared error equal to 0.015 over those instances.\n",
    "\n",
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE.  Just like for classification tasks, Decision Trees are prone to overfitting when dealing with regression tasks.  Without any regularization you get predcitions like those in the left image of Figure 6-6 on page 184."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2da0cf-1502-4f7c-bb89-311c81753692",
   "metadata": {},
   "source": [
    "### Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b34ec1-071b-40ef-8fcc-89ca27502e73",
   "metadata": {},
   "source": [
    "Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful.  However, they do have a few limitations.\n",
    "\n",
    "    1. Decisions Trees love  orthogonal decision boundaries which makes them sensitive to training set rotation.\n",
    "        - One way to limit this problem is to use Principal Component Analysis (chapter 8) which often resutls in a better orientation of the training data.\n",
    "    2. More generally, the main issue with Decision Trees is that they are ver sensitive to small variations in the training data.\n",
    "        - Since the training algorithm used by Sklearn is stochastic, you may get very different models even on the same training data (unless you set the <b>random_state</b> hyperparameter.)\n",
    "        \n",
    "Random Forests can limit this instability by averaging predictions over many trees, as we will see in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadb160-cd26-444d-833f-b8c427c67d18",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584da4b9-c1e3-49ce-8be7-f502fbb66756",
   "metadata": {},
   "source": [
    "#### 1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08bd98-46c4-47e8-b0cd-be0157acfcfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<b>My Answer:</b>\n",
    "\n",
    "Assuming the CART algorithm continues to split the dataset in half until only 1 instance remains in each of the lowest leaf nodes, the depth would be approximately 10.  <b> Nowhere in the chapter is this discussed. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398159c-0366-4f27-b509-351cfbca5d12",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "$\\log_{2}m^{2}$ = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a076454-d2a9-4931-a4e2-7e110010164b",
   "metadata": {},
   "source": [
    "#### 2. Is a node's Gini impurity generally lower or greater than its parent's?  Is it <i> generally </i> lower/greater or <i>always</i> lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255dec1f-3349-41ea-8813-42d03560aefd",
   "metadata": {},
   "source": [
    "<b>My Answer:</b>  \n",
    "\n",
    "If the CART algorithm is used to create binary trees, the Gini impurity of leaf nodes will always be lower than the parent's.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343bd17-8e84-45af-9531-c245caf9bfb7",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "A node's Gini impurity is generally lower than its parent's.  The is due ot the CART training algorithm's cost function, which splits each node in a way that minimizes the weighted sum of its children's Gini impurities.  However, it is possible for a node to have a higher Gini impurity than its parent, as long as the increase is more than compensated for by a decrease in other child's impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1a4e3-dcaf-4354-8252-abf9dd69b7e0",
   "metadata": {},
   "source": [
    "#### 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing <b>max_depth</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05717663-ee7e-4ad0-8eee-f9702a0db637",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>My Answer:</b>  \n",
    "\n",
    "Yes, decreasing \"max\" parameters will help regularize an overfitting Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13874d5b-b9fb-4207-8f2f-bf374ff9d12b",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "If a Decision Tree is overfitting the training set, it may be a good idea to decrease max_depth, since this will contrain the model, regularizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3cb4cb-9572-4748-879f-da1ea8d32a9d",
   "metadata": {},
   "source": [
    "#### 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74fbda-ce5d-40b7-85a0-463d7699a2ac",
   "metadata": {},
   "source": [
    "<b>My Answer:</b>\n",
    "\n",
    "Generally, Decision Trees work without scaling features and scaling features won't change the output.  However, I am of the opinion that scaling input features is best practice and if it doesn't hurt the model you should do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd7b758-0e38-485b-8b47-f5596a600967",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "Decision Trees don't care whether or not the training data is scaled or centered; that's one of the nice things about them.  So if a Decision Tree underfits the training set, scaling the input features will just be a waste of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e384019-2536-451e-89eb-5361a2310d59",
   "metadata": {},
   "source": [
    "#### 5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take to train another Decision Tree on a training set containing 10 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734fd77-d4fd-4782-80da-3aee00ab0d27",
   "metadata": {},
   "source": [
    "<b>My Answer:</b>\n",
    "\n",
    "Training complexity is O(n x m$log_2$(m)).  So training 1 million instances is 1log(1M) and training 10 million instances is 10log(10M) time units.  10log(10M)/log(1M) = ~11.5x longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950e0d88-a6c7-476a-b897-7acfceaf47b7",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "Same math, but precise answer of 11.7 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf98a06-638e-4263-b074-cf61ebf1e756",
   "metadata": {},
   "source": [
    "#### 6. If your training set contains 100,000 instances, will setting presort=True speed up training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd46f63-7603-4de6-bfbd-775aa3d578ff",
   "metadata": {},
   "source": [
    "<b>My Answer:</b>\n",
    "\n",
    "Unlikely.  presort=True is only a time save for a few thousand instances.  100,000 is likey too many for a performance boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c81562-afb2-4025-bba8-961ffc9a30c3",
   "metadata": {},
   "source": [
    "<b>Book Answer:</b>\n",
    "\n",
    "Presorting the training set speeds up training only if the dataset is smaller than a few thousand instances.  If it contains 100,000 instances, setting presort=True will considerably slow down trianing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2d729-24ef-458a-ae32-164e342e2d8c",
   "metadata": {},
   "source": [
    "#### 7. Train and fine-tune a Decision Tree for the moons dataset by following steps:\n",
    "\n",
    "    1. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "    2. Use train_test_split() to split the dataset into a training set and a test set.\n",
    "    3. Use grid search with cross validation to find good hyperparameter values for the a DecisionTreeClassifer. Hint: Try various values for max_leaf_nodes.\n",
    "    4. Train it on the full training set using these hyperparameters, and measure your model's performance on the test set.  You should get roughly 85% - 87% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de06ec-9849-4da4-bb83-98b05773b979",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>My Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b394a965-5546-4619-929d-92cbac30bd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125000 candidates, totalling 625000 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2, max_leaf_nodes=4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the raw data\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "\n",
    "# Split the data into trianing and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "param_grid = [\n",
    "    {\n",
    "        'max_depth': np.linspace(start=2, stop=20, num=10, endpoint=True, dtype=int), \n",
    "        'min_samples_split': np.linspace(start=2, stop=20, num=10, endpoint=True, dtype=int), \n",
    "        'max_leaf_nodes': np.linspace(start=2, stop=20, num=10, endpoint=True, dtype=int)\n",
    "    }\n",
    "]\n",
    "\n",
    "optimized_tree_clf = GridSearchCV(estimator=tree_clf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs= -1, verbose=0)\n",
    "optimized_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Train on the whole training set using the optimized parameters\n",
    "tree_clf = DecisionTreeClassifier(**optimized_tree_clf.best_params_)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a274ec73-568c-4d8c-aa9d-2d68dc7ca211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8646666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# See how the model performs on the test set\n",
    "predicted = tree_clf.predict(X_test)\n",
    "print (accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bfd8c-53b9-4e6e-a0a2-f0d8950a41bb",
   "metadata": {},
   "source": [
    "#### 8. Grow a forest by following these steps:\n",
    "\n",
    "    1. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly.  Hint: You can use Sklearns ShuffleSplit class for this.\n",
    "    2. Train one Decision Tree on each subset, using the hyperparameter values found in the previous exercise.  Evaluate these 1,000 Decision Trees on the test set.  Since they were trained on smaller sets, these Decision Trees will likely perform worse than the first Decision Tree, achieving only about 80% accuracy.\n",
    "    3. Now comes the magic.  For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction. Hint: You can use SciPy's mode() function for this.  This approach gives you the majority-vote predictions over the test set.\n",
    "    4. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher).  Congratulations, you have trained a Random Forest classifer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a0fd3-b97c-41f2-91eb-334aaa87bfbd",
   "metadata": {},
   "source": [
    "<b>My Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2161f2d6-7f4f-4121-8c29-e932e137b947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566666666666669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Split the dataset\n",
    "rs = ShuffleSplit(n_splits=1000, random_state=0, test_size=100, train_size=None)\n",
    "\n",
    "# Create a hash table to store the individual models\n",
    "clf_collection = {}\n",
    "\n",
    "# Train the optimzed tree classifier on each of the subsets.  Store them indexed by fold.\n",
    "for i, (train_index, test_index) in enumerate(rs.split(X_train)):\n",
    "    clf_collection[i] = tree_clf.fit(X_train[test_index], y_train[test_index])\n",
    "\n",
    "# Create a list for accuracies\n",
    "acc = []\n",
    "\n",
    "# Apply the models to the whole test set\n",
    "for fold, model in clf_collection.items():\n",
    "    prediction = model.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, prediction))\n",
    "\n",
    "# Print average accuracy\n",
    "acc = np.array(acc)\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44d08e16-0d27-49e4-8b85-7a45820cfcba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.502"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new containers to store the predictions of every model for each test instance and the mode of those predictions\n",
    "preds = []\n",
    "modes = []\n",
    "acc = []\n",
    "\n",
    "# For each test instance, make predictions with every model. Keep only the mode prediction for each instance.\n",
    "for test_instance in X_test:\n",
    "    for fold, model in clf_collection.items():\n",
    "        prediction = model.predict(test_instance.reshape(1, -1))\n",
    "        preds.append(prediction.astype(int))\n",
    "    modes.append(mode(preds, keepdims=True).mode.flatten())\n",
    "\n",
    "\n",
    "accuracy_score(y_test, modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d3d28-ffe8-4ddf-aa12-15ab02f7054b",
   "metadata": {},
   "source": [
    "<b> Book Answer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cc59846-8185-4d63-b2a1-6da41728dc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8573333333333333"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))\n",
    "\n",
    "forest = [clone(optimized_tree_clf.best_estimator_) for _ in range(n_trees)]\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "\n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(np.mean(accuracy_scores))\n",
    "\n",
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)\n",
    "\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0, keepdims=True)\n",
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a73850-aab5-42b0-95f4-6ddac94fa1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
