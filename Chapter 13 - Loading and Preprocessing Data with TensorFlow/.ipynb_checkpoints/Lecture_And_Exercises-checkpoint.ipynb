{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88471804-169b-478a-94b8-2129590b17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af8b62-17bc-4e11-a57b-aa48fa1bcbba",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab14e7a-85e7-4e9e-b42d-3021090e65f0",
   "metadata": {},
   "source": [
    "Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but tensorFlow makes it easy thanks to the *Data API*: you just create a dataset object, and tell it where to get the data and how to transform it.\n",
    "\n",
    "Off the shelf, the Data API can read from text files, binary files with fixed-sized records, and binary files that use TensorFlow's TFRecord format. The Data API also has support for reading from SQL databases.\n",
    "\n",
    "Reading huge datasets efficiently is not the only difficulty: the data also needs to be preprocessed, usually normalized. Moreover, it is not always composed strictly of convenient numerical fields: there may be text features, categorical features, and so on. These need to be encoded, for example using one-hot encoding, bag-of-words encoding, or _embeddings_ (as we will see, an embedding is a trainable dense vector that represents a category or token). One option to handle all this preprocessing is to write your own custom preprocessing layers. Another is to use the standard preprocessing layers provided by Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc17d2-faf2-407a-9ac6-6671a5e373cc",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b95943a-f17a-42dc-9845-b9a68498431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b0334-d7c3-4a97-a761-396362dcbc81",
   "metadata": {},
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X (along the first dimension), so this dataset contains 10 items. In this case we would have obtained the same dataset if we had used tf.data.Dataset.range(10). Note that you can iterate over the dataset's items intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab36d9c-7cb4-4dd0-8515-2781b2619136",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4755-e476-4b57-baf0-2eec138bf2d0",
   "metadata": {},
   "source": [
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5cbb7d-4e7d-4466-8c63-b7a0c4f91ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2dc62-a2ac-4f1c-bf9c-3a37d54c5b9a",
   "metadata": {},
   "source": [
    "In this example, we first call the repeat() method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset three times. Of course, this will not copy all the data in memory three times! (If you call this method with no arguments the new dataset will repeat the srouce dataset forever, so the code that iterates over the dataset will have to decide when to stop.) Then we call the batch() method on this new dataset, and again this creates a new dataset. This one will group the items of this final dataset. As you can see, the batch() method had to output a final batch of size two instead of seven, but you can call it with drop_remainder=True if you want it to drop this final batch so that all batches have the exact same size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fe061-d96b-496c-81c8-228cd6b93a2f",
   "metadata": {},
   "source": [
    "The dataset methods do _not_ modify datasets, they create new ones, so make sure to keep a reference to these new datasets (e.g. with dataset = ...) or else nothing will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cd6f58-2074-43ea-8b6c-49ca5d717494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a12ba5-dfe4-45e5-aded-29aa334feee8",
   "metadata": {},
   "source": [
    "You can also transform the items by calling the map() method. For example, the code above doubles the values of the original dataset. This function is the on you will call to apply any preprocessing you want to your data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speeds things up: it's as simple as setting the num_parallel_calls argument. Note that the function you pass to the map() method must be convertible to a TF Function.\n",
    "\n",
    "While the map() method applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole. For example, the following code applies the unbatch() to the dataset. Each item in the new dataset will be a single-integer tensor instead of a batch of seven integers. It is also possible to simply filter the dataset using filter() or look at just a few items using take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623c16a1-c423-41c1-9adf-39f9186d4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.unbatch() #dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f83575-c823-4d70-bdb5-d5ee9c76ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785424c0-5cdf-45bb-b643-58947d16294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f4de1-d513-4b32-9ee7-171e4e3c1051",
   "metadata": {},
   "source": [
    "### Shuffling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0ce65-620d-4fe1-b8f4-f8daf95afe81",
   "metadata": {},
   "source": [
    "Gradient Descent works best when the instances in the training set are independent and identically distributed. A simple way to ensure this is to shuffle the instances using the shuffle() method. You must specify the buffer size, and it is important to make it large enough, or else shuffling will not be very effective. Just don't exceed the amount of RAM you have, and even if you ahve plenty of it, there's no need to go beyond the dataset's size.\n",
    "\n",
    "For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a buffer of size 5 and a random seed of 42, then batched with a batch size of 7.\n",
    "\n",
    "If you call repeat() on a shuffled dataset, but default it will generate a new order at every iteration. This is generally a good idea, but if you prefer to reuse the same order at each iteration you can set reshuffle_each_iteration=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a9ea71-f836-4d3a-9829-bde92766ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3).shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3f2af-0be6-4df9-b445-96bc1195fa55",
   "metadata": {},
   "source": [
    "For a large dataset that does not fit into memory, this simple shuffling-buffer approach may not be sufficient. One solution is to shuffle the source data itself. Then, to shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the shuffle() method. The Data API makes all of this possible in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b47f3-3a6e-493e-a41f-5e6f47df8458",
   "metadata": {},
   "source": [
    "#### Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1fb17-e4e6-4512-a349-6f018c797855",
   "metadata": {},
   "source": [
    "The following example assumes the Califorina housing dataset has been downloaded, split into train, val, and test sets, and that each set is further broken down into multiple csv files. I'm not going to do this because it is unncessary bloat on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3776d9-cdf0-4f81-a914-b731951ecc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_filepaths = ['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]\n",
    "# filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# n_readers = 5\n",
    "# dataset = filepath_dataset.interleave(\n",
    "#     lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "#     cycle_length=n_readers\n",
    "# )\n",
    "\n",
    "# for line in dataset.take(5):\n",
    "#     print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572e3ad-ada1-43f5-98cc-d07542c6c804",
   "metadata": {},
   "source": [
    "Let's suppose train_filepaths contains the list of training file paths. By default, the list_files() function returns a dataset that shuffles the file paths. In general, this is a good thing, but you can set shuffle=False if you do not want that for some reason.\n",
    "\n",
    "Next, you can call the interleave() method to read from five files (n_readers) at a time and interleave their lines (skipping the first line of each file, which is the header row, using the skip() method).\n",
    "\n",
    "The interleave() method will create a dataset that will pull five file paths from the filepath_dataset, and for each one it will call the function you feed it (a lambda in this example) to create a new dataset (in this case a TextLineDataset).\n",
    "\n",
    "For interleaving to work best, it is preferable to have files of identical length; otherwise the ends of the longest files will not be interleaved.\n",
    "\n",
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the num_parallel_calls argument to the number of threads you want. Alteratively, you can set num_parallel_calls=tf.data.experimental.AUTOTUNE for TensorFlow to choose the right number of threads dynamically based on the availble CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082840-1876-42b0-b36e-811aac64a76c",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5443a867-8adc-4617-bfa6-1386701de09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_mean, x_std = [...] # mean and scale of each feature in the training set\n",
    "# n_inputs = 8\n",
    "\n",
    "# def preprocess(line):\n",
    "#     defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "#     fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "#     x = tf.stack(fields[:-1])\n",
    "#     y = tf.stack(fields[-1:])\n",
    "#     return (x - x_mean) / x_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a2416-cf46-40b2-8dbc-c0bb3fe72226",
   "metadata": {},
   "source": [
    "Let's walk through the code above:\n",
    "\n",
    "1. First, the code assumes that we have precomputed the mean and standard deviation of each feature in the training set. x_mean and x_std are just 1D tensors (or Numpy arrays) containing 8 floats, one per input feature.\n",
    "2. The preprocess() function takes one CSV line and starts by parsing it. For this it uses the tf.io.decode_csv() function, which takes 2 arguments: the first is the line to parse, and the second is an array containing the default value for each column in CSV File. This array tells TensorFlow not only the default value for each column, but also what number of columns and their types. In this example, we tell it that all feature columns are floats and that missing values should default to 0, but we provide an empty array of tf.float32 as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters the missing value.\n",
    "3. The decode_csv() function returns a list of scalar tensors (one per column), but we need to return 1D tensor arrays. So we call tf.stack() on all tensors except the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value (this makes it a 1D tensor array with a single value, rather than a scalar tensor).\n",
    "4. Finally, we scale the input features by subtracting the feature means and then dividing by the feature standard deviations, and we return a tuple containing the scaled features and target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb1e76-1b5c-46ec-85cc-a8c37eb36bd2",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc88eed-6c46-444c-bcc9-ae06ae614dbb",
   "metadata": {},
   "source": [
    "To make the code reusable, let's put together everything we have discussed so far int oa small helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e8468d-44e7-415e-81fd-f8e43857843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(\n",
    "    filepaths,\n",
    "    repeat=1,\n",
    "    n_readers=5,\n",
    "    n_read_threads=None,\n",
    "    shuffle_buffer_size=1e5,\n",
    "    n_parse_threads=5,\n",
    "    batch_size=32\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_parse_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd70c3b-17d1-4f79-8f8d-d35e3bf0fc70",
   "metadata": {},
   "source": [
    "### Prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d780-50eb-4bc2-ac46-51a911b69e26",
   "metadata": {},
   "source": [
    "By calling prefetch(1) at the end, we are creating a dataset that will do its best to always be one batch ahead. In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready. This can improve performance dramatically.\n",
    "\n",
    "In general, just prefetching one batch is fine, but in some cases you may way to prefetch a few more. Alternatively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE.\n",
    "\n",
    "With prefetching, the CPU and the GPU work in parallel: as the GPU works on one batch, the CPU works on the next. If you plan to purchase a GPU, its processing power and its memory size are of course very important (in particular, a large amount of RAM is crucial for computer vision). Just as important to get good performance is its _memory bandwidth_; this is the number of gigabytes of data it can get into or out of its RAM per second.\n",
    "\n",
    "If the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset's cache() method to cache its content to RAM. This way, each instance will only be read and preprocessed once.\n",
    "\n",
    "We have discussed the most common dataset methods, but there are a few more you may want to look at:\n",
    "\n",
    "1. concatenate()\n",
    "2. zip()\n",
    "3. window()\n",
    "4. reduce()\n",
    "5. shard()\n",
    "6. flat_map()\n",
    "7. padded_batch()\n",
    "\n",
    "There are also a couple more class methods work mentioning:\n",
    "\n",
    "1. from_generator()\n",
    "2. from_tensors()\n",
    "\n",
    "which create a new dataset from a Python generator or a list of tensors, respectively. Also note that there are experimental features available in tf.data.experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71860f3b-3cec-45d7-b448-c8edf1fba7b7",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a01998-c5f0-4221-b501-e362d5ea6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = csv_reader_dataset(train_filepaths)\n",
    "# val_set = csv_reader_dataset(val_filepaths)\n",
    "# test_set = csv_reader_dataset(test_filepaths)\n",
    "\n",
    "# model = tf.keras.models.Sequential([...])\n",
    "# model.compile([....])\n",
    "# model.fit(train_set, epochs=10, validation_data=val_set)\n",
    "\n",
    "# model.evaluate(test_set)\n",
    "# new_set = test_set.take(3).map(lambda X: y: X) # pretend we have 3 new instances\n",
    "# model.predict(new_set) # a dataset containing new instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775557e-4571-49de-9421-b87ab39c5a47",
   "metadata": {},
   "source": [
    "Now we can use the csv_reader_dataset(). Note that we do not need to repeat it, as this will be taken care of by tf.keras.\n",
    "\n",
    "If you want to build your own custom training loop (as in Chapter 12), you can just iterate over the training set, very naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27393231-13b8-4f11-bfd5-a66c3186b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, y_batch in train_set:\n",
    "#     [...] # perform one gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581fdf7-2dd0-4524-bc19-c5f5fe4d16f0",
   "metadata": {},
   "source": [
    "In fact, it is even possible to create a TF Function (see Chapter 12) that performs the whoel training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3602701-46e9-4e89-8741-e02e1d2ca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train(model, optimizer, loss_fun, n_epochs, **kwargs): \n",
    "#     train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
    "#     for X_batch, y_batch in train_set:\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = model(X_batch)\n",
    "#             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "#             loss = tf.add_n([main_loss] + model.losses)\n",
    "#         grads = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075989f-2ef2-4f67-867a-d83513d3a8ca",
   "metadata": {},
   "source": [
    "Note that CSV files do not support large or complex data structures (such as images or audio) very well. So let's see how to use TFRecords instead. If you are happy with CSV files (or whatever other format you are using), you do not _need_ to use TFRecords. As the saying goes, if it ain't broke, don't fix it! __TFRecords are useful when the bottleneck during training is loading and parsing data.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0dd03-247b-44c4-8b34-8ed7128b4886",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97ab3b-2a1f-4fde-a1b8-0fddfda9501c",
   "metadata": {},
   "source": [
    "The TFRecord format is TensorFlow's preferred format for storing large amounts of data and reading it efficiently. You can easily create a TFRecord file using the tf.io.TFRecordWriter class. By default, a TFRecordDataset will read files one by one, but you can make it read multiple files in parallel and interleave their records by setting num_parallel_reads. Alternatively, you could obtain the same results by using list_files() and interleave() as we did earlier to read multiple CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da24b16c-8aa4-4ff3-9e51-b513daa2ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'This is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'This is the second record')\n",
    "\n",
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274adb56-6035-44b7-a7dd-245529c362df",
   "metadata": {},
   "source": [
    "### Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304362e-5184-4d5a-a894-04754b553dbc",
   "metadata": {},
   "source": [
    "It can sometimes be useful to compress your TFRecord files, especially if they need to be loaded via a network connection. You can create a compressed TFRecord file by setting the options argument. Whjen reading a compressed TFRecord file you need to specify the compression type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7521ebc-c12e-4467-9531-eec6dfc9bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'This is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed_data.tfrecord', options) as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'This is the second record')\n",
    "\n",
    "filepaths = ['my_compressed_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths, compression_type='GZIP')\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99bd34-7638-4e1f-a3f9-0928ea2a9731",
   "metadata": {},
   "source": [
    "### A Brief Introduction to Protocol Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6ca06-d121-491f-8f24-5919acb58a08",
   "metadata": {},
   "source": [
    "TFRecord files usually contain serialized protocol buffers (also called _protobuffs_). This is a portable, extensible, and efficient binary format developed at Google back in 2001. The following example says we are using version 3 of the protobuf format, and it specifies that each Person object may (optionally) have a name of type string, an id of type int32, and zero or more email fields, each of type string. The numbers 1, 2, and 3 are the field identifiers: they will be used in each record's binary representation. Once you have a definition in a .proto file, you can compile it. This requires protoc, the protobuf compiler, to generate access classes in Python. Fortuantely, TensorFlow does include special protobuf definitions for which it provies parsing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9f2f54-e2e4-4954-ac4f-252241abacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntax = 'proto3';\n",
    "# message Person {\n",
    "#     string name = 1;\n",
    "#     int32 id = 2;\n",
    "#     repeated string email = 3;\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac0a02-e29f-44da-9a75-c6b7137da1c1",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851670a7-a6da-4842-ad3c-df7dd08a1354",
   "metadata": {},
   "source": [
    "The main protobuf typically used in a TFRecord file is the __Example__ protobuf, which represents one instance in a dataset. Normally you would write much more than one Example. __Typically, you would create a conversion script that reads from your current format (say, CSV files), creates an Example protobuf for reach instance, serializes them, and saves them to several TFRecord files, ideally shuffling them in the process. This requires a bit of work, so once again make sure it is really necessary__ (perhaps your pipeline works fine with CSV files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53404511-9e47-44d0-aff3-5063594ff8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List, Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'name': Feature(bytes_list=BytesList(value=[b'Alice'])),\n",
    "            'id': Feature(int64_list=Int64List(value=[123])),\n",
    "            'emails': Feature(bytes_list=BytesList(value=[\n",
    "                b'a@b.com',\n",
    "                b'c@d.com'\n",
    "            ]))\n",
    "        })\n",
    ")\n",
    "\n",
    "with tf.io.TFRecordWriter('my_contacts.tfrecord') as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e1c9c-583a-4208-a314-9161d336431e",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf42d8-5921-4a93-a8cd-891e3a1f0107",
   "metadata": {},
   "source": [
    "To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset once again, and we will parse each Example using tf.io.parse_single_example(). This is a TensorFlow operations, so it can be included in a TF Function. It requires at least two arguments: a string scalar tensor containing the serialized data, and a description of each feature.\n",
    "\n",
    "The following code defines a description dictionary, then it iterates over the TFRecordDataset and parses the serialized Example protobuf this dataset constains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a230980c-3330-49e3-bb9d-22beefd72dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(indices=tf.Tensor(\n",
       "[[0]\n",
       " [1]], shape=(2, 1), dtype=int64), values=tf.Tensor([b'a@b.com' b'c@d.com'], shape=(2,), dtype=string), dense_shape=tf.Tensor([2], shape=(1,), dtype=int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_description = {\n",
    "    'name': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'id': tf.io.FixedLenFeature([], tf.int64, default_value=0), \n",
    "    'emails': tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset(['my_contacts.tfrecord']):\n",
    "    parsed_example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        feature_description\n",
    "    )\n",
    "\n",
    "parsed_example['emails']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cc685-60a3-471d-8a14-c75a087bb034",
   "metadata": {},
   "source": [
    "A BytesList can contain any binary data you want, including any serialized object. You can also store any tensor you want in a BytesList by serializing the tensor using tf.io.serialize_tensor() then putting the resulting byte string in a BytesList feature. Later, when you parse the TFRecord, you can parse this data using tf.io.parse_tensor().\n",
    "\n",
    "As you can see, the Example protobuf will probably be sufficient for mose use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be represented as a list of sentences, where each sentence is represented as a list of words. And perhaps each document also has a list of comments, where each comment is represented as a list of words. __TensorFlow's SequenceExample protobuf is designed for such use cases__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "202cbac5-a41d-4b8e-901e-11ae66aaf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Instead of parsing examples one by one using tf.io.parse_single_example() you may want to parse them batch by batch using tf.io.parse_example()'''\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).batch(10)\n",
    "for serialized_examples in dataset:\n",
    "    parsed_examples = tf.io.parse_example(\n",
    "        serialized_examples,\n",
    "        feature_description\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5192a23-1011-48eb-a3e1-23cd6509b0a4",
   "metadata": {},
   "source": [
    "### Handling Lists of Lists Using the SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002868ed-1d38-4b02-9bbf-5548dc81d12d",
   "metadata": {},
   "source": [
    "A SequenceExample contains a Features object for the contextual data and a FeatureLists objects that contains one or more named FeatureList objects (e.g. a FeatureList named 'content' and a Featurelist named 'comments').\n",
    "\n",
    "Each FeatureList contains a list of Feature objects, each of which may be a list of byte strings, a list of 64-bit integers, or a list of floats (in this example, each Feature would represent a sentence or a comment, perhaps in the form of a list of word identifiers). Building a SequenceExample, serializing it, and parsing it is simliar to building, serializing, and parsing an Example, but you must use tf.io.parse_single_sequence_example() to parse a single SequenceExample or tf.io.parse_sequence_example() to parse a batch. \n",
    "\n",
    "Both functions return a tuple containing the context features (as a dictionary) and the feature lists (also as a dictionary). IF the feature lists contain sequences of varying sizes (as in the preceding example), you may want to convert them to ragged tensors, using tf.RaggedTensor.from_sparse()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f00c6557-c9a0-4ad8-93de-f57b4f706392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "#     serialized_sequence_example,\n",
    "#     context_feature_descriptions,\n",
    "#     sequence_feature_descriptions\n",
    "# )\n",
    "# parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc75a2-9cdb-49de-878a-a95e9db7c810",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1f6b9-217d-4717-a69d-e78323830625",
   "metadata": {},
   "source": [
    "Preparing your data for a neural network requires converting all features into numerical features, generally normalizing them, and more. If you data contains categorical features or text features, they need to be converted to numbers. This can be done ahead of time using NumPy, pandas, or Scikit-Learn, for example, or you can preprocess your data with the Data API (e.g. using the dataset's map() method). Alternatively, you can include a preprocessing layer directly in your model. Let's look at this last option now.\n",
    "\n",
    "This is an example of how you can implement a standardization layer using a Lambda layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef9a42cb-9e02-42ae-bcbc-8b23d51053fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = np.mean(X_train, axis=0, keepdims=True)\n",
    "# stds = np.std(X_train, axis=0, keepdims=True)\n",
    "# eps = tf.keras.backend.epsilon()\n",
    "# model = tf.keras.models.Sequential([\n",
    "#     tf.keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
    "#     [...] # other layers\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4db04-ff3e-4568-bbc5-531ae6176188",
   "metadata": {},
   "source": [
    "You may prefer to use a nice self-contained custom layer (must like Scikit-Learns StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1db8074c-6829-4807-8cdf-c320512a7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(tf.keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + tf.keras.backend.epsilon())\n",
    "\n",
    "std_layer = Standardization()\n",
    "# std_layer.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1485f-63cd-4a84-a0b0-b4ac279be21d",
   "metadata": {},
   "source": [
    "Before you can use this standardization layer, you will need to adapt it to your dataset by calling the adapt() method and passing it the data sample. This will allow it to use the appropriate mean and standard deviation for each feature.\n",
    "\n",
    "The sample must be large enough to be representative of your dataset, but it does not have to be the full training set: it general, a few hundred randomely selected instances will suffice. Next, you can use this preprocessing layer like a normal layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e89881c1-b334-4fd1-8fef-460b0f65b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(std_layer)\n",
    "# [...] # create the rest of the model\n",
    "# model.compile([...])\n",
    "# model.fit([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc69ce54-1da4-4cea-b17b-3c10ec496326",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc881a8-070e-4a00-9efe-6280667ebfe5",
   "metadata": {},
   "source": [
    "Consider the ocean_proximity feature in the California housing dataset we explored in Chapter 2. We need to encode this feature before we feed it into a neural network. Since there are very few categories, we can use one-hot encoding. For this, we first need to map each category to its index (0 to 4), which can be done using a lookup table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6734d04d-01bb-4367-9fbd-83f4493e2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773f14f-30ab-4fe9-812c-b7c3b50e9994",
   "metadata": {},
   "source": [
    "Let's go through this code:\n",
    "\n",
    "1. We first define the _vocabulary_: this is the list of all possible categories\n",
    "2. Then we create a tensor with the corresponding indices (0 to 4)\n",
    "3. Next, we create an initializer for the lookup table, passing it the list of categories and their corresponding indices. In this example, we already have this data, so we use a KeyValueTensorInitializer; __but if the categories were listed in a text file (with one category per line), we would use a TextFileInitializer instead.__\n",
    "4. In the last two lines we create the lookup table, giving it the initializer and specifying the number of _out-of-vocabulary_ (oov) buckets. If we look up a category that does not exist in the vocabulary, the lookup table will compute a hash of this category and use it to assign the unknown category to one of the oov buckets. Their indices start after the known categories, so in this example the indices of the two oov buckets are 5 and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab73af-1dfa-426a-b71a-25490f219b36",
   "metadata": {},
   "source": [
    "Why use oov buckets? Well, if the number of categories is large (e.g. zip codes, cities, words, products, or users) and the dataset is large as well, or it keeps changing, then getting the full list of categories may not be convenient. One solution is to define the vocabulary based on a data sample (rather than the whole training set) and add some oov buckets for the other categories that were not in the data sample. The more unknown categories you expect to find during training, the more oov buckets you should use. Indeed, if there are not enough oov buckets, there will be collisions: different categores will end up in the same bucket, so the neural network will not be able to distinguish them (at least not based on this feature).\n",
    "\n",
    "Now let's use the lookup table to encode a small batch of categorical features to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71436113-cccb-44a0-a4a1-6b158abcedff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eea69d7-3e6a-40a3-bbe3-6e96e1282c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7733e-d785-466f-9ae3-99bcc77a416c",
   "metadata": {},
   "source": [
    "Notice that we have to tell this function the total number of indices, which is equal to the vocabulary size plus the number of oov buckets. Just like earlier, it wouldn't be too difficult to bundle all of this logic into a nice self-contained class. Its adapt() method would take a data sample and extract all the distinct categories in contains. It would create a lookup table to map each category to its index (including unknown categories using oov buckets). Then its call() method would use the lookup table to map the input categories to their indices. \n",
    "\n",
    "By the time you read this, tf.keras will probably include a layer called tf.keras.layers.TextVectorization, which will be capable of doing exactly this. You can add this layer at the beginning of your model, followed by a Lambda layer that woudl apply the tf.one_hot() function, if you want to convert these indices to one-hot vectors.\n",
    "\n",
    "__One-hot encoding may not be the best solution, though. The size of each one-hot vector is the vocabulary length plus the number of oov buckets. This is fine when there are just a few possible categories, but if hte vocabulary is large, it is much more efficient to encode them using _embeddings_ instead.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b4643-6a49-4fda-a1e5-6a68a998faf8",
   "metadata": {},
   "source": [
    "___As a rule of, if the number of categories is lower than 10, then one-hot encoding is generally the way to go. If the number of categories is greater than 50 (which is often the case when you use hash buckets), then embeddings are usually preferable. In between 10 and 50 you may want to experiment with both options and see which one works best for your use case.___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c705f0-309c-4873-90fa-a21452734a3d",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28582af3-6a57-496d-9bb4-2b8dd5d40d7b",
   "metadata": {},
   "source": [
    "An embedding is a trainable dense vector that represents a category. By default, embeddings are initalized randomly, so for example the 'NEAR BAY' category could be represented initially by a random vector such as [0.131, 0.890], while the 'NEAR OCEAN' category might be represented by another random vector such as [0.631, 0.791].\n",
    "\n",
    "__In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter you can tweak.__ Since these embeddings are trainable, they will gradually improve during training; and as they represent fairly simliar categories, Gradient Descent will certainly end up pushing them closer together, while it will tend to move them away from the 'ISLAND' category's embedding. __This is called representation learning__ (more on this in Chapter 17)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc0f5b-f768-48eb-83a0-d24790b08969",
   "metadata": {},
   "source": [
    "Let's look at how we could implement embeddings manually, to understand how they work. First, we need to create an _embedding matrix_ containing each category's embedding, initialized randomly; it will have one row per category and per oov bucket, and one column per embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eb1fa89-ac08-4031-9f44-e5906ceea260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.2865181 , 0.90083325],\n",
       "       [0.97996306, 0.87893903],\n",
       "       [0.41598547, 0.41983974],\n",
       "       [0.2688321 , 0.17283237],\n",
       "       [0.8883368 , 0.40318513],\n",
       "       [0.3780378 , 0.9709221 ],\n",
       "       [0.09544849, 0.07433176]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This embedding matrix is a random 6x2 matrix stored in a variable (so it can be tweaked by Gradient Descent during training)\n",
    "'''\n",
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4119ff15-7e87-4b5b-8cdf-f943e8be3a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.2688321 , 0.17283237],\n",
       "       [0.3780378 , 0.9709221 ],\n",
       "       [0.97996306, 0.87893903],\n",
       "       [0.97996306, 0.87893903]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Now lets encode the same batch of categorical features as earlier, but this time using these embeddings.\n",
    "\n",
    "The tf.nn.embedding_lookup() function looks up the rows in the embedding matrix, at the given indices-- thats all it does.\n",
    "'''\n",
    "categories = tf.constant(['NEAR BAY', 'DESERT', 'INLAND', 'INLAND'])\n",
    "cat_indices = table.lookup(categories)\n",
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26b25890-3125-4a35-a920-6872b5d7dcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-0.04671315,  0.02867899],\n",
       "       [ 0.02541573, -0.04870735],\n",
       "       [ 0.00680504,  0.04371003],\n",
       "       [ 0.00680504,  0.04371003]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tf.keras provides a tf.keras.layers.Embedding layer that handles the embedding matrix (trainable by default);\n",
    "when the layer is created it initializes the embedding matrix randomly, and then when it is called with some category\n",
    "indices it returns the rows at those indices in the embedding matrix\n",
    "'''\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=len(vocab) + num_oov_buckets,\n",
    "    output_dim=embedding_dim\n",
    ")\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b150653-8e4e-4cac-a49d-001f9187200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Putting everything together, we can now create a tf.keras model that can process categorical features (along with regular numerical features)\n",
    "and learn an embedding for each category (as well as for each oov bucket)\n",
    "'''\n",
    "regular_inputs = tf.keras.layers.Input(shape=[8])\n",
    "categories = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "cat_indices = tf.keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = tf.keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs = tf.keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[regular_inputs, categories],\n",
    "    outputs=[outputs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6246cc69-44d0-4230-aa84-b38cbd9ffea5",
   "metadata": {},
   "source": [
    "__In this example we are using 2D embeddings, but as a rule of thumb embeddings typically have 10 to 300 dimensions, depending on the task and the vocabulary size__.\n",
    "\n",
    "This model takes two inputs: a regular input containing 8 numerical features per instance, plus a categorical input (contianing one categorical feature per instance). It uses a Lambda layer to look up each category's index, then it looks up the embeddings for these indices. Next, it concatenates the embeddings and the regular inputs in order to give the encoded inputs, which are ready to be fed to a neural network. We could add any kind of neural network at this point, but we just add a dense output layer, and we creat the tf.keras.Model.\n",
    "\n",
    "When the tf.keras.layers.TextVectorization layer is available, you can call its adapt() method ot make it extract the vocabulary from a data sample (it will take care of creating the lookup table for you). Then you can add it to your model, and it will perform the index lookup (replacing the Lambda layer in the previous example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaddc10-7fd8-4828-9261-fcb07eba8b5d",
   "metadata": {},
   "source": [
    "One-hot encoding followed by a Dense layer (with no activation function and no biases) is equivalent to an Embedding layer. However, the Embedding layer uses way fewer computations (the performance difference becomes clear when the size of the embedding matrix grows). The Dense layer's weight matrix plays the role of the embedding matrix. For example, __using one-hot vectors of size 20 and a Dense layer with 10 units is equivalent to use an Embedding layer with input_dim=20 and output_dim=10. As a result, it would be wasteful to use more embedding dimensions than the number of units in the layer that follows the Embedding layer.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b7fee-7831-4da3-bb9c-88e9e55556b8",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1799e-9f92-44aa-a9f2-728cd8ac8715",
   "metadata": {},
   "source": [
    "Not only will embeddings generally be useful representations for the task at hand, but quite often these same embeddings can be reused successfully for other tasks. The most common example of this is _word embeddings_: when you are working on a natural language processing task, you are often better off reusing pretrained word embeddings than training your own.\n",
    "\n",
    "In 2013 Google researchers published a paper describing an efficient technique to learn word embeddings using neural networks. They trained a neural network to predict the words near any given word, and obtained astounding word embeddings. For example, synonyms had very close embeddings, and semantically related words such as France, Spain, and Italy ended up clustered together.\n",
    "\n",
    "It's not just about proximity, though: word embeddings were also organized along meaningful axes in the embedding space. HEre is a famous example: if you compute King - Man + Woman (add and subtracting the embedding vectors of these words), then the result will be very close to the embedding of the word Queen.\n",
    "\n",
    "In other words, the word embeddings encode the concept of gender! Similary, you can compute Madrid - Spain + France and the result is close to Paris, which seems to show that the notion of captial city was also encoded in the embeddings.\n",
    "\n",
    "Unfortunately, word embeddings sometimes capture biases, such as learning Man is to Doctor as Woman is to Nurse. Ensuring fairness in Deep Learning algorithms is an important and active research topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583448e4-227b-4c57-937f-451b10053d7e",
   "metadata": {},
   "source": [
    "### Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fb766-3515-4d3f-87ba-7b4803c67a40",
   "metadata": {},
   "source": [
    "The TensorFlow team is working on providing a set of standard Keras preprocessing layers. This new API will likely supersede the existing Feature Columns API, which is harder to use and less intuitive (if you want to learn more about the Feature Columns API anyway pleace check the notebook for this chapter).\n",
    "\n",
    "We already discussed two of these layers: __the tf.keras.layers.Normalization layer that will perform feature standardization__ (it will be equivalent to the Standardization layer we defined earlier), and __the TextVectorization layer that will be capable of encoding each word in the inputs into its index in the vocabulary__. In both cases, you create the layer, you call its adapt() method on a data sample, and then you use the layer normally in your model. The other preprocessing layers will follow the same pattern. \n",
    "\n",
    "The API will also include a __tf.keras.layers.Discretization layer that will chop continuous data into different bins and encode each bin as a one-hot vector. For example, you could use it to discretize prices into three categories (low, medium, high), which would be encoded as [1, 0 0], [0, 1, 0], and [0, 0, 1], respectively__. Of course, this loses a lot of information, but in some cases it can help the model detect patterns that would be otherwise not obvious when just looking at hte continuous values.\n",
    "\n",
    "__The Discretization layer will not be differentiable, and it should only be used at the start of your model. Indeed, the model's preprocessing layers will be frozen during training, so their parameters will not be affected by Gradient Descent, and thus they do not need to be differentiable. This also means that you should not use an Embedding layer directly in a custom preprocessing layer, if you want it to be trainable: instead, it should be added separately to your model, as in the previous code example.__\n",
    "\n",
    "___It will also be possible to chain multiple preprocessing layers using the PreprocessingStage class. For example, the following code will create a preprocessing pipeline that will first normalize the inputs, then discretize them (this may remind you of Scikit-Learn pipelines). After you adapt this pipeline to a data sample, you can use it like a regular layer in your models.___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc181585-09dc-4ecf-b9a8-6413a6bf1369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is depreciated and no longer exists in the TF API. There are other ways to create preprocessing pipelines. TF Transform, for example'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' This is depreciated and no longer exists in the TF API. There are other ways to create preprocessing pipelines. TF Transform, for example'\n",
    "# normalization = tf.keras.layers.Normalization()\n",
    "# discretization = tf.keras.layers.Discretization([...])\n",
    "# pipeline = tf.keras.layers.PreprocessingStage([normalization, discretization])\n",
    "# pipeline.adapt(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7dcbc9-500a-43f2-8188-fe41dbe88a8b",
   "metadata": {},
   "source": [
    "The TextVectorization layer will also have an option to output word-count vectors instead of word indices. For example, if the vocabulary contains three words, say ['and', 'basketball', 'more'], then the text 'more and more' will be mapped to the vector [1, 0, 2]: the word 'and' appears once and the word 'basketball' does not appear at all, and the word 'more' appears twice. This text representation is called a _bag of words_, since it completely loses the order of the words. Common words like 'and' will have a large value in most texts, even though they are usually the least interesting (e.g. in the text 'more and more basketball' the word 'basketball' is clearly the most important, precisely because it is not a very frequent word).\n",
    "\n",
    "__So, the word ccounts should be normalized in a way that reduces the importance of frequent words. A common way to do this is to divide each word count by the log of the total number of training instances in which the word appears.__ This technique is called _Term-Frequency x Inverse-Document-Frequency_ (TF-IDF).\n",
    "\n",
    "For example, let's imagine that the words 'and', 'basketball', and 'more' appear respectively in 200, 10 and 100 text instances in the training set: in this case, the final vector will be [ 1/log(200), 0/log(10), 2/log(100) ], which is aproximately equal to [0.19, 0., 0.43]. The TextVectorization layer will (likely) have an option to perform TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2a2e5-0be9-4366-9e5d-a2712836c8db",
   "metadata": {},
   "source": [
    "If the standard preprocessing layers are insufficient for your task, you will still have the option to create your own custom preprocessing layer, much like we did earlier with the Standardization class.\n",
    "\n",
    "Create a subclass of the tf.keras.layers.PreprocessingLayer class with an adapt() method, which should take a data_sample argument and optionally an extra reset_state argument: if True, then the adapt() method should reset any existing state before computing the new state, if False, it should try to update the existing state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edf212-57ae-44c5-a15e-6e088f12808d",
   "metadata": {},
   "source": [
    "## TF Transform (TFX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e12e5-0f85-4bc7-950e-ac3051b1be04",
   "metadata": {},
   "source": [
    "tf.Transform makes it possible to write a single preprocessing function that can be run in batch mode on your full training set, before training (to speed it up), and then exported to a TF Function and incorporated into your trained model so that once it is deployed in production it can take care of preprocessing new instances on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8936bc-716d-4258-97fc-737bedc44dad",
   "metadata": {},
   "source": [
    " If preprocessing is computationally expensive, then handling it before training rather than on the fly may give you a significant speedup: the data will be preprocessed just once per instance _before_ training, rather than once per instance _during_ trianing. This is what TF Transform was designed for.\n",
    "\n",
    "First, to use a TFX component such as TF Transform, you must install it; it does not come bundled with TensorFlow. Here is what the preprocessing function might look like if we just had two features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc10db4f-7792-4ae0-bcda-53e16cf4e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00065124-763e-4a63-904d-4cdfb4247655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "\n",
    "def preprocess(inputs): # inputs = a batch of input features\n",
    "    median_age = inputs['housing_median_age']\n",
    "    ocean_proximity = inputs['ocean_proximity']\n",
    "    standardized_age = tft.scale_to_z_score(median_age)\n",
    "    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "    return {\n",
    "        'standardized_median_age': standardized_age,\n",
    "        'ocean_proximity_id': ocean_proximity_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829431-60fe-4d45-a283-380a6fb5c37c",
   "metadata": {},
   "source": [
    "Next, TF Transform lets you apply this preprocess() function to the whole training set using Apache Beam (it provides an AnalyzeAndTransformDataset class that you can use for this purpose in your Apaceh Beam pipeline). In this process, it will also compute all the necessary statistics over the whole training set: in this example, the mean and standard deviation of the housing_median_age feature, and the vocabulary for the ocean_proximity feature.\n",
    "\n",
    "Importantly, TF Transform will also generate an equivalent TensorFlow function that you can plug into the model you deploy. With the Data API, TFRecord, the Keras preprocessing layers, and TF Transform, you can build highly scalable input pipelines for training and benefit from fast and portable preprocessing in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731d89a-d345-45bc-8a0e-fa7ae6bfb007",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a30b3-c8af-4309-83cb-66c4dc014634",
   "metadata": {},
   "source": [
    "TFDS provides a convenient function to download many common datasets of all kinds, including large ones like ImageNet, as well as convenient dataset objects to manipulate them using the Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466caa7-6611-4328-b213-8438a5d4de40",
   "metadata": {},
   "source": [
    "The TensorFlow Datasets project makes it very easy to download common datasets, from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet. \n",
    "\n",
    "You can visit https://homl.info/tfds to view the full list, along with a description of each dataset. TFDS is not bundled with TensorFlow, so you need to install the tensorflow-datasets library. \n",
    "\n",
    "Then call the tfds.load() function and it will download the data you want. The load() function shuffles each data shard it downloads (only for the training set). This may not be sufficient, so it's best to shuffle the training data some more.\n",
    "\n",
    "Note that each item in the dataset is a dictionary containing both the features and the labels. But tf.Keras expects each item to be a tuple containing two elements (again, the features and the labels.) Ask the load() function to do this for you by setting as_supervised=True (obviously this works only for labeled datasets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10f2f258-b2a1-40ab-8448-9934a2927b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73dfb8c4-9725-4716-8ad6-de67e939af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist_dataset = tfds.load('mnist', batch_size=32, as_supervised=True)\n",
    "mnist_train, mnist_test = mnist_dataset.get('train').shuffle(10000).prefetch(1), mnist_dataset.get('test')\n",
    "\n",
    "imdb_dataset = tfds.load('imdb_reviews', batch_size=32, as_supervised=True)\n",
    "imdb_train, imdb_test = imdb_dataset.get('train').shuffle(10000).prefetch(1), imdb_dataset.get('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c1456-5cb8-459d-9e86-58ad32ce9e9e",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a97d99-bae0-4e55-ab49-5bff99a01cea",
   "metadata": {},
   "source": [
    "__1. Why would you want to use the Data API?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca83ad-1f28-4ff2-99ef-f388bde22744",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    The Data API makes ingesting and transforming large datasets effecient. It is also native to TensorFlow and as such is optimized to feed into TensorFlow pipelines / models, port, and is compatible with TF Functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87eae9c-64a4-42d1-835d-361ad91a6a22",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    Ingesting a large dataset and preprocessing it efficiently can be a complex engineering challenge. The Data API makes it fairly simple. It offers many features, including loading data from various sources (such as text or binary files), reading data in parallel from multiple sources, transforming it, interleaving the records, shuffling the data, batching it, and prefetching it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac09939-33dc-4732-a7c0-c3041713b287",
   "metadata": {},
   "source": [
    "__2. What are the benefits of splitting a large dataset into multiple files?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773b72-ee66-4a20-beba-f469a348f05a",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    One benefit is that the files can be read in parallel which can greatly increase how fast the data is loaded. Another benefit is the data can be more thoroughly shuffled when interleaved in parallel and shuffled before, during and after the data is loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2eb9a-b6db-4fe3-bead-6ac2efd58115",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    Splitting a large dataset into multiple files makes it possible to shuffle it at a coarse level before shuffling it at a finer level using a shuffling buffer. It also makes it possible to handle huge datasets taht do not fit on a single machine. It's also simpler to manipulate thousands of small files rather than one huge file; for example, it's easier to split the data into multiple subsets. lastly, if the data is split across multiple files spread across multiple servers, it is possible to download serveral files from different servers simultaneously, which improves the bandwidth usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f6676-dc56-4079-83cc-542f0ec06ef1",
   "metadata": {},
   "source": [
    "__3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffd7b1-bc3b-4f31-9b7d-41358aeac1e9",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    This isn't specifically mentioned in the chapter but one way to tell is probably by using TensorBoard and seeing which part of the model is taking the longest to complete, isolating the data ingestion part. If you're using large CSV files and the model is training slow it might be worth converting the CSV files to TFRecords and seeing if that boosts performance. You can further boost performance, specifically if the data is being processed via network, by compressing the TF Records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5757d7-0cf4-4a55-b39c-a6c75623b6d1",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck. You can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches. If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. You can also try saving the dataset into multiple TFRecord files, and if necessary, perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training (TF Transform can help with this). If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa915c5-f3a5-49c8-9eef-5f9e50629dae",
   "metadata": {},
   "source": [
    "__4. Can you save any binary data to the TFRecord file, or only serialized protocol buffers?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8f2e8-b31c-4881-821f-26a5872413b8",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    You can save binary data to a TFRecord but typically TFRecords are composed of serialized protocol buffers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f2bee-2d79-4432-bb61-020cff7a4412",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    A TFRecord file is composed of a sequence of arbitrary binary records: you can store absolutely any binary data you want in each record. However, in practice most TFRecord files contain sequences of serialized protocol buffers. This makes it possible to benefit from the advantages of protocol buffers, such as the fact that they can be read easily across multiple platforms and languages and their definition can be updated later in a backward-compatible way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c8073-49b4-4c8c-ae01-329a90aa8919",
   "metadata": {},
   "source": [
    "__5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cd0076-78e6-48ce-a6ab-c036a945b61f",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    One reason would be because the Example protobuf is native to TensorFlow and therefore TensorFlow has parsing operations designed for it. Another is because in the future TensorFlow may add additional fields to the Example protobuf and it will be backwards compatible. Using your own protobuf definition is probably possible, but why reinvent the wheel when there is already an optimized format built into TensorFlow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad6ec1d-a23d-4eda-a9f3-b2f26bca38a6",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    The Example protobuf format has the advantage that TensorFlow provides some operations to parse it (the tf.io.parse_example() functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets. However, if it does not cover your use case, you can define your own protocol buffer, compile it using protoc (setting the --descriptor_set_out and --include_imports arguments to export the protobuf descriptor), and use the tf.io.decode_proto() function to parse the serialized protobufs. It's more complicated and it requries deploying the descriptor along with the model, but it can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3259d9-c4e1-4877-bfa9-21d72428f81f",
   "metadata": {},
   "source": [
    "__6. When using TFRecords, when would you want to activate compression? Why not do it systematically?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70be22-c2cd-4065-949e-c2980cb9b7e6",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    The example provided in the text is if the TFRecords need to be loaded via network connection. When you compress the files and subsequently read them you need to provide a compression type. One problem that can arise from compressing TFRecords systematically is this compression type is abstracted away or otherwise unavailble downstream when the data is loaded, making the data unavailable. Another reason is because for most practical purposes it is unnecessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4479b9e-439e-4405-811c-9e45d4df6390",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    When using TFRecords, you will generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thsu reduce download time. But if the files are located on the same machine as the training script, it's usually preferable to leave compression off, to avoid wasting CPU for decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c1b0c-0c11-4154-9c23-1e458e7a3cf4",
   "metadata": {},
   "source": [
    "__7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ffa19-8e28-4c2a-b050-2c221a0a5dbd",
   "metadata": {},
   "source": [
    ">My Answer: For this question I'll use the book answer to avoid speculation/guessing since this isn't explicity covered in the chapter.\n",
    "\n",
    "1. Directly when writing the data files:\n",
    "    * Pros: Training script will run faster, possibly smaller than the original data so you can save space and speed up downloads. Also makes it easier to inspect or archive the preprocessed data.\n",
    "    * Cons: Not easy to experiment with different preprocessing logics, data augmentation requires making many copies/variants of the dataset, the trained model will expect preprocessed data so the preprocessing code needs to be deployed alongside the model\n",
    "3. Within the tf.data pipeline\n",
    "    * Pros: Much easier to tweak the preprocessing logic and apply data augmentation, tf.data makes it easy to build highly efficient preprocessing pipelines (multithreading, prefetching , etc.)\n",
    "    * Cons: Will slow down training, each training instance will be preprocessed once per batch rather than just once, the trained model will still expect preprocessed data, the same as above.\n",
    "5. Preprocessing layers within the model\n",
    "    * Pros: Only need to write the preprocessing code once for both training and inference, consistent because part of the model\n",
    "    * Cons: Slow down training because preprocessing happens once per epoch, unless implemented after this book was written parallel operations and prefetching is unavailable. Though Keras is expected to deploy preprocessing layers that can run as part of the tf.data pipeline in the future.\n",
    "7. TF Transform\n",
    "    * Pros: Many of the benefits from the previous options\n",
    "    * Cons: Learning how to use this tool properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3af23-d33e-4db0-9b72-f0e1e8c0639b",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "1. Directly when writing the data files:\n",
    "    * Pros: Training script will run faster, possibly smaller than the original data so you can save space and speed up downloads. Also makes it easier to inspect or archive the preprocessed data.\n",
    "    * Cons: Not easy to experiment with different preprocessing logics, data augmentation requires making many copies/variants of the dataset, the trained model will expect preprocessed data so the preprocessing code needs to be deployed alongside the model\n",
    "3. Within the tf.data pipeline\n",
    "    * Pros: Much easier to tweak the preprocessing logic and apply data augmentation, tf.data makes it easy to build highly efficient preprocessing pipelines (multithreading, prefetching , etc.)\n",
    "    * Cons: Will slow down training, each training instance will be preprocessed once per batch rather than just once, the trained model will still expect preprocessed data, the same as above.\n",
    "5. Preprocessing layers within the model\n",
    "    * Pros: Only need to write the preprocessing code once for both training and inference, consistent because part of the model\n",
    "    * Cons: Slow down training because preprocessing happens once per epoch, unless implemented after this book was written parallel operations and prefetching is unavailable. Though Keras is expected to deploy preprocessing layers that can run as part of the tf.data pipeline in the future.\n",
    "7. TF Transform\n",
    "    * Pros: Many of the benefits from the previous options\n",
    "    * Cons: Learning how to use this tool properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69731edf-b260-4b63-a7b9-6232e5ae95e5",
   "metadata": {},
   "source": [
    "__8. Name a few common techniques you can use to encode categorical features. What about text?__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ab610-6d03-48bc-bf8f-2d573f7b4aad",
   "metadata": {},
   "source": [
    ">My Answer:\n",
    "\n",
    "    For categorical features some common techniques include ranking (when there is a natural order to the categories such that numerical differences model the categories well: ex. Cold, Warm, Hot), one-hot encoding, discretization and embedding. For text, embedding is also a common technique as is bag-of-words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91603cfd-d68c-462c-bbc6-3a7abe9f685f",
   "metadata": {},
   "source": [
    ">Book Answer:\n",
    "\n",
    "    Let's look at how to encode categorical features and text:\n",
    "    \n",
    "    To encode categorical features with natural order, such as a movie rating (bad, average, good), the simplest option is to use ordinal encoding: sort the categories by their natural order and map each category to its rank. However, most categorical features don't have such a natural order. For example, there's no natural order for professions or countries. In this case, you can use one-hot encoding, or, if there are many categories, embeddings.\n",
    "    \n",
    "    For text, one option is to use bag-of-words representation: a sentence is represented by a vector counting the counts of each possible word. Since common words are usually not very important, you'll want to use TF-IDF to reduce their weight. Instead of counting words, it is also common to count n_grams, which are sequences of n consecutive words. Alternatively, you can encode each word using word embeddings, possibly pretrained. Rather than encoding words, it is also possible to encode each letter, or sub-word tokens (e.g. splitting 'smartest' into 'smart' and 'est'). These last two options are discussed in Chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c19d5-37b4-4383-8334-d76363795b7b",
   "metadata": {},
   "source": [
    "__9. Load the Fashion MNIST dataset; split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized Example protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image), and the label. Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59722133-7dae-4408-8ebc-41ab3340db81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2493 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.1429 - val_sparse_categorical_accuracy: 0.9590\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.1041 - sparse_categorical_accuracy: 0.9687 - val_loss: 0.0998 - val_sparse_categorical_accuracy: 0.9708\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0699 - sparse_categorical_accuracy: 0.9785 - val_loss: 0.0918 - val_sparse_categorical_accuracy: 0.9737\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0497 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.0841 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0373 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0869 - val_sparse_categorical_accuracy: 0.9747\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0285 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0822 - val_sparse_categorical_accuracy: 0.9783\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9929 - val_loss: 0.0804 - val_sparse_categorical_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 4s 3ms/step - loss: 0.0185 - sparse_categorical_accuracy: 0.9939 - val_loss: 0.0900 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0139 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.0900 - val_sparse_categorical_accuracy: 0.9782\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 5s 3ms/step - loss: 0.0124 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.0968 - val_sparse_categorical_accuracy: 0.9770\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0734 - sparse_categorical_accuracy: 0.9779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07337647676467896, 0.9779000282287598]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.train import BytesList, FloatList, Int64List, Feature, Features, Example\n",
    "from tensorflow.keras.layers import Input, InputLayer, Dense, Flatten, Reshape\n",
    "\n",
    "# Load the mnist dataset into a train, val and test set.\n",
    "mnist_train, mnist_val, mnist_test = tfds.load('mnist', batch_size=1000, split=['train[:90%]', 'train[90%:]', 'test'], as_supervised=True)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Write the training data to TFRecords\n",
    "for index, pair in enumerate(mnist_train):\n",
    "    \n",
    "    # Extract image and label from tuple\n",
    "    image = pair[0]\n",
    "    label = pair[1]\n",
    "    \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    serialized_image = tf.io.serialize_tensor(image).numpy()\n",
    "    serialized_label = tf.io.serialize_tensor(label).numpy()\n",
    "    instance_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'image': Feature(bytes_list=BytesList(value=[serialized_image])),\n",
    "            'label': Feature(bytes_list=BytesList(value=[serialized_label]))\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # Write to .tfrecord file\n",
    "    with tf.io.TFRecordWriter(f'Exercise_9_Files/mnist_training_tfrecords/{index}_mnist_train.tfrecord') as f:\n",
    "        f.write(instance_example.SerializeToString())\n",
    "\n",
    "# Write the validation data to TFRecords\n",
    "for index, pair in enumerate(mnist_val):\n",
    "\n",
    "    # Extract image and label from tuple\n",
    "    image = pair[0]\n",
    "    label = pair[1]\n",
    "    \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    serialized_image = tf.io.serialize_tensor(image).numpy()\n",
    "    serialized_label = tf.io.serialize_tensor(label).numpy()\n",
    "    instance_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'image': Feature(bytes_list=BytesList(value=[serialized_image])),\n",
    "            'label': Feature(bytes_list=BytesList(value=[serialized_label]))\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # Write to .tfrecord file\n",
    "    with tf.io.TFRecordWriter(f'Exercise_9_Files/mnist_validation_tfrecords/{index}_mnist_val.tfrecord') as f:\n",
    "        f.write(instance_example.SerializeToString())\n",
    "\n",
    "# Write the test data to TFRecords\n",
    "for index, pair in enumerate(mnist_test):\n",
    "\n",
    "    # Extract image and label from tuple\n",
    "    image = pair[0]\n",
    "    label = pair[1]\n",
    "    \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    serialized_image = tf.io.serialize_tensor(image).numpy()\n",
    "    serialized_label = tf.io.serialize_tensor(label).numpy()\n",
    "    instance_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            'image': Feature(bytes_list=BytesList(value=[serialized_image])),\n",
    "            'label': Feature(bytes_list=BytesList(value=[serialized_label]))\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    # Write to .tfrecord file\n",
    "    with tf.io.TFRecordWriter(f'Exercise_9_Files/mnist_test_tfrecords/{index}_mnist_test.tfrecord') as f:\n",
    "        f.write(instance_example.SerializeToString())\n",
    "\n",
    "# Define the file lists\n",
    "training_files = glob.glob(\"Exercise_9_Files/mnist_training_tfrecords/*.tfrecord\")\n",
    "validation_files = glob.glob(\"Exercise_9_Files/mnist_validation_tfrecords/*.tfrecord\")\n",
    "test_files = glob.glob(\"Exercise_9_Files/mnist_test_tfrecords/*.tfrecord\")\n",
    "\n",
    "# Mappings\n",
    "feature_description = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def map_parse_example(proto, description=feature_description):\n",
    "    parsed_example = tf.io.parse_single_example(proto, description)\n",
    "    parsed_tensor = (\n",
    "        tf.io.parse_tensor(parsed_example.get('image'), out_type=tf.uint8),\n",
    "        tf.io.parse_tensor(parsed_example.get('label'), out_type=tf.int64))\n",
    "    return parsed_tensor\n",
    "\n",
    "def shuffle_and_rebatch(dataset, batch_size=1, prefetch_size=1):\n",
    "    dataset = dataset.shuffle(buffer_size=dataset.cardinality()).batch(batch_size).prefetch(prefetch_size)\n",
    "    return dataset\n",
    "\n",
    "# Preprocessing Layer\n",
    "class Standardization(tf.keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        return data_sample\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs / 255.\n",
    "\n",
    "# Read and process data\n",
    "train_dataset = tf.data.TFRecordDataset(training_files, num_parallel_reads=8, name='tfrecord_training_data_ingestion').map(map_parse_example).unbatch()\n",
    "train_dataset = shuffle_and_rebatch(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.TFRecordDataset(validation_files, num_parallel_reads=8, name='tfrecord_validation_data_ingestion').map(map_parse_example).unbatch()\n",
    "val_dataset = shuffle_and_rebatch(val_dataset, batch_size=BATCH_SIZE, prefetch_size=AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_files, num_parallel_reads=8, name='tfrecord_test_data_ingestion').map(map_parse_example).unbatch()\n",
    "test_dataset = shuffle_and_rebatch(test_dataset, batch_size=BATCH_SIZE)\n",
    "     \n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(28, 28, 1)),\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Standardization(),\n",
    "    Dense(200, activation='relu', name='layer1'),\n",
    "    Dense(10, name='output'),\n",
    "])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=3,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='TensorBoard/',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, tb]\n",
    "\n",
    "# Optimizer\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.999,\n",
    "    ema_overwrite_frequency=None,\n",
    "    name='nadam'\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(x=train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce04ca4-f0b2-4d5e-9f2a-6b85afbf209a",
   "metadata": {},
   "source": [
    "10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it effeciently, then build and train a binary classification model containing an Embedding layer\n",
    "    1. Download the Large Movie Review Dataset which contains 50,000 movies reviews from the Internet Movie Database. This data is organized in two directories, _train_ and _test_, each containing a _pos_ subdirectory with 12,500 positive reviews and a _neg_ subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders but we will ignore them for this exercise.\n",
    "    2. Split the test set into a validation set (15,000) and a test set (10,000).\n",
    "    3. Use tf.data to create an efficient dataset for each set\n",
    "    4. Create a binary classification model, using a TextVectorization layer to preprocess each review.\n",
    "    5. Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words. This rescaled mean embedding can then be passed to the rest of your model\n",
    "    6. Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible\n",
    "    7. Use TFDS to load this same dataset more easily: tfds.load('imdb_reviews')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a746d877-7fc7-4ebb-a3f6-26f14217d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Input, InputLayer, Dense, Flatten, Reshape, TextVectorization, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EMBEDDING_DIM = 100\n",
    "VOCABULARY_SIZE = 1\n",
    "SAMPLE_SIZE = 300\n",
    "\n",
    "# Load the data\n",
    "imdb_train, imdb_val, imdb_test = tfds.load('imdb_reviews', batch_size=BATCH_SIZE, split=['train[:90%]', 'test[:60%]', 'test[60%:]'], as_supervised=True)\n",
    "\n",
    "# Adapt the TextVectorization\n",
    "adapt_sample = tf.Variable([x[0] for x in imdb_train.unbatch().take(SAMPLE_SIZE)])\n",
    "text_vectorization = TextVectorization(output_mode = 'tf_idf', sparse=False)\n",
    "text_vectorization.adapt(adapt_sample)\n",
    "VOCABULARY_SIZE = text_vectorization.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d00866e1-f432-4cc6-aa97-91c257073672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11720"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in tf.Variable([x[0] for x in imdb_train.take(1).unbatch()]):\n",
    "#     print(x)\n",
    "\n",
    "text_vectorization.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e715cde-a8db-4538-89d8-f84d6cd40c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x in imdb_train.unbatch().take(10):\n",
    "    print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d8f478e-1b2e-4c43-b6f9-fa7e64cc06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1407/1407 [==============================] - 20s 13ms/step - loss: 0.4564 - binary_accuracy: 0.7646 - val_loss: 0.4088 - val_binary_accuracy: 0.8049\n",
      "Epoch 2/20\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 0.3546 - binary_accuracy: 0.8376 - val_loss: 0.4167 - val_binary_accuracy: 0.8001\n",
      "Epoch 3/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.3255 - binary_accuracy: 0.8558 - val_loss: 0.4262 - val_binary_accuracy: 0.8006\n",
      "Epoch 4/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.3103 - binary_accuracy: 0.8662 - val_loss: 0.4335 - val_binary_accuracy: 0.8007\n",
      "Epoch 5/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.3020 - binary_accuracy: 0.8716 - val_loss: 0.4372 - val_binary_accuracy: 0.8026\n",
      "Epoch 6/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2975 - binary_accuracy: 0.8747 - val_loss: 0.4392 - val_binary_accuracy: 0.8045\n",
      "Epoch 7/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2951 - binary_accuracy: 0.8763 - val_loss: 0.4401 - val_binary_accuracy: 0.8052\n",
      "Epoch 8/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2939 - binary_accuracy: 0.8767 - val_loss: 0.4407 - val_binary_accuracy: 0.8046\n",
      "Epoch 9/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2932 - binary_accuracy: 0.8770 - val_loss: 0.4411 - val_binary_accuracy: 0.8045\n",
      "Epoch 10/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2929 - binary_accuracy: 0.8776 - val_loss: 0.4413 - val_binary_accuracy: 0.8048\n",
      "Epoch 11/20\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 0.2927 - binary_accuracy: 0.8779 - val_loss: 0.4414 - val_binary_accuracy: 0.8049\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Input, InputLayer, Dense, Flatten, Reshape, TextVectorization, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EMBEDDING_DIM = 100\n",
    "VOCABULARY_SIZE = 10000\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "# Load the data\n",
    "imdb_train, imdb_val, imdb_test = tfds.load('imdb_reviews', batch_size=BATCH_SIZE, split=['train[:90%]', 'test[:60%]', 'test[60%:]'], as_supervised=True)\n",
    "\n",
    "# Adapt the TextVectorization\n",
    "adapt_sample = tf.Variable([x[0] for x in imdb_train.unbatch().take(SAMPLE_SIZE)])\n",
    "text_vectorization = TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=EMBEDDING_DIM\n",
    ")\n",
    "text_vectorization.adapt(adapt_sample)\n",
    "# VOCABULARY_SIZE = text_vectorization.vocabulary_size()\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(1,), dtype=tf.string),\n",
    "    text_vectorization,\n",
    "    Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, name=\"embedding\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(50, activation='relu', name='layer1'),\n",
    "    Dense(50, activation='relu', name='layer2'),\n",
    "    Dense(50, activation='relu', name='layer3'),\n",
    "    Dense(1, name='output')\n",
    "])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=0\n",
    ")\n",
    "\n",
    "tb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='TensorBoard/',\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, tb]\n",
    "\n",
    "# Optimizer\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.50)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    weight_decay=None,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    name='nadam'\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "# Cache and prefetch the datasets\n",
    "imdb_train = imdb_train.prefetch(AUTOTUNE)\n",
    "imdb_val = imdb_val.prefetch(AUTOTUNE)\n",
    "imdb_test = imdb_test.prefetch(AUTOTUNE)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(x=imdb_train, validation_data=imdb_val, epochs=20, callbacks=callbacks)\n",
    "model.evaluate(imdb_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
