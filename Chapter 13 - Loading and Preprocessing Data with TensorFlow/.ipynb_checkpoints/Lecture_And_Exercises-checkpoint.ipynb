{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88471804-169b-478a-94b8-2129590b17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af8b62-17bc-4e11-a57b-aa48fa1bcbba",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab14e7a-85e7-4e9e-b42d-3021090e65f0",
   "metadata": {},
   "source": [
    "Ingesting a large dataset and preprocessing it efficiently can be tricky to implement with other Deep Learning libraries, but tensorFlow makes it easy thanks to the *Data API*: you just create a dataset object, and tell it where to get the data and how to transform it.\n",
    "\n",
    "Off the shelf, the Data API can read from text files, binary files with fixed-sized records, and binary files that use TensorFlow's TFRecord format. The Data API also has support for reading from SQL databases.\n",
    "\n",
    "Reading huge datasets efficiently is not the only difficulty: the data also needs to be preprocessed, usually normalized. Moreover, it is not always composed strictly of convenient numerical fields: there may be text features, categorical features, and so on. These need to be encoded, for example using one-hot encoding, bag-of-words encoding, or _embeddings_ (as we will see, an embedding is a trainable dense vector that represents a category or token). One option to handle all this preprocessing is to write your own custom preprocessing layers. Another is to use the standard preprocessing layers provided by Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc17d2-faf2-407a-9ac6-6671a5e373cc",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b95943a-f17a-42dc-9845-b9a68498431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b0334-d7c3-4a97-a761-396362dcbc81",
   "metadata": {},
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X (along the first dimension), so this dataset contains 10 items. In this case we would have obtained the same dataset if we had used tf.data.Dataset.range(10). Note that you can iterate over the dataset's items intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab36d9c-7cb4-4dd0-8515-2781b2619136",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c4755-e476-4b57-baf0-2eec138bf2d0",
   "metadata": {},
   "source": [
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e5cbb7d-4e7d-4466-8c63-b7a0c4f91ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2dc62-a2ac-4f1c-bf9c-3a37d54c5b9a",
   "metadata": {},
   "source": [
    "In this example, we first call the repeat() method on the original dataset, and it returns a new dataset that will repeat the items of the original dataset three times. Of course, this will not copy all the data in memory three times! (If you call this method with no arguments the new dataset will repeat the srouce dataset forever, so the code that iterates over the dataset will have to decide when to stop.) Then we call the batch() method on this new dataset, and again this creates a new dataset. This one will group the items of this final dataset. As you can see, the batch() method had to output a final batch of size two instead of seven, but you can call it with drop_remainder=True if you want it to drop this final batch so that all batches have the exact same size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fe061-d96b-496c-81c8-228cd6b93a2f",
   "metadata": {},
   "source": [
    "The dataset methods do _not_ modify datasets, they create new ones, so make sure to keep a reference to these new datasets (e.g. with dataset = ...) or else nothing will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98cd6f58-2074-43ea-8b6c-49ca5d717494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a12ba5-dfe4-45e5-aded-29aa334feee8",
   "metadata": {},
   "source": [
    "You can also transform the items by calling the map() method. For example, the code above doubles the values of the original dataset. This function is the on you will call to apply any preprocessing you want to your data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speeds things up: it's as simple as setting the num_parallel_calls argument. Note that the function you pass to the map() method must be convertible to a TF Function.\n",
    "\n",
    "While the map() method applies a transformation to each item, the apply() method applies a transformation to the dataset as a whole. For example, the following code applies the unbatch() to the dataset. Each item in the new dataset will be a single-integer tensor instead of a batch of seven integers. It is also possible to simply filter the dataset using filter() or look at just a few items using take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "623c16a1-c423-41c1-9adf-39f9186d4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.unbatch() #dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24f83575-c823-4d70-bdb5-d5ee9c76ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "785424c0-5cdf-45bb-b643-58947d16294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f4de1-d513-4b32-9ee7-171e4e3c1051",
   "metadata": {},
   "source": [
    "### Shuffling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0ce65-620d-4fe1-b8f4-f8daf95afe81",
   "metadata": {},
   "source": [
    "Gradient Descent works best when the instances in the training set are independent and identically distributed. A simple way to ensure this is to shuffle the instances using the shuffle() method. You must specify the buffer size, and it is important to make it large enough, or else shuffling will not be very effective. Just don't exceed the amount of RAM you have, and even if you ahve plenty of it, there's no need to go beyond the dataset's size.\n",
    "\n",
    "For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a buffer of size 5 and a random seed of 42, then batched with a batch size of 7.\n",
    "\n",
    "If you call repeat() on a shuffled dataset, but default it will generate a new order at every iteration. This is generally a good idea, but if you prefer to reuse the same order at each iteration you can set reshuffle_each_iteration=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4a9ea71-f836-4d3a-9829-bde92766ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3).shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3f2af-0be6-4df9-b445-96bc1195fa55",
   "metadata": {},
   "source": [
    "For a large dataset that does not fit into memory, this simple shuffling-buffer approach may not be sufficient. One solution is to shuffle the source data itself. Then, to shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the shuffle() method. The Data API makes all of this possible in just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b47f3-3a6e-493e-a41f-5e6f47df8458",
   "metadata": {},
   "source": [
    "#### Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1fb17-e4e6-4512-a349-6f018c797855",
   "metadata": {},
   "source": [
    "The following example assumes the Califorina housing dataset has been downloaded, split into train, val, and test sets, and that each set is further broken down into multiple csv files. I'm not going to do this because it is unncessary bloat on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3776d9-cdf0-4f81-a914-b731951ecc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_filepaths = ['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv', ...]\n",
    "# filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "# n_readers = 5\n",
    "# dataset = filepath_dataset.interleave(\n",
    "#     lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "#     cycle_length=n_readers\n",
    "# )\n",
    "\n",
    "# for line in dataset.take(5):\n",
    "#     print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572e3ad-ada1-43f5-98cc-d07542c6c804",
   "metadata": {},
   "source": [
    "Let's suppose train_filepaths contains the list of training file paths. By default, the list_files() function returns a dataset that shuffles the file paths. In general, this is a good thing, but you can set shuffle=False if you do not want that for some reason.\n",
    "\n",
    "Next, you can call the interleave() method to read from five files (n_readers) at a time and interleave their lines (skipping the first line of each file, which is the header row, using the skip() method).\n",
    "\n",
    "The interleave() method will create a dataset that will pull five file paths from the filepath_dataset, and for each one it will call the function you feed it (a lambda in this example) to create a new dataset (in this case a TextLineDataset).\n",
    "\n",
    "For interleaving to work best, it is preferable to have files of identical length; otherwise the ends of the longest files will not be interleaved.\n",
    "\n",
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the num_parallel_calls argument to the number of threads you want. Alteratively, you can set num_parallel_calls=tf.data.experimental.AUTOTUNE for TensorFlow to choose the right number of threads dynamically based on the availble CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082840-1876-42b0-b36e-811aac64a76c",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5443a867-8adc-4617-bfa6-1386701de09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_mean, x_std = [...] # mean and scale of each feature in the training set\n",
    "# n_inputs = 8\n",
    "\n",
    "# def preprocess(line):\n",
    "#     defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "#     fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "#     x = tf.stack(fields[:-1])\n",
    "#     y = tf.stack(fields[-1:])\n",
    "#     return (x - x_mean) / x_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a2416-cf46-40b2-8dbc-c0bb3fe72226",
   "metadata": {},
   "source": [
    "Let's walk through the code above:\n",
    "\n",
    "1. First, the code assumes that we have precomputed the mean and standard deviation of each feature in the training set. x_mean and x_std are just 1D tensors (or Numpy arrays) containing 8 floats, one per input feature.\n",
    "2. The preprocess() function takes one CSV line and starts by parsing it. For this it uses the tf.io.decode_csv() function, which takes 2 arguments: the first is the line to parse, and the second is an array containing the default value for each column in CSV File. This array tells TensorFlow not only the default value for each column, but also what number of columns and their types. In this example, we tell it that all feature columns are floats and that missing values should default to 0, but we provide an empty array of tf.float32 as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters the missing value.\n",
    "3. The decode_csv() function returns a list of scalar tensors (one per column), but we need to return 1D tensor arrays. So we call tf.stack() on all tensors except the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value (this makes it a 1D tensor array with a single value, rather than a scalar tensor).\n",
    "4. Finally, we scale the input features by subtracting the feature means and then dividing by the feature standard deviations, and we return a tuple containing the scaled features and target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb1e76-1b5c-46ec-85cc-a8c37eb36bd2",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc88eed-6c46-444c-bcc9-ae06ae614dbb",
   "metadata": {},
   "source": [
    "To make the code reusable, let's put together everything we have discussed so far int oa small helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70e8468d-44e7-415e-81fd-f8e43857843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(\n",
    "    filepaths,\n",
    "    repeat=1,\n",
    "    n_readers=5,\n",
    "    n_read_threads=None,\n",
    "    shuffle_buffer_size=1e5,\n",
    "    n_parse_threads=5,\n",
    "    batch_size=32\n",
    "):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers,\n",
    "        num_parallel_calls=n_parse_threads\n",
    "    )\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd70c3b-17d1-4f79-8f8d-d35e3bf0fc70",
   "metadata": {},
   "source": [
    "### Prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d780-50eb-4bc2-ac46-51a911b69e26",
   "metadata": {},
   "source": [
    "By calling prefetch(1) at the end, we are creating a dataset that will do its best to always be one batch ahead. In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready. This can improve performance dramatically.\n",
    "\n",
    "In general, just prefetching one batch is fine, but in some cases you may way to prefetch a few more. Alternatively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE.\n",
    "\n",
    "With prefetching, the CPU and the GPU work in parallel: as the GPU works on one batch, the CPU works on the next. If you plan to purchase a GPU, its processing power and its memory size are of course very important (in particular, a large amount of RAM is crucial for computer vision). Just as important to get good performance is its _memory bandwidth_; this is the number of gigabytes of data it can get into or out of its RAM per second.\n",
    "\n",
    "If the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset's cache() method to cache its content to RAM. This way, each instance will only be read and preprocessed once.\n",
    "\n",
    "We have discussed the most common dataset methods, but there are a few more you may want to look at:\n",
    "\n",
    "1. concatenate()\n",
    "2. zip()\n",
    "3. window()\n",
    "4. reduce()\n",
    "5. shard()\n",
    "6. flat_map()\n",
    "7. padded_batch()\n",
    "\n",
    "There are also a couple more class methods work mentioning:\n",
    "\n",
    "1. from_generator()\n",
    "2. from_tensors()\n",
    "\n",
    "which create a new dataset from a Python generator or a list of tensors, respectively. Also note that there are experimental features available in tf.data.experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71860f3b-3cec-45d7-b448-c8edf1fba7b7",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85a01998-c5f0-4221-b501-e362d5ea6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = csv_reader_dataset(train_filepaths)\n",
    "# val_set = csv_reader_dataset(val_filepaths)\n",
    "# test_set = csv_reader_dataset(test_filepaths)\n",
    "\n",
    "# model = tf.keras.models.Sequential([...])\n",
    "# model.compile([....])\n",
    "# model.fit(train_set, epochs=10, validation_data=val_set)\n",
    "\n",
    "# model.evaluate(test_set)\n",
    "# new_set = test_set.take(3).map(lambda X: y: X) # pretend we have 3 new instances\n",
    "# model.predict(new_set) # a dataset containing new instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775557e-4571-49de-9421-b87ab39c5a47",
   "metadata": {},
   "source": [
    "Now we can use the csv_reader_dataset(). Note that we do not need to repeat it, as this will be taken care of by tf.keras.\n",
    "\n",
    "If you want to build your own custom training loop (as in Chapter 12), you can just iterate over the training set, very naturally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27393231-13b8-4f11-bfd5-a66c3186b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_batch, y_batch in train_set:\n",
    "#     [...] # perform one gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581fdf7-2dd0-4524-bc19-c5f5fe4d16f0",
   "metadata": {},
   "source": [
    "In fact, it is even possible to create a TF Function (see Chapter 12) that performs the whoel training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3602701-46e9-4e89-8741-e02e1d2ca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train(model, optimizer, loss_fun, n_epochs, **kwargs): \n",
    "#     train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
    "#     for X_batch, y_batch in train_set:\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = model(X_batch)\n",
    "#             main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "#             loss = tf.add_n([main_loss] + model.losses)\n",
    "#         grads = tape.gradient(loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075989f-2ef2-4f67-867a-d83513d3a8ca",
   "metadata": {},
   "source": [
    "Note that CSV files do not support large or complex data structures (such as images or audio) very well. So let's see how to use TFRecords instead. If you are happy with CSV files (or whatever other format you are using), you do not _need_ to use TFRecords. As the saying goes, if it ain't broke, don't fix it! __TFRecords are useful when the bottleneck during training is loading and parsing data.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b0dd03-247b-44c4-8b34-8ed7128b4886",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97ab3b-2a1f-4fde-a1b8-0fddfda9501c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "274adb56-6035-44b7-a7dd-245529c362df",
   "metadata": {},
   "source": [
    "### Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304362e-5184-4d5a-a894-04754b553dbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af99bd34-7638-4e1f-a3f9-0928ea2a9731",
   "metadata": {},
   "source": [
    "### A Brief Introduction to Protocol Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6ca06-d121-491f-8f24-5919acb58a08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edac0a02-e29f-44da-9a75-c6b7137da1c1",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851670a7-a6da-4842-ad3c-df7dd08a1354",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "235e1c9c-583a-4208-a314-9161d336431e",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf42d8-5921-4a93-a8cd-891e3a1f0107",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5192a23-1011-48eb-a3e1-23cd6509b0a4",
   "metadata": {},
   "source": [
    "### Handling Lists of Lists Using the SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002868ed-1d38-4b02-9bbf-5548dc81d12d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5afc75a2-9cdb-49de-878a-a95e9db7c810",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1f6b9-217d-4717-a69d-e78323830625",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc69ce54-1da4-4cea-b17b-3c10ec496326",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc881a8-070e-4a00-9efe-6280667ebfe5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72c705f0-309c-4873-90fa-a21452734a3d",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28582af3-6a57-496d-9bb4-2b8dd5d40d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "892b7fee-7831-4da3-bb9c-88e9e55556b8",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1799e-9f92-44aa-a9f2-728cd8ac8715",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "583448e4-227b-4c57-937f-451b10053d7e",
   "metadata": {},
   "source": [
    "### Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fb766-3515-4d3f-87ba-7b4803c67a40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76edf212-57ae-44c5-a15e-6e088f12808d",
   "metadata": {},
   "source": [
    "## TF Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e12e5-0f85-4bc7-950e-ac3051b1be04",
   "metadata": {},
   "source": [
    "tf.Transform makes it possible to write a single preprocessing function that can be run in batch mode on your full training set, before training (to speed it up), and then exported to a TF Function and incorporated into your trained model so that once it is deployed in production it can take care of preprocessing new instances on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8936bc-716d-4258-97fc-737bedc44dad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d731d89a-d345-45bc-8a0e-fa7ae6bfb007",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56a30b3-c8af-4309-83cb-66c4dc014634",
   "metadata": {},
   "source": [
    "TFDS provides a convenient function to download many common datasets of all kinds, including large ones like ImageNet, as well as convenient dataset objects to manipulate them using the Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c466caa7-6611-4328-b213-8438a5d4de40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c78c1456-5cb8-459d-9e86-58ad32ce9e9e",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a97d99-bae0-4e55-ab49-5bff99a01cea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ca83ad-1f28-4ff2-99ef-f388bde22744",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87eae9c-64a4-42d1-835d-361ad91a6a22",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55074580-76e9-4971-8688-8c7e44042c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f285ca5-2767-4b01-aefd-f67a275b90ab",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb2990f-7c8c-48d0-bfba-01e9c8ca0a9c",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3185d-1084-4f4f-8482-4e6969abdaf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24880e27-3520-448c-b80f-e2365077c7c3",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11939c13-f73f-4a99-af10-8874b3f1feba",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da731d4c-74e5-4378-89e5-ca0ebe28986d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ada91e8-8386-44db-8cc6-92b13a152225",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a5613-66dc-43cc-acde-42d351f595e0",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb294604-a3bf-4ea4-8abe-0db7dd374e55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71154dce-f2e4-41be-9104-c2b32f0b2c72",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313acc9-dc4b-472b-8436-470e4f8c01f8",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a54afb-b89b-41ce-8836-22352bc37e37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b374e2df-c43a-427d-bda2-cc2eba5d0569",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982be37-5910-426a-8c5f-7789bc52b488",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384d2a6-f571-43a7-af2c-5120f695fc2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6c47630-4f84-4859-af65-5d74233b4466",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607a0d4-f0ac-4027-9740-cfaaf2db508d",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a73af-4535-49e1-b106-c18b713c9df2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf5abe9-63f4-4234-9609-890991b9ec20",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd10aee-806b-4f69-ad6e-91b1f5df9532",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186d575-592e-47ac-98e6-9e084dcd07e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c136c-de45-41a9-a0e5-09fb3e39816d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6f1d5-f46e-4eeb-af4e-9f3e6744eb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883ad1b-ab44-419a-8bb8-2f48aab32429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8677ff-7662-4c9b-8da8-03c263967e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c288a-1b4b-4f00-ab29-f70cbcf4df54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155c836-d83e-4e5d-a242-7e51024aef49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0478b77-2f6c-4481-9cfa-b24be92f0cc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a583e1-0152-455f-adb5-de7888a502f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782f6df-fbcd-4494-9911-60ce0b3fafa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d76e0-3433-4966-b28f-96db307f9f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e45a0-40b5-4d29-a1a9-80b0ceb06bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb6349-450d-422f-8a2e-33cd2a4b9ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
