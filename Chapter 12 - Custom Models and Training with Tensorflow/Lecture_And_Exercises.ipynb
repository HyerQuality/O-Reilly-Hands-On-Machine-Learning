{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0dd750b2-bd12-401e-8f93-480021a8364e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8094e3-839a-4ec6-b7ce-4cfb4e2c6a95",
   "metadata": {},
   "source": [
    "# A Quick Tour of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061a1ee-39cf-4ecf-a629-809d3aaf7653",
   "metadata": {},
   "source": [
    "Here's a summary of what TensorFlow has to offer:\n",
    "\n",
    "1. Its core is very similar to NumPy, but with GPU support\n",
    "2. It supports distributed computing (across multiple devices and servers)\n",
    "3. It includes a kind of just-in-time (JIT) compiler that allows it to optimize computations for speed and memory usage. It works by extracting the *computation graph* from a Python function, then optimizing it (e.g. by pruning unused nodes), and finally running it efficiently (e.g. by automatically running independent operations in parallel.)\n",
    "4. Computation graphs can be exported to a portable format so you can train a TensorFlow model in on environment (e.g. using Python on Linux) and run it in another (e.g. using Java on an Android device)\n",
    "5. It implements autodiff (see Chapter 10 and Appendix D) and provides some excellent optimizers, such as RMSProp and Nadam (see Chapter 11), so you can easily minimize all sorts of loss functions.\n",
    "\n",
    "TensorFlow offers many more features built on top of these core features: the most important is of course tf.keras, but it also has data loading and preprocessing ops, image processing ops, signal processing ops, and more. \n",
    "\n",
    "As you may know, GPUs can dramatically speed up computations by splitting them into many smaller chunks and running them in parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips built specifically for Deep Learning operations.\n",
    "\n",
    "There's even more to the TensorFlow library:\n",
    "1. TensorBoard - for visualization\n",
    "2. TensorFlow Extended (TFX) - a set of libraries built by Google to productionize TensorFlow projects. It includes tools for data validation, preprocessing, model analysis, and serving.\n",
    "3. TensorFlow Hub - provides a way to easily download and reuse pretrained neural networks. You can also get many neural network architectures, some of them pretrained, in TensorFlows *model garden*\n",
    "4. TensorFlow Resources - contains TensorFlow-based projects. You will find hundreds of TensorFlow projects on GitHub, so it is often easy to find existing coded for whatever you are trying to do.\n",
    "\n",
    "More and more ML papers are released along with their implementations, and sometimes even with pretrained models. Check out https://paperswithcode.com/ to easily find them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e335b5-237e-4827-893c-3dd501ceadd4",
   "metadata": {},
   "source": [
    "## Using Tensorflow like NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34405f-9bf3-4bb4-8744-4f35002e284b",
   "metadata": {},
   "source": [
    "TensorFlow's API revolves around tensors. A tensor is usually a multidimensional array, b ut it can also hold a scaler. Let's see how to create and manipulate them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d11df0-0b3e-4f20-a7c5-04b3c321aeed",
   "metadata": {},
   "source": [
    "### Tensors and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc868b54-9590-4cbf-97b7-acd77adda999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=42>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6]]), tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "406d9184-1e5f-4dce-9238-5550579f076a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), tf.float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like an ndarray, a tf.Tensor has a shape and a data type\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6]])\n",
    "t.shape, t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe9c3bae-e3dd-4924-b2dc-bdbad945451e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing works much like in Numpy\n",
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "510cecb7-cfe4-4d4a-a1e1-2b841d15e68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de757633-60eb-4bd0-8150-e69bafc2bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[11., 12., 13.],\n",
       "        [14., 15., 16.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[14., 32.],\n",
       "        [32., 77.]], dtype=float32)>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More importantly, all sorts of tensor operations are available\n",
    "t + 10, tf.square(t), t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6115fa8-4080-4aae-84d9-0f69d5c8e872",
   "metadata": {},
   "source": [
    "You will find all the basic math operations you need (tf.add(), tf.multiply(), tf.square(), tf.exp(), tf.sqrt(), etc.) and most operations that you can find in Numpy (e.g. tf.reshape(), tf.squeeze(), tf.tile()). Some functions have a different name than Numpy; for instance, tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(), and tf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log().\n",
    "\n",
    "When the name differs, there is often a good reason for it. For example, in TensorFlow you must write tf.transpose(t); you cannot use write t.T like in NumPy. The reason is that the tf.transpose() function does not do exactly the same thing as Numpy's T attribute: in TensorFlow, a new tensor is created with its own copy of the transposed data, whlie in Numpy, t.T is just a transposed view of the same data. \n",
    "\n",
    "Similarly, the tf.reduce_sum() operation is named this way because its GPU kernel (i.e. GPU implementation) uses a reduce algorithm that does not guarantee the order in which the elements are added: because 32-bit floats have limited precision, the result may change ever so slightly every time you call this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f364299-3332-4359-9eb4-2ac570fbcf9b",
   "metadata": {},
   "source": [
    "### Tensors and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30a817cf-7af0-45a0-b384-d5ccc825f86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>,\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors play nice with Numpy\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a), t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "202ecb66-c823-4796-afb5-fc06cc69bc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>,\n",
       " array([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.]], dtype=float32))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a), np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eeac0-245a-47d6-8797-7852e24b12ca",
   "metadata": {},
   "source": [
    "Notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when you create a tensor from a NumPy array, make sure to set dtype=tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f2188-6283-4f2d-9569-1d0c0777aace",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c2c85-4dae-4578-98f8-7bd6e9c716f8",
   "metadata": {},
   "source": [
    "Type conversions can significantly hurt performance, and they can easily go unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. This may be a bit annoying at first, but remember that it's for a good cause! And of course you can use tf.cast() when you really need to convert types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12479a6a-4534-4d4d-a132-40bf0de1b79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Exception\n",
    "# tf.constant(2.) + tf.constant(40)\n",
    "\n",
    "' InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bf6a2e16-ff6c-4105-b80d-0d1cdb393f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example casting variables as the correct type\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b486de6-c2ab-48b8-bddc-84b3389f864c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f2296-d01a-4bf3-a854-811cfd16dc33",
   "metadata": {},
   "source": [
    "The tf.Tensor values we've seen so far are immutable: you cannot modify them. For mutable tf.Tensor values we need tf.Variable. A tf.Variable acts much like a tf.Tensor: you can perform the same operations with it, it plays nicely with NumPy as well, and it is just as picky with types. But it can also be modified in place using the assign() method (or assign_add() or assign_sub(), which increment or decrement the variable by the given value).\n",
    "\n",
    "In practice you will rarely have to create variables manually, since Keras provides an add_weight() method that will take care of it for you, as we will see. Moreover, model parameters will generally be updated directly by the optimizers, so you will rarely need to update variables manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bbdf1d29-890b-46c8-8673-e6a9f029797d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable examples\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e1218e7-a77c-4dc4-b876-4778b8f39b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d058338-e8bb-4545-a859-33844f7a7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1831af6e-ba10-4ecd-999d-ceb2bcb1e3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00a610e6-a9f7-4d56-b07f-e20a24774535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(\n",
    "    indices=[[0, 0], [1, 2]],\n",
    "    updates=[100., 200.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ca8c9-a1d1-45ed-b2d8-d585b31c5180",
   "metadata": {},
   "source": [
    "## Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074b7fc-29fb-4ad0-a51a-47d641cb7c80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3724d785-505f-40ce-af62-7663c438a2f0",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a3ae6-0fcf-45f7-8dde-9ba895c441b8",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cd3b4-2e28-42e5-b6ef-7a7cbd1f45ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ee870a-65d5-4ce4-95da-594d993113b4",
   "metadata": {},
   "source": [
    "## Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfc6dd-df75-43ef-8085-2e3fca01f267",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5833e170-acc5-4c35-886c-172da359e5e1",
   "metadata": {},
   "source": [
    "## Custom Activation Functions, Initializers, Regularizers, and Contstraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a6519-dbe4-4349-94b2-e51f29151cf6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a55655-04de-4e5c-a866-7676ced96ca1",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed1de0-e447-4e5e-9a6e-dd1ffeb0a518",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0508ca75-0bc4-4112-95f6-f9db59350845",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f0275-0541-4ba8-be1e-958432191dbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5c87f5-9c9c-4509-bdd9-47fdc86618a0",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b218b7-4b57-4699-b775-009ac92e83d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0856d2bd-ae47-4be9-bb04-1d027896e28a",
   "metadata": {},
   "source": [
    "# Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8622767-94ca-42f2-956e-aa393002ef09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f47eef3-2afa-4662-b717-bdc75706890f",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d7f5-de82-42ba-907c-8eb8ffd0dc20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4afb229b-0e37-473e-b59d-2312f26c6ba8",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe02832-4608-42fb-89f4-9073d9fc793b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb709f7b-93d4-478f-8fae-18bdd8c3d8df",
   "metadata": {},
   "source": [
    "# TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327b537-9b2d-4d31-b605-7f144c2609f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a169d43a-aa8b-4ecd-98eb-d597b3cd26fe",
   "metadata": {},
   "source": [
    "## AutoGraph and Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d18d2-6ee4-4a14-9173-c2e619ad822f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff2b696-5007-46d7-b3fb-1bc3e1e7592b",
   "metadata": {},
   "source": [
    "## TF Function Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a147c-751f-40a3-92cd-73fb0ab9da20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf4a771b-1fc6-4c88-940a-c0d674a548df",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e5d5b-aa28-450a-9d18-655582896691",
   "metadata": {},
   "source": [
    "1. **How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e49138-9667-45c5-8371-f82056a81e03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d491fb93-8e27-46c0-b03d-d5a3238eb808",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34490673-fefa-419f-b854-3315263e5e1a",
   "metadata": {},
   "source": [
    "2. **Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ce4d8-9f5d-481c-bc29-6cefb28cc9ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a22016a-6d7d-4915-a7ae-2ca934c002da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef6a86c9-8004-4fd7-a0a1-80167b38d1ad",
   "metadata": {},
   "source": [
    "3. **Do you get the same result with tf.range(10) and tf.constant(np.arange(10))?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78906a64-c3e5-418d-b01b-257fd44ab716",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "215f8113-ac06-4297-bdac-ea2ee947d1e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aae2392-877c-4354-982c-1b8323e18abb",
   "metadata": {},
   "source": [
    "4. **Can you name six other data structures available in TensorFlow, beyond regular tensors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea924e-71ae-4706-93ab-ed0fa790c54c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d7fa164-9c98-486c-aa67-e6f40e4a8fa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c29b2f-d248-4b2e-9a99-24f19ef63f41",
   "metadata": {},
   "source": [
    "5. **A custom loss function can b e defined by writing a function or by subclassing the keras.losses.Loss class. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2566cb-d6d8-41a0-8e32-d6b19fc47ed0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75472267-358a-488d-be68-b53444ce5ca8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c859aa-2c5b-47ce-bf19-7d7aca9534c9",
   "metadata": {},
   "source": [
    "6. **Similarly, a custom metric can be defined in a function or a subclass of keras.metrics.Metric. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a754c-62f6-4267-8d10-61cab0b82ada",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b942dfa-05c4-418f-9585-2faf1bc2e4e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc1057b-ee1f-47df-a11e-6f3f7b320188",
   "metadata": {},
   "source": [
    "7. **When should you create a custom layer versus a custom model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06639a46-48c7-466d-990d-7b8537bcf9ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f01d7645-079e-4827-8018-02df2ca52a03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0a2d882-df35-4922-b521-7d9bd523dc72",
   "metadata": {},
   "source": [
    "8. **What are some use cases that require writing your own custom training loop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15426460-ea3b-4d2d-b9f0-18209ba5d91c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ffda58c-a65b-4a45-83eb-82a6b04a8bd2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822c5080-e975-46fc-ae53-82bfe89b349c",
   "metadata": {},
   "source": [
    "9. **Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c444c-39de-47ae-92f0-6a0b5c635c53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebacc5dc-bb56-4e96-a3a5-1ec27d67cc80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09841192-a3cf-44af-ad5a-738c4f4bf49c",
   "metadata": {},
   "source": [
    "10. **What are the main rules to respect if you want a function to be convertible to a TF Function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf456c-2590-4137-afe8-36da885b7715",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3511c4c-76b6-4191-9873-23867f3c21ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2b93346-38f7-4e70-aafb-bb9acd75a85b",
   "metadata": {},
   "source": [
    "11. **When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f1ab2-bc17-48f5-99c5-4a72dcd2d87b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "354ca20f-efd6-4966-9a08-c296cc2d3fc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f1358e9-3307-4eeb-9776-18ed91f282f4",
   "metadata": {},
   "source": [
    "12. **Implement a custom layer that performs *Layer Normalization* (we will use this type of layer in Chapter 15):**\n",
    "\n",
    "    a. The build() method should define two trainable weights $\\alpha$ and $\\beta$, both of shape input_shape[-1:] and data type tf.float32. $\\alpha$ should be initialized with 1s and $\\beta$ with 0s\n",
    "\n",
    "    b. The call() method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance's features. For this, you can use tf.nn.moments(inputs, axes=-1, keepdims=True), which returns the mean $\\mu$ and the variance $\\sigma^{2}$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should computer and return $\\alpha\\bigotimes\\frac{(X - \\mu)}{\\sigma + \\epsilon} + \\beta$, where \\bigotimes represents itemwise multiplication and \\epsilon is a smoothing term (small constant to avoid division by zero, e.g. 0.001)\n",
    "\n",
    "    c. Ensure that your custom layer produces the same (or very nearly the same) outoput as the keras.layers.LayerNormalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfc97f-7098-4d61-9a4c-336c1daea037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e483fef-5995-4d26-a6b8-838cd0a28f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19575386-b124-4952-a369-9f96ebc494f9",
   "metadata": {},
   "source": [
    "13. **Train a model using a custom training loop to tackle the Fashion MNIST dataset (see Chapter 10).**\n",
    "\n",
    "    a. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\n",
    "    \n",
    "    b. Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fb924-5af8-4ab6-a862-b7d79a746011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb16dd8-3429-49be-9f1e-8a3e1b664fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
