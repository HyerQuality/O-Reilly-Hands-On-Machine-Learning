{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd750b2-bd12-401e-8f93-480021a8364e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8094e3-839a-4ec6-b7ce-4cfb4e2c6a95",
   "metadata": {},
   "source": [
    "# A Quick Tour of Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061a1ee-39cf-4ecf-a629-809d3aaf7653",
   "metadata": {},
   "source": [
    "Here's a summary of what TensorFlow has to offer:\n",
    "\n",
    "1. Its core is very similar to NumPy, but with GPU support\n",
    "2. It supports distributed computing (across multiple devices and servers)\n",
    "3. It includes a kind of just-in-time (JIT) compiler that allows it to optimize computations for speed and memory usage. It works by extracting the *computation graph* from a Python function, then optimizing it (e.g. by pruning unused nodes), and finally running it efficiently (e.g. by automatically running independent operations in parallel.)\n",
    "4. Computation graphs can be exported to a portable format so you can train a TensorFlow model in one environment (e.g. using Python on Linux) and run it in another (e.g. using Java on an Android device)\n",
    "5. It implements autodiff (see Chapter 10 and Appendix D) and provides some excellent optimizers, such as RMSProp and Nadam (see Chapter 11), so you can easily minimize all sorts of loss functions.\n",
    "\n",
    "TensorFlow offers many more features built on top of these core features: the most important is of course tf.keras, but it also has data loading and preprocessing ops, image processing ops, signal processing ops, and more. \n",
    "\n",
    "As you may know, GPUs can dramatically speed up computations by splitting them into many smaller chunks and running them in parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips built specifically for Deep Learning operations.\n",
    "\n",
    "There's even more to the TensorFlow library:\n",
    "1. TensorBoard - for visualization\n",
    "2. TensorFlow Extended (TFX) - a set of libraries built by Google to productionize TensorFlow projects. It includes tools for data validation, preprocessing, model analysis, and serving.\n",
    "3. TensorFlow Hub - provides a way to easily download and reuse pretrained neural networks. You can also get many neural network architectures, some of them pretrained, in TensorFlows *model garden*\n",
    "4. TensorFlow Resources - contains TensorFlow-based projects. You will find hundreds of TensorFlow projects on GitHub, so it is often easy to find existing coded for whatever you are trying to do.\n",
    "\n",
    "More and more ML papers are released along with their implementations, and sometimes even with pretrained models. Check out https://paperswithcode.com/ to easily find them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e335b5-237e-4827-893c-3dd501ceadd4",
   "metadata": {},
   "source": [
    "## Using Tensorflow like NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c34405f-9bf3-4bb4-8744-4f35002e284b",
   "metadata": {},
   "source": [
    "TensorFlow's API revolves around tensors. A tensor is usually a multidimensional array, but it can also hold a scaler. Let's see how to create and manipulate them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d11df0-0b3e-4f20-a7c5-04b3c321aeed",
   "metadata": {},
   "source": [
    "### Tensors and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc868b54-9590-4cbf-97b7-acd77adda999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=42>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6]]), tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "406d9184-1e5f-4dce-9238-5550579f076a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 3]), tf.float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just like an ndarray, a tf.Tensor has a shape and a data type\n",
    "t = tf.constant([[1., 2., 3.], [4., 5., 6]])\n",
    "t.shape, t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe9c3bae-e3dd-4924-b2dc-bdbad945451e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing works much like in Numpy\n",
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510cecb7-cfe4-4d4a-a1e1-2b841d15e68b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de757633-60eb-4bd0-8150-e69bafc2bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[11., 12., 13.],\n",
       "        [14., 15., 16.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[14., 32.],\n",
       "        [32., 77.]], dtype=float32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More importantly, all sorts of tensor operations are available\n",
    "t + 10, tf.square(t), t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6115fa8-4080-4aae-84d9-0f69d5c8e872",
   "metadata": {},
   "source": [
    "You will find all the basic math operations you need (tf.add(), tf.multiply(), tf.square(), tf.exp(), tf.sqrt(), etc.) and most operations that you can find in Numpy (e.g. tf.reshape(), tf.squeeze(), tf.tile()). Some functions have a different name than Numpy; for instance, tf.reduce_mean(), tf.reduce_sum(), tf.reduce_max(), and tf.math.log() are the equivalent of np.mean(), np.sum(), np.max() and np.log().\n",
    "\n",
    "When the name differs, there is often a good reason for it. For example, in TensorFlow you must write tf.transpose(t); you cannot use write t.T like in NumPy. The reason is that the tf.transpose() function does not do exactly the same thing as Numpy's T attribute: in TensorFlow, a new tensor is created with its own copy of the transposed data, whlie in Numpy, t.T is just a transposed view of the same data. \n",
    "\n",
    "Similarly, the tf.reduce_sum() operation is named this way because its GPU kernel (i.e. GPU implementation) uses a reduce algorithm that does not guarantee the order in which the elements are added: because 32-bit floats have limited precision, the result may change ever so slightly every time you call this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f364299-3332-4359-9eb4-2ac570fbcf9b",
   "metadata": {},
   "source": [
    "### Tensors and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30a817cf-7af0-45a0-b384-d5ccc825f86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>,\n",
       " array([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors play nice with Numpy\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a), t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "202ecb66-c823-4796-afb5-fc06cc69bc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>,\n",
       " array([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.]], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a), np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739eeac0-245a-47d6-8797-7852e24b12ca",
   "metadata": {},
   "source": [
    "Notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when you create a tensor from a NumPy array, make sure to set dtype=tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f2188-6283-4f2d-9569-1d0c0777aace",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c2c85-4dae-4578-98f8-7bd6e9c716f8",
   "metadata": {},
   "source": [
    "Type conversions can significantly hurt performance, and they can easily go unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. This may be a bit annoying at first, but remember that it's for a good cause! And of course you can use tf.cast() when you really need to convert types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12479a6a-4534-4d4d-a132-40bf0de1b79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Exception\n",
    "# tf.constant(2.) + tf.constant(40)\n",
    "\n",
    "' InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf6a2e16-ff6c-4105-b80d-0d1cdb393f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example casting variables as the correct type\n",
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b486de6-c2ab-48b8-bddc-84b3389f864c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f2296-d01a-4bf3-a854-811cfd16dc33",
   "metadata": {},
   "source": [
    "The tf.Tensor values we've seen so far are immutable: you cannot modify them. For mutable tf.Tensor values we need tf.Variable. A tf.Variable acts much like a tf.Tensor: you can perform the same operations with it, it plays nicely with NumPy as well, and it is just as picky with types. But it can also be modified in place using the assign() method (or assign_add() or assign_sub(), which increment or decrement the variable by the given value).\n",
    "\n",
    "In practice you will rarely have to create variables manually, since Keras provides an add_weight() method that will take care of it for you, as we will see. Moreover, model parameters will generally be updated directly by the optimizers, so you will rarely need to update variables manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbdf1d29-890b-46c8-8673-e6a9f029797d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable examples\n",
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e1218e7-a77c-4dc4-b876-4778b8f39b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d058338-e8bb-4545-a859-33844f7a7e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1831af6e-ba10-4ecd-999d-ceb2bcb1e3db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00a610e6-a9f7-4d56-b07f-e20a24774535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(\n",
    "    indices=[[0, 0], [1, 2]],\n",
    "    updates=[100., 200.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ca8c9-a1d1-45ed-b2d8-d585b31c5180",
   "metadata": {},
   "source": [
    "## Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074b7fc-29fb-4ad0-a51a-47d641cb7c80",
   "metadata": {},
   "source": [
    "TensorFlow supports several other data structures, including the following:\n",
    "\n",
    "1. Sparse tensors (tf.SparseTensor)\n",
    "> Efficiently represent tensors containing mostly zeros. The tf.sparse package contains operations for sparse tensors.\n",
    "\n",
    "2. Tensor arrays (tf.TensorArray)\n",
    "> Are lists of tensors. They have a fixed size by default but can optionally be made dynamic. All tensors they contain must have the same shape and data type.\n",
    "\n",
    "3. Ragged tensors (tf.RaggedTensor)\n",
    "> Represent static lists of lists of tensors, where every tensor has the same shape and data type. The tf.ragged package contains operations for ragged tensors.\n",
    "\n",
    "4. String tensors\n",
    "> Are regular tensors of type tf.string. These represent byte strings, not Unicode strings, so if you create a string tensor using a Unicode string (e.g., a regular Pythong 3 string like \"coffee\"), then it will get encoded to UTF-8 automatically. Alternatively, you can represent Unicode strings using tensors of type tf.int32, where each item represents a Unicode code point (e.g., [99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte strings and Unicode strings (and to convert one into the other). It's important to note that a tf.string is atomic, meaning that its length does not appear in the tensor's shape. Once you convert it to a Unicode tensor (i.e. a tensor of type tf.int32 holding Unicode code points), the length appears in the shape.\n",
    "\n",
    "5. Sets\n",
    "> Are represented as regular tensors (or sparse tensors). For example, tf.constant([[1, 2], [3, 4]]) represents the two sets {1, 2} and {3, 4}. More generally, each set is represented by a vector in the tensor's last axis. You can manipulate sets using operations from the tf.sets package.\n",
    "\n",
    "6. Queues\n",
    "> Store tensors across multiple steps. TensorFlow offers various kinds of queues, these classes are all in the tf.queue package:\n",
    ">    1. Simple First In, First Out (FIFO) queues (FIFOQueue)\n",
    "    2. Queues that can prioritize some items (PriorityQueue)\n",
    "    3. Queues that shuffle their items (RandomShuffleQueue)\n",
    "    4. Queus that batch items of different shapes by padding (PaddingFIFOQueue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724d785-505f-40ce-af62-7663c438a2f0",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a3ae6-0fcf-45f7-8dde-9ba895c441b8",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cd3b4-2e28-42e5-b6ef-7a7cbd1f45ed",
   "metadata": {},
   "source": [
    "Suppose you want to train a regression model, but your training set is a bit noisy. Of course, you start by trying to clean up your dataset by removing or fixing the outliers, but that turns out to be insufficient; the dataset is still noisy. Which loss function should you use? The mean squared error might penalize large errors too much and cause your model to be imprecise. The mean absolute error would not penalize outliers as much, but training might take a while to converge, and the trained model might not be very precise. This is probably a good time to use the Huber loss (introduced in Chapter 10) instead of the good old MSE. The Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the keras.losses.Huber class). But let's pretend it's not there: implementing it is easy as pie! Just create a function that takes the labels and predictions as arguments, and use TensorFlow operations to compute every instance's loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b77a284c-0136-4792-bd5f-6049624c2b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' For best performance you should always vectorize implementations, as in this example. Moreover, if you want to benefit from TensorFlows graph features, you should use only TensorFlow operations'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "' For best performance you should always vectorize implementations, as in this example. Moreover, if you want to benefit from TensorFlows graph features, you should use only TensorFlow operations'\n",
    "\n",
    "# To use with a model\n",
    "# model.compile(loss=huber_fn, optimizer='nadam')\n",
    "# model.fit(X_train, y_train, [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421320a9-f86b-4df8-a078-86dd1550015d",
   "metadata": {},
   "source": [
    "It is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss. This way, Keras can apply class weights or sample weights when requested (Chapter 10). Now you can use this loss when you compile the Keras model, then train your model, and that's it! But what happens to this custom loss when you save the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee870a-65d5-4ce4-95da-594d993113b4",
   "metadata": {},
   "source": [
    "## Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfc6dd-df75-43ef-8085-2e3fca01f267",
   "metadata": {},
   "source": [
    "Saving a model containing a custom loss function works fine, as Keras saves the name of the function. When you load a model containing custom objects, you need to map the names to the objects, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7e93857-dace-4918-b312-8d8e15976c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('model_with_custom_loss.h5', custom_objects={'huber_fn': huber_fn})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fcbede-15e3-4dba-b3a1-a3ad0af022ac",
   "metadata": {},
   "source": [
    "With the current implementation, any error between -1 and 1 is considered \"small\". But what if you want a different threshold? You can solve this by creating a subclass of the keras.losses.Loss class, and then implementing its get_config() method, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c7d0aac-b586-421c-920f-0a21cedd921c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9559f-8ca9-4677-8c0d-a185775f01e5",
   "metadata": {},
   "source": [
    "The Keras API currently only specifies how to use subclassing to define layers, models, callbacks, and regularizers. If you build other components (such as losses, metrics, initializers, or constraints) using subclassing, they may not be portable to other Keras implementations. It's likely that the Keras API will be updated to specify subclassing for all these components as well.\n",
    "\n",
    "That said, let's walk through the code above:\n",
    "\n",
    "1. The constructor accepts **kwargs** and passes them to the parent constructor, which handles standard hyperparameters: the name of the loss and the reduction algorithm to use to aggregate the individual instance losses. By default, it is 'sum_over_batch_size', which means that the loss will be the sum of the instance losses, weighted by the sample weights, if any, and divided by the batch size (not by the sum of weights, so this is *not* the weighted mean). It would not be a good idea to use a weighted mean: if you did, then two instances with the same weight but in different batches would have a different impact on training, depending on the total weight of each batch. Other possible values are 'sum' and 'None'.\n",
    "\n",
    "2. The call() method takes the labels and predictions, computes all the instance losses, and returns them.\n",
    "\n",
    "3. The get_config() method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent classes get_config() method, then adds the new hyperparameters to this dictionary.\n",
    "\n",
    "You can then use any instance of this class when you compile the model. When you save the model, the threshold will be saved along with it; and when you load the model, you just need to map the class name to the class itself, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6efb2f7-fde2-4614-adf0-7216efaaf60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.compile(loss=HuberLoss(threshold=2.), optimizer='nadam')\n",
    "# model = tf.keras.models.load_model('my_model_with_custom_loss_class.h5', custom_objects={'HuberLoss': HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833e170-acc5-4c35-886c-172da359e5e1",
   "metadata": {},
   "source": [
    "## Custom Activation Functions, Initializers, Regularizers, and Contstraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a6519-dbe4-4349-94b2-e51f29151cf6",
   "metadata": {},
   "source": [
    "Most Keras functionalities, such as losses, regularizers, constraints, initializers, metrics, activation functions, layers, and even full models, can be customized in very much the same way. Most of the time, you will just need to write a simple function with the appropriate inputs and outputs. Some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "299f49c7-1df0-45ea-9406-d2a2c94d1fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    ' Return value is just tf.nn.softplus(z)'\n",
    "    return tf.math.log(tf.exp(z) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dc55477-d87e-492f-a3f7-d822ce683fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cda65e96-f15f-4871-b80b-592aed7c9209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e5a2e54-5f26-43f3-9aad-51fcbcdecea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_positive_weights(weights):\n",
    "    ' Return value is just tf.nn.relu(weights)'\n",
    "    return tf.where(weights < 0., tf.zeroes_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea090e9d-cf51-4c9b-8f16-cb2e5eb83b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(\n",
    "    units=30,\n",
    "    activation=my_softplus,\n",
    "    kernel_initializer=my_glorot_initializer,\n",
    "    kernel_regularizer=my_l1_regularizer,\n",
    "    kernel_constraint=my_positive_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61b1ce-ea67-4565-a84f-0fafe7795ec4",
   "metadata": {},
   "source": [
    "The activiation function will be applied to the output of this Dense layer, and its result will be passed on to the next layer. The layer's weights will be initialized using the value returned by the initializer. At each training step the weights will be passed to the regularization function to compute the regularization loss, which will be added to the main loss to get the final loss used for training. Finally, the constraint function will be called after each training step, and the layer's weights will be replaced by the constained weights.\n",
    "\n",
    "If a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class. Note that you must implement the call() method for losses, layers (including activation functions), and models, or the __ call__() method for regularizers, initializers, and constraints. For metrics, things are a bit different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1bcb1187-0eaa-4274-b4dd-8ac56edfd18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'factor': self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a55655-04de-4e5c-a866-7676ced96ca1",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed1de0-e447-4e5e-9a6e-dd1ffeb0a518",
   "metadata": {},
   "source": [
    "Losses and metrics are conceptually not the same thing: losses are used by Gradient Descent to _train_ a model, so they must be differentiable and their gradients should not be 0 everywhere. In contrast, metrics are used to _evaluate_ a model: they must be more easily interpretable, and they can be non-differentiable or have 0 gradients everywhere. \n",
    "\n",
    "__That said, in most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric; it would work just fine.__\n",
    "\n",
    "For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. Most of the time, this is exactly what you want. But not always! Consider a binary classifier's precision, for example. In this case, what we need is an object that can keep track of the number of true positives and the number of false positives and that can compute their ratio when requested. This is precisely what the tf.keras.metrics.Precision class does. This is called a _streaming metric_ (or _stateful metric_), as iti s gradually updated, batch after batch.\n",
    "\n",
    "If you need to create such a streaming metric, create a subclass of the tf.keras.metrics.Metric class. Here is simple example that keeps track of the total Huber loss and the number of instances seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13af4ef0-a74b-4b1a-9185-d9b06f3a4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ca2939fc-634d-4513-991f-a7f9e86103f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example using a Loss function as a metric\n",
    "# model.compile(loss='mse', optimizer='nadam', metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abedd58c-f63e-45b7-a4d0-2fe4cd767749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g. dtype)\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weights=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'threshold': self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72689f84-4766-4145-acd4-665ea5063199",
   "metadata": {},
   "source": [
    "Let's walk through the class above:\n",
    "\n",
    "1. The constructor uses the add_weight() method to create the variables needed to keep track of the metric's state over multiple batches - in this case, the sum of all Huber losses (total) and the number of instances seen so far (count). You could just create variables manually if you preferred. Keras tracks any tf.Variable that is set as an attribute (and more generally, any 'trackable' object, such as layers or models).\n",
    "\n",
    "2. The update_state() method is called when you use an instance of this class as a function. It updates the variables, given the labels and predictions for one batch (and sample weights, but in this case we ignore them).\n",
    "\n",
    "3. The result() method computes and returns the final result, in this case the mean Huber metric over all instances. When you use the metric as a function, the update_state() method gets called first, then the result() method is called, and its output is returned.\n",
    "\n",
    "4. We also implement the get_config() method to ensure the threshold gets saved along with the model.\n",
    "\n",
    "5. The default implementation of the reset_states() method resets all variables to 0.0 (but you can override this if needed).\n",
    "\n",
    "Keras will take care of variable persistence seamlessly; no action is required. Now that we have built a streaming metric, building a custom layer will seem like a walk in the park!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508ca75-0bc4-4112-95f6-f9db59350845",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f0275-0541-4ba8-be1e-958432191dbe",
   "metadata": {},
   "source": [
    "You may occasionally want to build an architecture that contains an exotic layer for which TensorFlow does not provide a default implementation. In this case, you will need to create a custom layer. Or you may simply want to build a very repretivie architecture to treat each block of layers as a single layer. __For example, if the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a custom layer D containing layers A, B, C such that the new sequence is D, D, D.__\n",
    "\n",
    "First off, some layers have no weights. If you want to create a custom layer without any weights, the simplest option is to write a function and wrap it in a keras.layers.Lambda layer, as shown below. As you've probably guessed by now, to build a custom stateful layer (i.e. a layer with weights), you need to create a subclass of the keras.layers.Layer class. An example of a simplified version of the Dense layer is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a873f58e-d532-4468-94f0-a63a35ae7eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "833000ff-d4b3-4746-b653-e823912f64f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activiation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=[batch_input_shape[-1], self.units],\n",
    "            initializer='glorot_normal' \n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=[self.units],\n",
    "            initializer='zeros'\n",
    "        )\n",
    "        super().build(batch_input_shape) # must be at the end of this method\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'units': self.units, 'activation': tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca9e6b-ebed-45f6-a9f6-5db7ac869b2e",
   "metadata": {},
   "source": [
    "Let's walk through the code above:\n",
    "\n",
    "1. The constructor takes all the hyperparameters as arguments (in this example, units and activation), and importantly it also takes the ** kwargs argument. It calls the parent constructor, passing it the kwargs: this takes care of standard arguments such as input_shape, trainable, and name. Then it saves the hyperparameters as attributes, converting the activation argument to the appropriate activation function using the tf.keras.activations.get() function (it accepts functions, standard strings like 'relu' or 'selu', or simply None).\n",
    "\n",
    "2. The build() method's role is to create the layer's variables by calling the add_weight() method for each weight. The build() method is called the first time a layer is used. At that point, Keras will know the shape of this layer's inputs, and it will pass it to the build() method, which is often necessary to create some the weights. For example, we need to know the number of neurons in the previous layer in order to create a connection weights matrix (i.e. the kernel): this corresponds to the size of the last dimension of the inputs. At the end of this build() method (and only at the end), you must call the parent's build() method: this tells Keras that hte layer is built (it just sets self.built = True)\n",
    "\n",
    "3. The call() method performs the desired operations. In this case, we compute the matrix multiplication of the inputs X and the layer's kernel, we add the bias vector, and we apply the activation function to the result, and this gives us the output of hte layer.\n",
    "\n",
    "4. The compute_output_shape() method simply returns the shape of this layer's outputs. It this case, it is the same shape as the inputs, except the last dimension is replaced with the number of neurons in the layer. Note that in tf.keras, shapes are instances o the tf.TensorShape class, which you can convert to Python lists using as_list().\n",
    "\n",
    "5. The get_config() method is just like in the previous custom classes. Note that we save the activation function's full configuration by calling tf.keras.activations.serialize()\n",
    "\n",
    "You can now use a MyDense layer like any other layer! You can generally omit the compute_output_shape() method as tf.keras automatically infers the output shape, except when the layer is dynamic (as we will see shortly). In other Keras implementations, this method is either required or its default implementation assumes the output shape is the same as the input shape.\n",
    "\n",
    "To create a layer with multiple inputs (e.g. Concatenate), the argument to the call() method should be a tuple containing all the inputs, and similarly the argument to the compute_output_shape() method should be a tuple containing each input's batch shape. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bdc97f71-79f6-45ad-9e0c-39ebf7f839b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        if X == 0:\n",
    "            X = 1e-9\n",
    "            \n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = [batch_input_shape]\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c77a22-94ea-41e6-bd3b-9afd75f0aa91",
   "metadata": {},
   "source": [
    "If your layer needs to have a different behavior during training and during testing (e.g. if it uses a Dropout or BatchNormalization layer(s)), then you must add a training argument to the call() method and use this argument to decide what to do. For example, let's create a layer that adds Gaussian noise during training but does nothing during testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a994f54-69c2-47b8-9647-a69252a67747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec0e6eba-9032-46a3-9942-9deccd477318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "' Residual block layer example'\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(\n",
    "                units=n_neurons,\n",
    "                activation='elu',\n",
    "                kernel_initializer='he_normal'\n",
    "            ) \n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c87f5-9c9c-4509-bdd9-47fdc86618a0",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b218b7-4b57-4699-b775-009ac92e83d8",
   "metadata": {},
   "source": [
    "We already looked at creating custom model classes in Chapter 10. It's straightforward: subclass the tf.keras.Model class, create layers and variables in the constructor, and implement the call() method to do whatever you want the model to do. \n",
    "\n",
    "If models provide more functionality than layers, why not just define every layer as a model? Well, technically you could, but it is usually cleaner to distinguish the internal components of your model (i.e. layers or reusable blocks of layers) from the model itself (i.e. the object you will train). The former should subclass the Layer class, while the latter should subclass the Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6bf5eb3-a573-425d-b053-e3bd2ee09f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(\n",
    "            units=30,\n",
    "            activiation='elu',\n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856d2bd-ae47-4be9-bb04-1d027896e28a",
   "metadata": {},
   "source": [
    "# Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8622767-94ca-42f2-956e-aa393002ef09",
   "metadata": {},
   "source": [
    "There will be times when you want to define losses based on other parts of your model, such as the weights or activations of its hidden layers. __This may be useful for regularization purposes or to monitor some internal aspect of your model.__ To define a custom loss based on model internals, compute it based on any part of the model you want, then pass the result to the add_loss() method. \n",
    "\n",
    "The example below will have an auxiliary output on top of the upper hidden layer. The loss associated to this auxiliary output will be called the _reconstruction loss_: it is the mean squared difference between the reconstruction and the inputs. __By adding this reconstruction loss to the main loss, we will encourage the model to preserve as much information as possible through the hidden layers-- even information that is not directly useful for the regression task itself.__ In practice, this loss sometimes improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7a2a054-ed46-4a69-af62-138591d34d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReconstructionRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [\n",
    "            tf.keras.layers.Dense(\n",
    "                units=30,\n",
    "                activation='selu',\n",
    "                kernel_initializer='lecun_normal'\n",
    "            )\n",
    "            for _ in range(5)\n",
    "        ]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1df8cd-090b-4af8-ae1c-08330419589c",
   "metadata": {},
   "source": [
    "Let go through the code above:\n",
    "    \n",
    "1. The constructor creates the DNN with five dense hidden layers and one dense output layer.\n",
    "2. The build() method creates an extra dense layer which will be used to reconstruct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, and this number is unknown before the build() method is called.\n",
    "3. The call() method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which prodces the reconstruction. Then the call() method computes the reconstruction loss (the mean squared difference between the reconstruction and inputs), and adds it to the model's list of losses using the add_loss() method. Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can tune). This ensures that the reconstruction loss does not dominate the main loss. Finally, the call() method passes the output of the hidden layers to the output layer and returns its output\n",
    "\n",
    "__In over 99% of cases, everything we have discussed so far will be sufficient to implement whatever model you want to build, even with complex architectures, losses, and metrics.__ However, in some rare cases you may need to customing the training loop itself. Before we get there, we need to look at how to compute gradients automatically in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47eef3-2afa-4662-b717-bdc75706890f",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7d7f5-de82-42ba-907c-8eb8ffd0dc20",
   "metadata": {},
   "source": [
    "To understand hwo to use autodiff to compute gradients automatically, lets consider a simple toy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c55a462f-583b-4332-ac69-b69b9bea78eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.000003007075065, 10.000000003174137)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps, (f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bcd70714-97a4-4691-a9ff-b2811a8ad486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92888d1a-91dd-49b0-8979-d624e2befa9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The tape is automatically erased immediately after you call its gradient() method, so you will get an exception if you try to call gradient() twice'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' The tape is automatically erased immediately after you call its gradient() method, so you will get an exception if you try to call gradient() twice'\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     z = f(w1, w2)\n",
    "    \n",
    "# dz_dw1 = tape.gradient(z, w1)\n",
    "# dz_dw2 = tape.gradient(z, w2)\n",
    "\n",
    "# RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d1415-e1ca-4b23-9bca-4cd12c24dd02",
   "metadata": {},
   "source": [
    "To save memory, only put the strict minimum inside the tf.GradientTape() block. Alternatively, pause recording by creating a tape.stop_recording() block inside the tf.GradientTape() block.\n",
    "\n",
    "If you need to call gradient() more than once, you must make the tape persistent and delete it each time you are done with it to free resources. By default, the tape will only track operations involving variables. However, you can force the tape to watch any tensors you like, to record every operation that involves. You can then compute gradients with regard to these tensors, as if they were variables. __This can be useful in some cases, like if you want to implement a regularization loss that penalizes activations that vary a lot when the inputs vary little: the loss will be based on the gradient of the activations with regard to the inputs.__ Since the inputs are not variables, you would need to tell the tape to watch them.\n",
    "\n",
    "In some cases you may want to stop gradients from backpropagating through some part of you neural network. To do this, you must the tf.stop_gradient() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fabea0c5-7454-41ec-96d9-c6eb99503966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Example keeping GradientTape persistent\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "print(dz_dw1, dz_dw2)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec68b77c-1fa4-40cb-b366-967b2aef82a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example using GradientTape to calculate the gradients of constants\n",
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fcb3df5-ecb3-489f-a826-5e7b1cb43479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example using GradientTape to calculate the gradients of any operations that use the constants from the previous examlpe\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "    \n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de66cbe8-3ef8-43b1-97f1-f83d3e466054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example using stop_gradient()\n",
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1229d74e-78dd-4679-b0cd-a5a620c0e6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example writing a custom gradient function\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cd7f5-b9e3-430d-9542-7356a986f409",
   "metadata": {},
   "source": [
    "Congratualtions, you can now compute the gradients of any function (provided it is differentiable at the point where you compute it), even blocking backpropagation when needed. This is probably more flexibility than you will ever need, even if you build your own custom training loops, as we will see next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb229b-0e37-473e-b59d-2312f26c6ba8",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe02832-4608-42fb-89f4-9073d9fc793b",
   "metadata": {},
   "source": [
    "In some rate cases, the fit() method may not be flexible enough for what you need to do. __For example, the Wide & Deep paper we discussed in Chapter 10 uses two different optimizers__. Implementing this paper requires writing your own custom loop.\n",
    "\n",
    "However, remember that writing a custom training loop will make your code longer, more error-prone, and harder to maintain. Unless you really need the extra flexibility, you should prefer using the fit() method rather than implementing your own training loop, especially if you work in a team. An example of a custom training loop is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1103b6f-a195-4c27-8be6-ab3ca945cdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "96/100 - mean: 5.163573 - mean_absolute_error: 1.139317Epoch 2/5\n",
      "96/100 - mean: 4.336100 - mean_absolute_error: 0.805593Epoch 3/5\n",
      "96/100 - mean: 3.884027 - mean_absolute_error: 0.559777Epoch 4/5\n",
      "96/100 - mean: 3.860552 - mean_absolute_error: 0.582307Epoch 5/5\n",
      "96/100 - mean: 3.692938 - mean_absolute_error: 0.579613"
     ]
    }
   ],
   "source": [
    "# Build a simple model with l2 regularization\n",
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=30, activation='elu', kernel_initializer='he_normal', kernel_regularizer=l2_reg),\n",
    "    tf.keras.layers.Dense(units=1, kernel_regularizer=l2_reg)\n",
    "])\n",
    "\n",
    "# Create a function that will randomly sample a batch of instances from the training set\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# Define a function that will display the training status\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = ' - '.join(['{}: {:4f}'.format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = '' if iteration < total else '\\n'\n",
    "    print('\\r{}/{} - '.format(iteration, total) + metrics, end=end)\n",
    "    \n",
    "# Declare hyperparameters\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# Build the custom training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print('Epoch {}/{}'.format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            \n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a1ca4-973c-4c3d-80e1-3d7578ea1cc8",
   "metadata": {},
   "source": [
    "There's a lot going on in this code, so let's walk through it:\n",
    "\n",
    "1. We create two nested loops: one for the epochs, the other for hte batches within an epoch\n",
    "2. Then we sample the random batch from the training set.\n",
    "3. Inside the tf.GradientTape() block, we make a prediction for one batch (using the model as a function), and we compute the loss: it is equal to the main loss plust the other losses (in this model, there is one regularization loss per layer). Since the mean_squared_error() function returns one loss per instance, we compute the mean over the batch using tf.reduce_mean() (if you wanted to apply different weights to each instance, this is where you would do it). The regulariztaion losses are already reduced to a single scaler each, so we just need to sum them (using tf.add_n(), which sums multile tensors of the same shape and data type).\n",
    "4. Next, we ask the tape to compute the gradient of the loss with regard to each trainable variable (_not_ all variables!), and we apply them to the optimizer to perform a Gradient Descent step.\n",
    "5. Then we update the mean loss and metrics (over the current epoch), and we display the status bar.\n",
    "6. At the end of each epoch, we display the status bar again to make it look complete and to print a line feed, and we reset the states of the mean loss and the metrics.\n",
    "\n",
    "As yo ucan see, there are quite a lot of things you need to get right, and it's easy to make a mistake. But on the bright side, you get full control, so it's your call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb709f7b-93d4-478f-8fae-18bdd8c3d8df",
   "metadata": {},
   "source": [
    "# TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327b537-9b2d-4d31-b605-7f144c2609f8",
   "metadata": {},
   "source": [
    "TensorFlow graphs are easy to use. Let's start with a trivial function that computes the cube of its input and go from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "86f125c7-81bd-4b57-a8ff-6180cd87a3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3\n",
    "\n",
    "# Next use tf.function() to convert this Python function into a TensorFlow function\n",
    "tf_cube = tf.function(cube)\n",
    "\n",
    "# Alternatively\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efa4be-9f51-4af5-b1af-e2d4fca4af32",
   "metadata": {},
   "source": [
    "Under the hood, tf.function() analyzed the computations performed by the cube() function and generated an equivalent computation graph. As a result, a TF Function will usually run much fatser than the original Python function, especially if it performs complex computations. By default, a TF Function generates a new graph for every unique set of input shapes and data types and caches it for subsequent calls.\n",
    "\n",
    "If you call a TF Function many times with different numerical Python values, then many graphs will be generated, slowing down your program and using a lot of RAM (you must delete the TF Function to release it). Python values should be reserved for arguments that will ahve few unique values, such as hyperparameters like the number of neurons per layer. This allows TensorFlow to better optimize each variant of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169d43a-aa8b-4ecd-98eb-d597b3cd26fe",
   "metadata": {},
   "source": [
    "## AutoGraph and Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d18d2-6ee4-4a14-9173-c2e619ad822f",
   "metadata": {},
   "source": [
    "So how does TensorFlow generate graphs? It starts by analyzing the Python function's source code to capture all the control flow statements, such as for loops, while loops, and if statements, as well as break, continue and return statements. This first step is called _AutoGraph_.\n",
    "\n",
    "After analyzing the function's code, AutoGraph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow operations, such as tf.while_loop() for loops and tf.cond() for if statements.\n",
    "\n",
    "Next, TensorFlow calls this 'upgraded' function, but instead of passing the argument, it passes a _symbolic tensor_ - a tensor without any actual value, only a name, a data type and a shape. The function will run in _graph mode_, meaning that each TensorFlow operation will add a node in the graph to represent itself and its output tensor(s) (as opposed to the regular mode, called _eager execution_, or _eager mode_).\n",
    "\n",
    "To view the generated function's source code, you can call tf.autograph.to_code(sum_squares.python_function) (replacing sum_squares with the name of your python function). The code is not meant to be pretty, but it can sometimes help for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2b696-5007-46d7-b3fb-1bc3e1e7592b",
   "metadata": {},
   "source": [
    "## TF Function Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a147c-751f-40a3-92cd-73fb0ab9da20",
   "metadata": {},
   "source": [
    "Most of the time, converting a Python function that performs TensorFlow operations into a TF Function is trivial: decorate it with @tf.function or let Keras take care of it for you. However, there are a few rules to respect:\n",
    "\n",
    "1. If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of np.sum(), tf.sort() instead of the built-in sorted() function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    1. If you define a TF Function f(x) that just returns np.random.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    2. If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side effects to occur every time you call the TF Function, as they will only occur when the function is traced.\n",
    "    3. You can wrap arbitrary Python code in a tf.py_function() operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "2. You can call other Python functions of TF Functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with @tf.function\n",
    "3. If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF Function (e.g. in the build() method of a custom layer). If you want to assign a new value to the variable, make sure you call its assign() method, instead of using the = operator.\n",
    "4. The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only compiled .pyc Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "5. TensorFlow will only capture for loops that iterate over a tensor or a dataset. So make sure you use for i in tf.range(x) rather than for i in range(x), or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for example to create each layer in a neural network).\n",
    "6. As always, for performance reasons, you should prefer a vectorized implementation whenever you can, rather than using loops.\n",
    "\n",
    "If you would like to open the black box a bit further, for example to explore the generated graphs, you will find technical details in Appendix G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a771b-1fc6-4c88-940a-c0d674a548df",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e5d5b-aa28-450a-9d18-655582896691",
   "metadata": {},
   "source": [
    "1. **How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e49138-9667-45c5-8371-f82056a81e03",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">TensorFlow is an ML library with end-to-end features for bringing an ML project from research to production. Some of its main features are code optimizers, parallel distribution, function graphs, and Keras, among many others. As of 12/2024 the most popular deep learning library is PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491fb93-8e27-46c0-b03d-d5a3238eb808",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">TensorFlow is an open-source library for numerical computation, particularly well suited and fine-tuned for large-scale ML. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis and optimization capabilities (with a portable graph format that allows you to train a TensorFlow model in one environment and run it in another), an optimization API based on reverse-mode autodiff, and several powerful APIs such as tf.keras, tf.image, tf.signal, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490673-fefa-419f-b854-3315263e5e1a",
   "metadata": {},
   "source": [
    "2. **Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ce4d8-9f5d-481c-bc29-6cefb28cc9ed",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">No, while TensorFlow can do many of the things NumPy can do they are not exactly equivalent. NumPy uses 64-bit floating points while TensorFlow uses 32-bit. Many NumPY functions have a different naming convention in TensorFlow, for example np.sum() is equivalent to tf.reduce_sum(). Lastly, TensorFlow is more sensitive to typing that NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22016a-6d7d-4915-a7ae-2ca934c002da",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">Although TensorFlow offers most of the functionalities provided by NumPy, it is not a drop-in replacement, for a few reasons.\n",
    ">1. The names of the functions are not always the same (for example, tf.reduce_sum() vs np.sum()).\n",
    "2. Some functions do not behave in exactly the same way (for example, tf.transpose() creats a transposed copy of a tensor, while NumPy's T attribute creates a treansposed view, without actually copying any data).\n",
    "3. NumPy arrays are mutable, while TensorFlow tensors are not (but you can use a tf.Variable if you need a muatable object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a86c9-8004-4fd7-a0a1-80167b38d1ad",
   "metadata": {},
   "source": [
    "3. **Do you get the same result with tf.range(10) and tf.constant(np.arange(10))?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78906a64-c3e5-418d-b01b-257fd44ab716",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">Yes, you get the same result.\n",
    "\n",
    ">(<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>,\n",
    "\n",
    "> <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f8113-ac06-4297-bdac-ea2ee947d1e7",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">Both tf.range(10) and tf.constant(np.arange(10)) return a one-dimensional tensor containing the integers 0 to 9. However, the former uses 32-bit integers while the latter uses 64-bit integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae2392-877c-4354-982c-1b8323e18abb",
   "metadata": {},
   "source": [
    "4. **Can you name six other data structures available in TensorFlow, beyond regular tensors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea924e-71ae-4706-93ab-ed0fa790c54c",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">1. Sparse Tensors\n",
    "2. Tensor Arrays\n",
    "3. Ragged Tensors\n",
    "4. String Tensors\n",
    "5. Sets\n",
    "6. Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7fa164-9c98-486c-aa67-e6f40e4a8fa6",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">1. Sparse Tensors\n",
    "2. Tensor Arrays\n",
    "3. Ragged Tensors\n",
    "4. String Tensors\n",
    "5. Sets\n",
    "6. Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c29b2f-d248-4b2e-9a99-24f19ef63f41",
   "metadata": {},
   "source": [
    "5. **A custom loss function can be defined by writing a function or by subclassing the keras.losses.Loss class. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2566cb-d6d8-41a0-8e32-d6b19fc47ed0",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">If a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class. Writing a function is simpler but subclassing is more cohesive with the entire TensorFlow architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75472267-358a-488d-be68-b53444ce5ca8",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">When you want to define a custom loss function, in general you can just implement it as a regular Python function. However, if your custom loss function must support some hyperparameters (or any other state), then you should subclass the keras.losses.Loss class and implement the __init__() and __call()__ methods. If you want the loss function's hyperparameters to be saved along with the model, then you must also implement the get_config() method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c859aa-2c5b-47ce-bf19-7d7aca9534c9",
   "metadata": {},
   "source": [
    "6. **Similarly, a custom metric can be defined in a function or a subclass of keras.metrics.Metric. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326a754c-62f6-4267-8d10-61cab0b82ada",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">Defining a custom metric function is exactly the same as defining a custom loss function. Specifically, if you need a streaming metric you must subclass. My personal choice is to subclass in all cases because it gives me access to the full capabilities of the TensorFlow classes on top of my customized processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b942dfa-05c4-418f-9585-2faf1bc2e4e1",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">Much like custom loss functions, most metrics can be defined as regular Python functions. But if you want your custom metric to support some hyperparameters (or any other state), then you should subclass the keras.metrics.Metric class. Moreover, if computing the metfic over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch (e.g. as for precision and recall metrics), then you should subclass the keras.metrics.Metric class and implement the __init__(), __update_state()__, and __result()__ methods to keep track of a running metric during each epoch. You should also implement the __reset_states()__ method unless all it needs to do is reset all variables to 0.0. If you want the state to be saved along with the model, then you should implement the __get_config__() method as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc1057b-ee1f-47df-a11e-6f3f7b320188",
   "metadata": {},
   "source": [
    "7. **When should you create a custom layer versus a custom model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06639a46-48c7-466d-990d-7b8537bcf9ba",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">In general for clean programming and debugging purposes you should always use a custom layer for layer operations and custom models for model operations. This separation of duty is best practice. That said, custom models offer more functionality and can technically be used in place of custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d7645-079e-4827-8018-02df2ca52a03",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">You should distinguish the interal components of your model (i.e. layers or reusable blocks of layers) from the model itself (i.e. the object you will train). The former should subclass the keras.layers.Layer class and the latter should subclass the keras.models.Model class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2d882-df35-4922-b521-7d9bd523dc72",
   "metadata": {},
   "source": [
    "8. **What are some use cases that require writing your own custom training loop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15426460-ea3b-4d2d-b9f0-18209ba5d91c",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">If you need to control aspects of the training process that are mid-layer or mid-epoch you can use a custom loop to do so. The example the book gave was the Wide & Deep paper which uses different optimizers on different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffda58c-a65b-4a45-83eb-82a6b04a8bd2",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">Writing your own custom training loop is fairly advanced, so you should only do it if you really need to. Keras provides several tools to customize training without having to write a custom training loop: callbacks, custom regularizers, custom constraints, custom losses, and so on. You should use these instead of writing a custom training loop whenever possible: writing a custom training loop is more error-prone, and it will be harder to reuse the custom code you write. However, in some cases writing a custom training loop is necessary -- for example, if you want to use different optimizers for different parts of your neural network, like in the Wide & Deep paper. A custom training loop can also be useful when debugging, or when trying to understand exactly how training works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c5080-e975-46fc-ae53-82bfe89b349c",
   "metadata": {},
   "source": [
    "9. **Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c444c-39de-47ae-92f0-6a0b5c635c53",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">They can contain arbitrary Python code but if the code is not convertible to TF Functions then it will add memory burdens to your code. It is always preferable to encapsulte python code that is written friendly to TF Functions and TensorFlow graphs when using TensorFlow to harness the full potential TF offers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacc5dc-bb56-4e96-a3a5-1ec27d67cc80",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">Custom keras components should be convertible to TF Functions, which means they should stick to TF operations as much as possible and respect all the rules listed in 'TF Function Rules' on page 409. If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a tf.py_function() operation (but this will reduce performance and limit your model's portability) or set dynamic=True when creating the custom layer or model (or set run_eagerly=True when calling the model's compile() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09841192-a3cf-44af-ad5a-738c4f4bf49c",
   "metadata": {},
   "source": [
    "10. **What are the main rules to respect if you want a function to be convertible to a TF Function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf456c-2590-4137-afe8-36da885b7715",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">1. If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of np.sum(), tf.sort() instead of the built-in sorted() function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    1. If you define a TF Function f(x) that just returns np.random.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    2. If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side effects to occur every time you call the TF Function, as they will only occur when the function is traced.\n",
    "    3. You can wrap arbitrary Python code in a tf.py_function() operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "2. You can call other Python functions of TF Functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with @tf.function\n",
    "3. If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF Function (e.g. in the build() method of a custom layer). If you want to assign a new value to the variable, make sure you call its assign() method, instead of using the = operator.\n",
    "4. The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only compiled .pyc Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "5. TensorFlow will only capture for loops that iterate over a tensor or a dataset. So make sure you use for i in tf.range(x) rather than for i in range(x), or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for example to create each layer in a neural network).\n",
    "6. As always, for performance reasons, you should prefer a vectorized implementation whenever you can, rather than using loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3511c4c-76b6-4191-9873-23867f3c21ec",
   "metadata": {},
   "source": [
    "Book Answer:\n",
    "\n",
    ">1. If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So make sure you use tf.reduce_sum() instead of np.sum(), tf.sort() instead of the built-in sorted() function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    1. If you define a TF Function f(x) that just returns np.random.rand(), a random number will only be generated when the function is traced, so f(tf.constant(2.)) and f(tf.constant(3.)) will return the same random number, but f(tf.constant([2., 3.])) will return a different one. If you replace np.random.rand() with tf.random.uniform([]), then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    2. If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side effects to occur every time you call the TF Function, as they will only occur when the function is traced.\n",
    "    3. You can wrap arbitrary Python code in a tf.py_function() operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "2. You can call other Python functions of TF Functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with @tf.function\n",
    "3. If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF Function (e.g. in the build() method of a custom layer). If you want to assign a new value to the variable, make sure you call its assign() method, instead of using the = operator.\n",
    "4. The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only compiled .pyc Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "5. TensorFlow will only capture for loops that iterate over a tensor or a dataset. So make sure you use for i in tf.range(x) rather than for i in range(x), or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for example to create each layer in a neural network).\n",
    "6. As always, for performance reasons, you should prefer a vectorized implementation whenever you can, rather than using loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b93346-38f7-4e70-aafb-bb9acd75a85b",
   "metadata": {},
   "source": [
    "11. **When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f1ab2-bc17-48f5-99c5-4a72dcd2d87b",
   "metadata": {},
   "source": [
    "My Answer:\n",
    "\n",
    ">From chapter 10, when your model involves loops, varying shapes, conditional branching, or other dynamic behavior the sublcassing API is the tool to use. This involves subclassing the tf.keras.Model class and peforming the dynamic operations in the call() method. The extra flexibility comes at a cost: your model's architecture is hidden within the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and when you call the summary() method, you only get a list of layers without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ca20f-efd6-4966-9a08-c296cc2d3fc8",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1358e9-3307-4eeb-9776-18ed91f282f4",
   "metadata": {},
   "source": [
    "12. **Implement a custom layer that performs *Layer Normalization* (we will use this type of layer in Chapter 15):**\n",
    "\n",
    "    1. The build() method should define two trainable weights $\\alpha$ and $\\beta$, both of shape input_shape[-1:] and data type tf.float32. $\\alpha$ should be initialized with 1s and $\\beta$ with 0s\n",
    "\n",
    "    2. The call() method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance's features. For this, you can use tf.nn.moments(inputs, axes=-1, keepdims=True), which returns the mean $\\mu$ and the variance $\\sigma^{2}$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should computer and return $\\large\\alpha\\bigotimes\\frac{(X - \\mu)}{\\sigma + \\epsilon} + \\beta$, where $\\bigotimes$ represents itemwise multiplication and $\\epsilon$ is a smoothing term (small constant to avoid division by zero, e.g. 0.001)\n",
    "\n",
    "    3. Ensure that your custom layer produces the same (or very nearly the same) outoput as the keras.layers.LayerNormalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bfc97f-7098-4d61-9a4c-336c1daea037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e483fef-5995-4d26-a6b8-838cd0a28f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56670af3-7a9f-4648-8843-1e92759d2a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc7a4f-fb56-4a6e-95e1-7daf08210c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ce7f-ac02-45de-bd5a-eec3b61e5027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19575386-b124-4952-a369-9f96ebc494f9",
   "metadata": {},
   "source": [
    "13. **Train a model using a custom training loop to tackle the Fashion MNIST dataset (see Chapter 10).**\n",
    "\n",
    "    1. Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch.\n",
    "    \n",
    "    2. Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419fb924-5af8-4ab6-a862-b7d79a746011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb16dd8-3429-49be-9f1e-8a3e1b664fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5aa42-5cc7-4f39-8ce0-8a50ffcdb32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ad9d4-6844-4977-9f9d-dc7b89b9e5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d5fdf-bc7f-4313-9aff-658a17d4b2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b18f22-d5d3-451b-ae46-0b5c30002f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
