{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f5a8b1-3307-4cea-85ea-73194da46cbc",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2dddf4-1d95-4377-86ea-6971a7c5ff16",
   "metadata": {},
   "source": [
    "A common approach for natural language tasks is to use recurrent neural networks. We will therefore continue to explore RNNs, starting with a *character RNN*, trained to predict the next character in a sentence. We will first use a *stateless RNN* (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a *stateful RNN* (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns). Next, we will build an RNN to perform sentiment analysis (e.g., reading movie reviews and extracting the rater's feeling about the movie), this time treating sentences as sequences of words, rather than characters. Then we will show how RNNs can be used to build an Encoder-Decoder architecture capable of performing neural machine translation (NMT). \n",
    "\n",
    "In the second part of this chapter, we look at *attention mechanisms*. As their name suggests, these are neural network components that learn to select the part of the inputs that the rest of the model should focus on at each time step. First, we will see how to boost the performance of an RNN-based Encoder-Decoder architecture using attention, then we will drop RNNs altogether and look at a very successful attention-only architecture called the *Transformer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6c2ea-f38c-49d0-a7b2-2bf536c306aa",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71361deb-9759-4672-9737-b4687f87c1a9",
   "metadata": {},
   "source": [
    "Let's look at how to build a Char-RNN, step by step, starting with the creation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a1354-372d-4531-9371-bb65274098f3",
   "metadata": {},
   "source": [
    "### Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd2bf1e-1c0f-498c-90cf-24f5263e03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ebf7f7f-4bb7-4021-b332-5570732f2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First, let's download all of Shakespeare's work, using Keras' handy get_file() function\n",
    "'''\n",
    "\n",
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff9a0c-1501-402d-b961-ee433e4ca144",
   "metadata": {},
   "source": [
    "Next, we must encode every character as an integer. In this case, it will be simpler to use Keras' Tokenizer class. First, we need to fit a tokenize to the text: it will fin all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (t does not start at 0, so we can use that value for masking, as we will see later in this chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0e8dd7-dea9-4d6d-b279-7bec6598d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0aaf8e-cf9c-4885-915d-bc703c0e38fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[20, 6, 9, 8, 3]], ['f i r s t'], 'Max ID: 39 | Total Characters: 1115394')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We set char_level=True to get character-level encoding rather than the default word-level encoding\n",
    "'''\n",
    "\n",
    "max_id = len(tokenizer.word_index)\n",
    "total_tokens = len(tokenizer.texts_to_sequences([shakespeare_text])[0])\n",
    "tokenizer.texts_to_sequences(['First']), tokenizer.sequences_to_texts([[20, 6, 9 , 8, 3]]), f'Max ID: {max_id} | Total Characters: {total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e403bba5-6a06-454b-9dff-3432f439c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "Let's encode the full text so each character is represented by its ID.\n",
    "We subtract 1 to get IDs from 0 to 38, rather than from 1 to 39\n",
    "'''\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe11ad-a878-44ef-9fca-07396d740570",
   "metadata": {},
   "source": [
    "Before we continue, we need to split the dataset into a training set, a validation set, and a test set. We can't just shuffle all the characters in the text, so how do you split a sequential dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea3602-aba2-4aa4-b67a-589907b25512",
   "metadata": {},
   "source": [
    "### How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db30aa-c6f9-4c6a-bc7b-d3ac84f1f11a",
   "metadata": {},
   "source": [
    "When dealing with time series, you would in general split across time: for example, you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2018 for the test set. However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on. For example, if you have data about the financial health of 10,000 companies from 2000 to 2018, you might be able to split this data across the different companies. It's very likely that many of these companies will be strongly correlated, though (e.g. whole economic sectors may go up or down jointly), and if you have correlated companies across the training set and the test set your test set will not be as useful, as its measure of the generalization error will be optimistically biased.\n",
    "\n",
    "So, it is often safer to split across time - but this implicitly assumes that the patterns the RNN can learn in the past (in the training set) will still exist in the future. In other words, we assume that the time series is *stationary* (at least in a wide sense). For many time series this assumption is reasonable (e.g. chemical reactions should be fine, since the laws of chemistry don't change every day), but for many others it is not (e.g. financial markets are notoriously not stationary since patterns disappear as soon as traders spot them and start exploiting them). **To make sure the time series is indeed sufficiently stationary, you can plot the model's errors on the validation set across time: if the model performs much better on the first part of the validation set than on the last part, then the time series may not be stationary enough, and you might be better off training the model on a shorter time span.**\n",
    "\n",
    "In short, spiltting a time series into a training set, a validation set, and a test set is not a trivial task, and how it's done will depend strongly on the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3469d8ea-4e37-4b1c-b75a-c1508e191230",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For this Shakespeare example we take the first 90% of the text for the training set and use the rest for validation and test\n",
    "'''\n",
    "\n",
    "# Convert text to integer sequence ---\n",
    "sequence = tokenizer.texts_to_sequences([shakespeare_text])[0]  # Flattened list of token IDs\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.9\n",
    "val_ratio = 0.05  # test will be the rest (0.05)\n",
    "\n",
    "# Compute split indices\n",
    "total_tokens = len(sequence)\n",
    "train_end = int(total_tokens * train_ratio)\n",
    "val_end = train_end + int(total_tokens * val_ratio)\n",
    "\n",
    "#  Split the data\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4081161-43a7-45ff-bb90-95636319abb5",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6573a-1195-497f-83b4-1250367c6be8",
   "metadata": {},
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can't just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset's window() method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. **This is called *truncated propagation through time*.** Let's call the window() method to create a dataset of short text windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fb5a5d-399f-4a44-abd7-9559c13273b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can try tuning n_steps: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than n_steps, so don't make it too small\n",
    "'''\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1da51a-982e-4a43-91dd-33eb30ebef3f",
   "metadata": {},
   "source": [
    "**By default, the window() method creates nonoverlapping windows, but to get the largest possible training set we use shift=1 so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on.** To ensure that all windows are exactly 101 characters long (which will allow us to create batches without having to do any padding), we set drop_remainder=True (otherwise the last 100 windows will contains 100 characters, 99 characters, and so on down to 1 character).\n",
    "\n",
    "The window() method creates a dataset that contains windows, each of which is also represented as a dataset. It's a *nested dataset*, analogous to a list of lists. This is useful when you want to transform each window by calling its dataset methods (e.g. to shuffle them or batch them).**However, we cannot use a nested dataset directly for training, as our moedel will expect tensors as input, not datasets. So, we must call the flat_map() method: it converts a nested dataset into a *flat dataset*.**\n",
    "\n",
    "Moreover, the flat_map() method takes a function as an argument, which allows you to transform each dataset in the nested dataset before flattening. For exapmle, if you pass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it's a dataset of tensors of size 2. With that in mind, we are ready to flatten our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501e53dd-4395-4611-862b-5bf7b0f13fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de50873-a15a-42c8-b939-f7a77a7a64c6",
   "metadata": {},
   "source": [
    "Notice that we call batch(window_length) on each window: since all windows have exactly that length, we will get a single tensor for each of them. Since Gradient Descent works best when the instances in the training set are independent and identically distributed, we need to shuffle these windows. Then we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070cbbc4-3463-4c9d-a6d6-0323ac04cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb53e858-256b-4b98-a533-c20b5a4c8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Catagorical input features should generally be encoded, usually as one-hot vectors or embeddings. Here, we will encode each character using a one-hot vector (because there are few [39])).\n",
    "'''\n",
    "\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Finally, add prefetching\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae359e97-4907-4160-87b5-12a981d6f3d7",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b4f14-6509-4117-b9e6-36d7b7081f89",
   "metadata": {},
   "source": [
    "Preparing the dataset was the hardest part. Now let's create the model. We can use an RNN with 2 GRU layers of 128 units each and a 20% dropout on both the inputs (dropout) and hidden states (recurrent_dropout). The output layer is a time-distributed Dense layer. This time this layer must have 39 units because there are 39 distinct characters in the text, and we want to output a probability for each possible character. We apply the softmax activation function to the outputs of the Dense layer. We can then compile this model, using the 'sparse_categorical_crossentropy' loss and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70fa2d1d-b2e5-4c83-a0f6-8e7d149af31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m3921/3921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3125s\u001b[0m 796ms/step - loss: 1.7444\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steph\\anaconda3\\envs\\OReilly\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3921/3921\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3152s\u001b[0m 804ms/step - loss: 1.5191\n"
     ]
    }
   ],
   "source": [
    "# This cell takes several hours to run 20 epochs\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Input([None, max_id]),\n",
    "    GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    TimeDistributed(Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f44ec-c1a8-43d4-933d-a4e7585968e8",
   "metadata": {},
   "source": [
    "### Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee181cc-4ee5-47f4-8728-4fb0afcad858",
   "metadata": {},
   "source": [
    "Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let's create a little function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079e8b0e-4e7a-4e53-afe3-1b4898e88678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['                                                     ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "# Now predict\n",
    "X_new = preprocess(['How are yo'])\n",
    "Y_pred = model.predict(X_new) + 1\n",
    "tokenizer.sequences_to_texts(Y_pred.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae35335-4080-4c69-bb66-964d862c6dc4",
   "metadata": {},
   "source": [
    "### Generating Fake Shakespearean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ee41d-0139-4317-88cf-100b12d0bb79",
   "metadata": {},
   "source": [
    "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice, this often leads to the same words being repeated over and over again. Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow's tf.random.categorical() function. \n",
    "\n",
    "The categorical() function samples random class indices, given the class log probabilities (logits). To have more control over the diversity of the generated text, we can divide the logits by a number called the *temperature*, which we can tweak as we wish: a temperature close to 0 will favor the high-probability characters, while a very high temperature will give all characters an equal probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73af5a4b-3d54-4166-abbd-ab3c5219fcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 983ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this fair for the rest,\\nand we will not be so will '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "complete_text('t', temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bad3fbc-9373-4600-b2f1-e001abbefc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"w'dined.\\nshe gold. i'll your yield?\\nbeyech! scyiath\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_text('w', temperature=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb4b09-abff-4f37-bac8-a27f3a7bf120",
   "metadata": {},
   "source": [
    "**To generate more convincing text**, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization (for example, you could set recurrent_dropout=0.3 in GRU layers). Moreover, the model is currently incapable of learning patterns longer than n_steps, which is just 100 characters. You could try making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. Alternatively, **you could use a stateful RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf2c10-98d3-4cf0-888c-330ba9b0c7c0",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f1284-6547-4efe-9217-b668d4901435",
   "metadata": {},
   "source": [
    "Until now, we have used only *stateless RNNs*: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore. What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? This way the model can learn long-term patterns despite only backpropagating through short sequences. This is called a *stateful RNN*. \n",
    "\n",
    "**First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off.** So the first thing we need to do to build a stateful RNN is to use sequential and nonoverlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs).\n",
    "\n",
    "**Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.** Indeed, if we were to call batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these windows where it left off. **The simplest solution to this problem is to juse use \"batches\" containing a single window.**\n",
    "\n",
    "Batching is harder, but it is not impossible. For example, we could chop Shakespeare's text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows)) to create proper consecutive batches, where the nth input sequence in a batch starts off exactly where the nth input sequence ended in the previous batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbb0db2-e0d9-41d3-b4bb-c242afe9b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_end])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7833d-4de3-41e2-b052-3ba202cc2975",
   "metadata": {},
   "source": [
    "Now let's create a stateful RNN. First, we need to set stateful=True when creating every recurrent layer. Second, the stateful RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch), so we must set the batch_input_shape argument in the first layer. Note that we can leave the second dimension unspecified, since the inputs could have any length.\n",
    "\n",
    "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small callback.\n",
    "\n",
    "**After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical *stateless* model, and copy the stateful model's weights to this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2daa423f-39f5-46d9-9d19-701a0bf27b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This cell takes an absolutely absurd amount of time. Run at your own risk.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This cell takes an absolutely absurd amount of time. Run at your own risk.'''\n",
    "\n",
    "# model = Sequential([\n",
    "#     Input(batch_shape=(1, window_length - 1, max_id)),\n",
    "#     GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.0),\n",
    "#     GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.0),\n",
    "#     TimeDistributed(Dense(max_id, activation='softmax'))\n",
    "# ])\n",
    "\n",
    "# from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# class ResetStatesCallback(Callback):\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         for layer in self.model.layers:\n",
    "#             if hasattr(layer, \"reset_states\"):\n",
    "#                 layer.reset_states()\n",
    "\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', run_eagerly=True)\n",
    "# model.fit(dataset, epochs=1, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074347b0-0ba7-4617-bbba-5ac5663b8ae5",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ab4bc-6c6f-4141-8d62-7be85fa778ff",
   "metadata": {},
   "source": [
    "The IMDb reviews dataset is the \"hello world\" of natural language processing. The IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled in a short amount of time, but challenging enough to be fun and rewarding. Keras provides a simple function to load it.\n",
    "\n",
    "The dataset is already preprocessed for you: **X_train consists of a list of reviews, each of which is represented as a NumPy array of integers, where each integer represents a word. All punctuation was removed, and then words were converted to lowercase, split by spaces, and finally indexed by frequency (so low integers correspond to frequent words).** The integers 0, 1, and 2 are special: they represent the padding token, the *start-of-sequence* (SSS) token, and unknown words, respectively. If you want to visualize a review, you can decode it like in the example below.\n",
    "\n",
    "In a real project, you will have to preprocess the text yourself. When encoding words, it filters out a lot of characters, including most punctuation, line breaks, and tabs (but you can change this by setting the ***filters*** argument). Most importantly, it uses spaces to identify word boundaries. This is OK for English and many other scripts that use spaces between words, but not all scripts use spaces this way. Chinese does not use spaces between words, Vietnamese uses spaces even within words, and languages such as German often attach multiple words together, without spaces.\n",
    "\n",
    "Fortunatley, there are better options! The 2018 paper by Taku Kudo \"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\" introduced an unsupervised learning techinque to tokenize and detokenize text at the subword level in a language-independent way, treating spaces like other characters. With this approach, even if your model encounters a word it has never seen before, it can still reasonably guess what it means. Google's *SentencePiece* project provides an open source implementation of this paper. \n",
    "\n",
    "Last but not least, the Tensorflow team released the TF.Text library in June 2019, which implements various tokenization strategies, including WordPiece (a variant of byte pair encoding).\n",
    "\n",
    "If you want to deploy your model to a mobile device or a web browser, and you don't want to have to write a different preprocessing function every time, then you will want to handle preprocessing using only TensorFlow operations, so it can be included in the model itself. An exmple of this is shown below.\n",
    "\n",
    "TF Transform (introduced in Chapter 13) provides some useful functions to handle such vocabularies. For example, check out the tft.compute_and_apply_vocabulary() function: it will go through the dataset to find all distinct words and build the vocabulary, and it will generate the TF operations required to encode each word using this vocabulary.\n",
    "\n",
    "Now we are ready to create the final training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4f8abd-b6ae-47ee-8f45-cacf8127d414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the IMDb dataset'''\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ffd23b-5486-439d-889d-5ee005879b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Decode a Review'''\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7a24821-43e6-4935-a3c3-716c0543f98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\1.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d91227425434621ab9691eb5ddb17b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3af6bd6fed4922b6743b495e1697f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46af2a5ef0f64f438f7eac416a3c6336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0133e00d4945bf87ca78417f8e13bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed77e1b84b214224b790d4b09147cffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\incomplete.CLH1XK_1.0.0\\imdb_r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6450d18132c4794a73002e910b6f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a9d7d82f384c25bd6368b590f6442d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\incomplete.CLH1XK_1.0.0\\imdb_r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574a5701ccb54d389e28a7bf144d38b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2afd139e394c56b0fb5783108e8f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\incomplete.CLH1XK_1.0.0\\imdb_r…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Steph\\AppData\\Local\\Temp\\tmp2ttykdih\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Most common words: [(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 52ms/step - accuracy: 0.5228 - loss: 0.6912\n"
     ]
    }
   ],
   "source": [
    "'''Preprocessing Using Only Tensorflow Operations'''\n",
    "import tensorflow_datasets as tfds\n",
    "import tempfile, shutil\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Temp Directory\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# Load the dataset and define the training partition\n",
    "try:\n",
    "    datasets, info = tfds.load(\n",
    "        \"imdb_reviews/plain_text\",\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "        data_dir=tmp_dir,\n",
    "    )\n",
    "    train_size = info.splits['train'].num_examples\n",
    "\n",
    "    # Write the preprocessing function\n",
    "    def preprocess(X_batch, y_batch):\n",
    "        X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "        X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \") # Replace the start of an HTML line break tag in common forms with blank\n",
    "        X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \") # Strip non-word characters while keeping contractions\n",
    "        X_batch = tf.strings.split(X_batch) # Split by spaces\n",
    "        return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch # Pad all reviews to ensure they're the same length\n",
    "    \n",
    "    # Construct the vocabulary\n",
    "    from collections import Counter\n",
    "    vocabulary = Counter()\n",
    "    for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "        for review in X_batch:\n",
    "            vocabulary.update(list(review.numpy()))\n",
    "    \n",
    "    print(f'Most common words: {vocabulary.most_common()[:3]}')\n",
    "    \n",
    "    # Truncate the vocabulary to the 10,000 most common words\n",
    "    vocab_size = 10_000\n",
    "    truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "    \n",
    "    # Replace each word with its ID / vocabulary index\n",
    "    words = tf.constant(truncated_vocabulary)\n",
    "    word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "    vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "    num_oov_buckets = 1_000\n",
    "    table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "    # Define and encode words\n",
    "    def encode_words(X_batch, y_batch):\n",
    "        return table.lookup(X_batch), y_batch\n",
    "\n",
    "    train_set = datasets['train'].batch(32).map(preprocess)\n",
    "    train_set = train_set.map(encode_words).prefetch(1)\n",
    "\n",
    "    # Build and train model\n",
    "    embed_size = 128\n",
    "    model = Sequential([\n",
    "        Input([None]),\n",
    "        Embedding(vocab_size + num_oov_buckets, embed_size),\n",
    "        GRU(128, return_sequences=True),\n",
    "        GRU(128),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(train_set, epochs=1)\n",
    "    \n",
    "finally:\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b15b2-2238-4ec2-b883-2880add44346",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c9049-1009-4863-b0d9-873c1682dfe3",
   "metadata": {},
   "source": [
    "As it stands, the model will need to learn that the padding tokens should be ignored. Why don't we tell the model to ignore the padding tokens, so that it can focus on the data that actually matters? It's actually quite trivial: simply add mask_zero=True when creating the Embedding Layer. This means that padding tokens (whose ID is 0) will be ignored by all downstream layers.\n",
    "\n",
    "The way this works is that the Embedding layer creates a *mask tensor* equal to K.not_equal(inputs, 0) (where K = keras.backend): it is a Boolean tensor with the same shape as the inputs, and it is equal to False anywhere ther word IDs are 0, or True otherwise.\n",
    "\n",
    "Each layer may handle the mask differently, but in general they simply ignore masked time steps (i.e. time steps for which the mask is False). The LSTM and GRU layers have an optimized implementation for GPUs, based on Nvidia's cdDNN library. However, this implementation does not support masking. If your model uses a mask, then these layers will fall back to the (much slower) default implementation. Note that the optimized implementation also requires you to use the default values for several hyperparameters: activation, recurrent_activation, recurrent_dropout, unroll, use_bias, and reset_after.\n",
    "\n",
    "All layers that receive the mask must support masking. Any layer that supports masking must have a supports_masking attribute equal to True. If you want to implement your own custom layer with masking support, you should add a mask argument to the call() method (and obviously make the method use the mask somehow ). Additionally, you should set self.supports_masking=True in the constructor. If your layer does not start with an Embedding layer, you may use the keras.layers.Masking layer instead: it sets the mask to K.any(K.not_equal(inputs, 0), axis=-1), meaning that time steps where the last dimension is full of zeros will be masked out in subsequent layers.\n",
    "\n",
    "Using masking layers and automatic mask propagation works best for simple Sequential models. It will not always work for more complex models, such as when you need to mix Conv1D layers with recurrent layers. In such cases, you will need to explicitly compute the mask and pass it to the appropriate layers, using either the Functional API or the Subclassing API. For example, the following model is identical to the previous model, except it is built using the Functional API and handles masking manually.\n",
    "\n",
    "If all postive words and all negative words form clusters, then this will be helpful for sentiment analysis. So instead of using so many parameters to learn word embeddings, let's see if we can't just reuse pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428111ef-2d5f-46a8-bef0-af47d5d6da32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Steph\\anaconda3\\envs\\OReilly\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Steph\\anaconda3\\envs\\OReilly\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "K = tf.keras.backend\n",
    "inputs = Input(shape=[None])\n",
    "mask = Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = GRU(128)(z, mask=mask)\n",
    "outputs = Dense(1, activation='sigmoid')(z)\n",
    "model = Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49fbfe-7b76-40d8-af70-d2995ecc88c5",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5e2e5-ea2b-4291-a603-deec6f93ecfa",
   "metadata": {},
   "source": [
    "The TensorFlow Hub project makes it easy to reuse pretrained model components in your own models. These model components are called *modules*. Simply browse the TF Hub repository (https://tfhub.dev), find the one you need, and copy the code example into your project, and the module will be automatically downloaded, along with its pretrained weights, and included in your model. By default, a hub .KerasLayer is not trainable, but you can set trainable=True when creating it to chagne that so that you can fine-tune it for your task. \n",
    "\n",
    "Not all TF Hub modules support TensorFlow 2, so make sure you choose a module that does. By default, TF Hub will cache the downloaded files into the local system's temporary directory. You may prefer to download them into a more permanent directory to avoid having to download them again after every system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to the directory of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc087348-7144-4363-8b4a-b87cf223f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tempfile, shutil\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# import tensorflow_hub as hub\n",
    "# import tf_keras as keras\n",
    "# from tf_keras import layers\n",
    "\n",
    "# handle = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     hub.KerasLayer(handle, dtype=tf.string, input_shape=(), trainable=False),\n",
    "#     layers.Dense(128, activation=\"relu\"),\n",
    "#     layers.Dense(1, activation=\"sigmoid\"),\n",
    "# ])\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# tmp_dir = tempfile.mkdtemp()\n",
    "# try:\n",
    "#     datasets, info = tfds.load(\n",
    "#         \"imdb_reviews/plain_text\",\n",
    "#         as_supervised=True,\n",
    "#         with_info=True,\n",
    "#         data_dir=tmp_dir,\n",
    "#     )\n",
    "#     train_set = datasets[\"train\"].batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "#     history = model.fit(train_set, epochs=1)\n",
    "# finally:\n",
    "#     shutil.rmtree(tmp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5027984-68a8-4504-afcf-a1037f04ca92",
   "metadata": {},
   "source": [
    "Next, let's look at another important NLP task: *neural machine translation* (NMT), first using a pure Encoder-Decoder model, then improving it with attention mechanisms, and finally looking at the extraordinary Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19275ac3-d479-4bb0-8d6f-3009a683863e",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Network for Neural Machine Translation (NMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d592243-f596-4408-b7a5-253b515d731a",
   "metadata": {},
   "source": [
    "Let's take a look at a simple neural machine translation model. In short, the English sentences are fed to the encoder, and the decoder outputs the French translations. For the very first word, it is given the start-of-sequence (SOS) token. The decoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that the English sentences are reversed before they are fed to the encoder. For exapmle, \"I drink milk\" is reversed to \"milk drink I\". This ensures that the beginning of the English sentence will be fed last to the encoder, which is useful because that's generally the first thing that the decoder needs to translate.\n",
    "\n",
    "Each word is initially represented by its ID. Next, an embedding layer returns the word embedding. These word embeddings are what is actually fed to the encoder and the decoder. At each step, the decoder outputs a score for each word in the output vocabulary (i.e French), and then the softmax layer turns these scores into probabilities. The word with the highest probability is output. This is very much like a regular classification task, so you can train the model using the \"sparse_categorical_crossentropy\" loss. Note that at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, simply feed the decoder the word that it output at the previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b66401-f68b-4443-868f-2e927df10282",
   "metadata": {},
   "source": [
    "**There are a few more details to handle if you implement this model:**\n",
    "\n",
    "> So far we have assumed that all input sequences have a constant length. But obviously sentence lengths vary. Since regular tensors have fixed shapes, they can only contain sentences of the same length. You can use masking to handle this, as discussed earlier. However, if the sentences have very different lengths, you can't just crop them like we did for sentiment analysis (because we want full translations). Instead, group sentences into buckets of similar lengths using padding for the shorter sequences to ensure all sentences in a bucket have the same length (check out the tf.data.experimental.bucket_by_sequence_length() function)\n",
    "<br><br>\n",
    "> We want to ignore any ouptut past the EOS token, so these tokens should not contribute to the loss. For example, if the model outputs \"Je bois du lait <eos> oui\", the loss for that last word should be ignored.\n",
    "<br><br>\n",
    "> When the output vocabulary is large (which is the case here), outputting a probability for each and every possible word would be terribly slow. To avoid this, one solution is to look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits. This *sampled softmax* technique was introduced in 2015. In TensorFlow you can use the tf.nn.sampled_softmax_loss() function for this during training and use the normal softmax function at inference time (sampled softmax cannot be used at inference because it requires the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21c67e-f7f7-4e68-9eb7-a33b8cde1baa",
   "metadata": {},
   "source": [
    "The TensorFlow Addons project includes many sequence-to-sequence tools to let you easily build production-ready Encoder-Decoders. The code is mostly self-explanatory, but there are a few points to note. First, we set return_state=True when creating the LSTM layer so that we can get its final hidden state and pass it to the decoder. Since we are using an LSTM cell, it actually returns two hidden states (short term and long term). The TrainingSampler is one of several samplers available in TensorFlow Addons: their role is to tell the decoder at each step what it should pretend the previous output was. During inference, this should be the embedding of the token that was actually output. During training, it should be the embedding of the previous target token: this is why we used the TrainingSampler. In practice, it is often a good idea to start training with the embedding of the target of the previous time step and gradually transition to using the embedding of the actual token that was output at the previous step. The ScheduledEmbeddingTrainingSampler will randomly choose between the target or the actual output, with a probability that you can gradually change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92577c3-2167-41f4-a903-3f8de2eb908d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TFA explicitly supports TensorFlow ≥ 2.12 and < 2.15'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TFA explicitly supports TensorFlow ≥ 2.12 and < 2.15'''\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "# from tensorflow.keras.layers import LSTM, LSTMCell\n",
    "\n",
    "# # encoder/decoder token ids: [batch, time]\n",
    "# encoder_inputs = Input(shape=(None,), dtype=tf.int32, name=\"encoder_inputs\")\n",
    "# decoder_inputs = Input(shape=(None,), dtype=tf.int32, name=\"decoder_inputs\")\n",
    "\n",
    "# # decoder lengths: [batch]\n",
    "# sequence_lengths = Input(shape=(), dtype=tf.int32, name=\"decoder_lengths\")\n",
    "\n",
    "# embeddings = Embedding(vocab_size, embed_size, name=\"token_embedding\")\n",
    "# encoder_embeddings = embeddings(encoder_inputs)   # [batch, time, embed]\n",
    "# decoder_embeddings = embeddings(decoder_inputs)   # [batch, time, embed]\n",
    "\n",
    "# # Encoder\n",
    "# encoder = LSTM(512, return_state=True, name=\"encoder_lstm\")\n",
    "# _, state_h, state_c = encoder(encoder_embeddings)\n",
    "# encoder_state = [state_h, state_c]  # LSTMCell state: [h, c]\n",
    "\n",
    "# # Decoder\n",
    "# sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "# decoder_cell = LSTMCell(512, name=\"decoder_cell\")\n",
    "# output_layer = Dense(vocab_size, name=\"vocab_projection\")\n",
    "\n",
    "# decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "#     cell=decoder_cell,\n",
    "#     sampler=sampler,\n",
    "#     output_layer=output_layer,\n",
    "# )\n",
    "\n",
    "# final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "#     decoder_embeddings,\n",
    "#     initial_state=encoder_state,\n",
    "#     sequence_length=sequence_lengths,\n",
    "# )\n",
    "\n",
    "# logits = final_outputs.rnn_output                 # [batch, time, vocab]\n",
    "# Y_proba = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "# model = Model(\n",
    "#     inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "#     outputs=Y_proba,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086700d5-33a0-4047-a74c-1dd3632a9107",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9387c1b-ab34-471a-93f6-8a2bbe431dc8",
   "metadata": {},
   "source": [
    "At each time step, a regular recurrent layer only looks at past and present inputs before generating its output. This type of RNN makes sense when forecasting time series, but for many NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at the next words before encoding a given word. To implement this, run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left. Then simply combine their outputs at each time step, typically by concatenating them. This is called a *bidirectional recurrent layer*.\n",
    "\n",
    "To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a keras.layers.Bidirectional layer. The Bidirectional layer will create a clone of the GRU layer (but in the reverse direction), and it will run both and concatenate their outputs. So although the GRU layer has 10 units, the Bidirectional layer will output 20 values per time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c842226-2b17-4490-85b0-057e6d6a57e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Bidirectional name=bidirectional, built=False>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "Bidirectional(GRU(10, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853797d5-34bc-4913-9d6d-cec9c85541c1",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e93b7-3f76-4dd4-b33d-71570e63811b",
   "metadata": {},
   "source": [
    "Suppose you train an Encoder-Decoder model, and use it to translate the French sentence \"Comment vas-tu?\" to English. You are hoping that it will output the proper translation (\"How are you?\") but unfortunately it outputs \"How will you?\". By greedily outputting the most likely word at every step, it ended up with a suboptimal translation. How can we give the model a chance to go back and fix mistakes it made earlier? One of the most common solutions is *beam search*: it keeps track of a short list of the k most promising sentences (say, the top three), and at each decoder step it tries to extend them by one word, keeping only the k most likely sentences. The parameter k is called the *beam width*. We can boost our Encoder-Decoder model's performance without any extra training simply by using it more wisely. You can implement beam search fairly easily using TensorFlow Addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5c73d-1893-4266-9892-9eab0c068278",
   "metadata": {},
   "source": [
    "We first create a BeamSearchDecoder, which wraps all the decoder clones (in this case 10 clones). Then we create one copy of the encoder's final state for each decoder clone, and we pass these states to the decoder, along with the start and end tokens. With all this, you can get good translations for fairly short sentences. Unfortunately, this model will be really bad at translating long sentences. Once again, the problem comes from the limited short-term memory of RNNs. *Attention mechanisms* are the game-changing innovation that addressed this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47764d82-0bdb-4e77-8250-c49cf5358f7f",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d7174-ea48-4a0a-a153-a799cac99b18",
   "metadata": {},
   "source": [
    "In a groundbreaking 2014 paper by Dzmitry Bahdanau et al they introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized neural machine translation (and NLP in general), allowing a significant imrprovement in the state of art, especially for long sentences (over 30 words.).\n",
    "\n",
    "At each time step, the decoder's memory cell computes a weighted sum of all these encoder outputs: this determines which words it will focus on at this step. The weight $\\alpha_{(t,j)}$ is the weight of the $i^{th}$ encoder ouptut at the $t^{th}$ decoder time step. For example, if the weight $\\alpha_{(3, 2)}$ is much larger than the weights $\\alpha_{(3, 0)}$ and $\\alpha_{(3, 1)}$, then the decoder will pay much more attention to word number 2 (\"milk\") than to the other two words, at least at this time step. The rest of the decoder works just like earlier. \n",
    "\n",
    "But where do these $\\alpha_{(t,j)}$ weights come from? It's actually pretty simple: they are generated by a type of small neural network called an *alignment model* (or an *attention layer*), which is trained jointly with the rest of the Encoder-Decoder model. This time-distributed Dense layer with a single neuron, which receives as input all the encoder outputs, concatenated with the decoder's previous hidden state The layer outputs a score (or energy) for reach encoder output: this score measures how well each output is aligned wit hteh decoder's previous hidden state. Finally, all the scores go through a softmax layer to get a final weight for each encoder output. All the weights for a given decoder time step add up to 1. This particular attention mechanism is called *Bahdanau attention*. Since it concatenates the encoder output with the decoder's previous hidden state, it is sometimes called *concatenative attention* or *additive attention*.\n",
    "\n",
    "***Recall that a time-distributed Dense layer is equiavalent to a regular Dense layer that you apply independently at each time step (only much faster)*** \n",
    "\n",
    "If the input sentence is n words long, and assuming the output sentence is about as long, then this model will need to compute about $n^2$ weights. Fortunately, this quadratic computational complexity is still tractable because even long sentences don't have thousands of words. \n",
    "\n",
    "Another common attention mechanism was proposed shortly after in a 2015 paper. Because the goal of the attention mechanism is to measure the similarity between one of the encoder's outputs and the decoder's previous hidden state, the authors proposed to simply compute the dot product of these two vectors, as this is often a fairly good similarity measure, and model hardware can compute it much faster. For this to be possible, both vectors must have the same dimensionality. This is called *Luong attention*, or sometimes *multiplicative attention*. The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights, just like the Bahdanau attention. Another simplification they proposed was to use the decoder's hidden state at the current step rather than at the previous time step, then to use the output of the attention mechanism directly to compute the decoder's predictions (rather than using it to compute the decoder's current hidden state). They also proposed a variant of the dot product mechanism where the encoder outputs first go through a linear transformation (i.e. a time-distributed Dense layer without a bias term) before the dot products are computed. This is called the \"general\" dot product approach. They compared both dot product approaches to the concatenative attention mechanism (adding a rescaling parameter vector **v**), and they observed that the dot product variants performed better than concatenative attention. For this reason, concatenative attention is much less used now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584698d-512a-4de1-adcb-65de9169cf6f",
   "metadata": {},
   "source": [
    "### Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0e0db-4a93-47b3-8583-8014a506e931",
   "metadata": {},
   "source": [
    "**Attention mechanisms are now used for a variety of purposes. One of their applications beyond NMT was in generating image captions using visual attention: a convolutional neural network first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one word at a time. At each decoder time step (each word), the decoder uses the attention model to focus on just the right part of the image.** Attention mechanisms are so powerful that you can actually build state-of-the-art models using only attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249219e-50f8-45f3-86df-c0751be7de83",
   "metadata": {},
   "source": [
    "### Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139fe81-1c0e-4d76-9946-ba8c675df775",
   "metadata": {},
   "source": [
    "One extra benefit of attention mechanisms is that they make it easier to understand what led the model to produce its output. This is called *explainability*. It can be especially useful when the model makes a mistake. In some applications, explainability is not just a tool to debug a model: it can be a legal requirement (think of a system deciding whether or not it should grant you a loan)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb92e3-a8e8-414e-a458-3bd5a6d92d61",
   "metadata": {},
   "source": [
    "### Attention Is All You Need: The Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af5f02-95bf-418f-80f3-9dc807ddca3b",
   "metadata": {},
   "source": [
    "In a groundbreaking 2017 paper a team of Google researchers managed to create an architecture called the *Transformer*, which significantly improved the state of the art in NMT without using any recurrent or convolutional layers, just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces). As an extra bonus, this architecture was also much faster to train and easier to parallelize, so they managed to train it at a fraction of the time and cost of the previous state-of-the-art models. Let's look a bit closer at both of the novel compenents of the Transformer architecture, starting with positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebdeff9-7c99-4881-81ad-5d5fd976c63b",
   "metadata": {},
   "source": [
    "#### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1694f6f-788f-4f3a-9b9e-6a75cca2ea1d",
   "metadata": {},
   "source": [
    "**A positional embedding is a dense vector that encodes the position of a word within the sentence**: the $i^{th}$ positional embedding is simply added to the word embedding of the $i^{th}$ word in the sentence. These positional embeddings can be learned by the model, but in the paper the authors preferred to use fixed positional embeddings, defined using the sine and cosine functions of different frequencies. This solution gives the same performance as learned positional embeddings do, but it can extend to arbitrarily long sentences, which is why it's favored. After the positional embeddings are added to the word embeddings, the rest of the model has access to the absolute position of each word in the sentence because there is a unique positional embedding for each position.\n",
    "\n",
    "Moreover, the choice of oscillating functions makes it possible for the model to learn relative positions as well. For example, words located 38 words apart (e.g. at positions p = 22 and = 60) always have the same positional embedding values in the embedding dimensions. **This explains why we need both the sine and cosine for each frequency: if we only use the sine, the model would not be able to distinguish positions p = 25 and p = 35**.\n",
    "\n",
    "There is no PositionalEmbedding layer in TensorFlow, but it is easy to create one. For efficiency reasons, we precompute the positional embedding matrix in the constructor (so we need to know the maximum sentnece length, max_steps, and the number of dimensions for each word representation, max_dims). Then the call() method crops the embedding matrix to the size of the inputs, and it adds it to the inputs. Since we added an extra first dimension of size 1 when creating the positional embedding matrix, the rules of broadcasting will ensure that hte matrix gets added to every sentence in the inputs. Next we look deeper into the heart of the Transformer model: The Multi-Head Attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a25c24c3-9b5f-499e-aa36-0a51aa521fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class that handles positional embedding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims +=1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs * self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "\n",
    "# Create the first layers of the Transformer\n",
    "embed_size = 512; max_steps = 500; vocab_size = 10_000\n",
    "encoder_inputs = Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = Input(shape=[None], dtype=np.int32)\n",
    "embeddings = Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91274da1-5cb7-46a7-b02b-9b75455c3faa",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed93fa-d943-4245-adaf-dc24935c910e",
   "metadata": {},
   "source": [
    "To understand how a Multi-Head Attention layer works, we must first understand the *Scaled Dot Product Attention* layer, which it is based on. Let's suppose the encoder analyzed the input sentence \"They played chess\" and it managed to understand that the word \"They\" is the subject and the word \"played\" is the verb. The model does not have discrete tokens to represent the keys (like \"subject\" or \"verb\"); it has vectorized representations of these concepts (which it learned during training), so they key it will use for the lookup (called the *query*) will not perfectly match any key in the dictionary. The solution is to compute a similarity measure between the query and each key in the dictionary, then use the softmax function to conver these similarity scores to weights that add up to 1.\n",
    "\n",
    "**In short, you can think of this whole preocess as a differentiable dictionary lookup. The similarity measure used by the Transformer is just the dot product, like in Luong attention.**\n",
    "\n",
    "The keras.layers.Attention layer implements Scaled Dot-Product Attention. If we ignore the skip connections, the layer normalization layers, the Feed Forward blocks, and the fact that this is Scaled Dot-Product attention, not exactly Multi-Head Attention, then the rest of the Transformer model can be implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6937d74-7153-4dc3-8805-9ff3b8110495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = Attention(use_scale=True)([Z, Z])\n",
    "    Z = Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164ce75-1b00-4090-ae33-939b14acff66",
   "metadata": {},
   "source": [
    "The use_scale=True argument creates an additional parameter that lets the layer learn how to properly downscale the similarity scores. This is a bit different from the Transformer model, which always downscales the similarity scores by the same factor $\\sqrt{d_{keys}}$.\n",
    "\n",
    "Now it's time to look at the final piece of the puzzle: **What is a Multi-Head Attention layer? It is just a bunch of Scaled Dot-Product Attention layers, each preceded by a linear transformation of the values, keys, and queries (i.e., a time-distributed Dense layer with no activation function). All the outputs are simply concatenated, and they go through a final linear transformation (again, time-distributed). But why?** What is the intuition behind this architecture? Well, consider the word \"played\" we discussed earlier. The encoder was smart enough to encode the fact that it is a verb. But the word representation also includes its position in the text, thanks to the positional encodings, and it probably includes many other features that are useful for its translation, such as the fact that it is in the past tense. **In short, the word representation encodes many different characteristics of the word. If we just used a single Scaled Dot-Product Attention layer, we would only be able to query all of these characteristics in one shot. This is why the Multi-Head Attention layer applies multiple different linear transformations of the values, keys, and queries: this allows the model to apply many different projections of the word representation into different subspaces, each focusing on a subset of the word's characteristics.** Perhpas one of the linear layers will project the word representation into a subspace where all that remains is the information that the word is a verb, another linear layer will extract just the fact that it is past tense, and so on. Then the Scaled Dot-Product Attention layers implement the lookup phase, and finally we concatenate all the results and project them back to the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf632ba-a6ea-4292-84b3-0111ed615b46",
   "metadata": {},
   "source": [
    "## Recent Innovations in Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5036ca-391d-4ada-9be4-723b827f2fb1",
   "metadata": {},
   "source": [
    "The year 2018 has been called the \"ImageNet moment for NLP\": progress was astounding, with larger nad larger LSTM and Transformer-based architectures trained on immense datatsets. Some of the following papers were the most influential of this period:\n",
    "\n",
    "1. The ELMo paper by Matthew Peters introduced *Embeddings from Language Models* (ELMo): these are contextualized word embeddings. For example, the word \"queen\" will not have the same embedding in \"Queen of the United Kingdom\" and in \"queen bee\". <br><br>\n",
    "2. The ULMFiT paper by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsuperivsed pretraining for NLP tasks: the authors trained an LSTM language model using self-supervised learning (i.e. generating the labels automatically from the data), then they fine-tuned it on various tasks. Their model outperformed the state of the art, reducing the error by 18-24% in most cases. Moreover, they showed that by fine-tuning the pretrained model on just 100 labeled examples, they could achieve the same performance as a model trained from scratch on 10,000 examples.<br><br>\n",
    "3. The GPT paper by Alec Radford and dother OpenAI researchers also demonstrated the effectiveness of unsupervised pretraining using only Masked Multi-Head Attention layers on a large dataset, once again trained using self-supervised learning. Then they fine-tuned it on various language tasks, using only minor adaptations for each task. Just a few months later, in February 2019, Alec Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper, which proposed a very similar architecture but larger still (with over 1.5 billion parameters!) and they showed that it could achieve **good performance on many tasks without any fine-tuning. This is called *zero-shot learning* (ZSL)**<br><br>\n",
    "4. The BERT paper by Jacob Devlin and other Google researchers also demonstrates the effectiveness of self-supervised pretraining on a large corpus, using a similar architecture to GPT but non-masked Multi-Head Attention layers (like the Transformer encoder). Most importantly, the authors proposed two pretraining tasks that explain most of the model's strenght:\n",
    "   1. *Masked Language Model* (MLM):\n",
    "      >If the original sentence is \"She had fun at the birthday party\", then the model may be given the sentence \"She <mask> fun at the <mask> party\" and it must predict the words \"had\" and \"birthday\". Each selected word has an 80% chance of being masked, a 10% chance of being replaced by a random word (to reduce the discrepancy between pretraining and fine-tuning) and a 10% chance of being left alone (to bias the model toward the correct answer).\n",
    "   2. *Next Sentence Prediction* (NSP)\n",
    "      >The model is trained to predict whether two sentences are consecutive or not. This is a challenging task, and it significantly improves the performance of the model when it is fine-tuned on tasks such as question answers or entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2691b3e-2c44-4d6d-9e12-1fbaf770bcff",
   "metadata": {},
   "source": [
    "**As you can see, the main innovations in 2018 and 2019 have been better subword tokenization, shifting from LSTMs to Transformers, and pretraining universal language models using self-supervised learning, the nfine-tuning them with very few architectural changes (or none at all).** Things are moving fast; no one can say what architectures will prevail next year. Today, it's clearly Transformers, but tomorrow it might be CNNs. Or it might even be RNNs, if they make a suprise comeback. In the next chapter we will discuss how to learn deep representations in an unsupervised way using autoencoders, and we will use generative adversarial networks (GANs) to produce images and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94484d6-f120-460c-b319-569dc505aff3",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f956f-3824-4393-9c88-bd9a2e482a1b",
   "metadata": {},
   "source": [
    "<b>1. What are the pros and cons of using a stateful RNN versus a stateless RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc8f947-5811-44a2-859e-84a842dff4b5",
   "metadata": {},
   "source": [
    "My Answer: <br> Stateless are simpler but cannot learn long-term patterns. Stateful are more complex when it comes to batching and sequence lengths but are capable of learning longer term patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e37be-3e47-4f8d-a66a-c2daa6ea4f5e",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2c962-3e44-4321-b372-6125d47f6ea4",
   "metadata": {},
   "source": [
    "<b>2. Why do people use Encoder-Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b27dd-8e70-4589-b27c-4522e2e6c01f",
   "metadata": {},
   "source": [
    "My Answer: <br>Encoder-Decoder is superior because: it handles different lengths naturally, it separates the processing of the source task and the writing to target language task, it avoids fixed-vector bottlenecks, it enables alignment and reordering and training is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbdfc76-b24c-4216-9d1e-1132ed0a216c",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789dcdc-47da-499e-971e-1a879fa3ee64",
   "metadata": {},
   "source": [
    "<b>3. How can you deal with variable-length input sequences? What about variable length output sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495c247-89ba-49a5-bc17-2adb0aea3fff",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be6bd9-034a-4bd3-8930-6e82957d31e7",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d153c25-c1ca-4987-a7f2-27c164d05820",
   "metadata": {},
   "source": [
    "<b>4.What is beam search and why would you use it? What tool can you use to implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e00b8c-a255-4d04-94a4-7bd85b5a3bf5",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57e05d-6c37-45dd-b6f7-b5f71ba51bc5",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597579e-0982-40d0-892c-1dadfc13746a",
   "metadata": {},
   "source": [
    "<b>5. What is an attention mechanism? How does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289695bc-1414-4ed0-9825-f0e6ad739840",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd46541-785f-4090-a8aa-69e031b11e4f",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bab4e9-78a8-4e63-a427-32945230b6aa",
   "metadata": {},
   "source": [
    "<b>6. What is the most important layer in the Transformer architecture? What is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc77ab5-01e5-4367-abd2-e558ba32cb4d",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed517b5d-b1c1-497b-a0de-6e67386e0579",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13d26f-3691-44d7-ba4c-34b3284ccb51",
   "metadata": {},
   "source": [
    "<b>7. When would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236fc4ce-6898-4a24-a395-5618614bea31",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced6c9f-1841-42fe-a09f-5f5073f91909",
   "metadata": {},
   "source": [
    "Book Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fface5f-97f0-4a8e-b170-c0ee601e4b4c",
   "metadata": {},
   "source": [
    "<b>8. Choose a particular embedded Reber grammar, then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar and 50% that don't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3dc9c7-467e-4f40-922f-b9a26f27ae4d",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42c0c0-64cc-45e6-a459-3799fc40beb7",
   "metadata": {},
   "source": [
    "<b>9. Train an Encoder-Decoder model that can convert a date string from one format to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5869256-48e4-48bd-91e8-bc066d735c64",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63043ff-2cc7-4019-b956-fdee1115f2e7",
   "metadata": {},
   "source": [
    "<b>10. Go through TensorFlow's Neural Machine Translation with Attention tutorial (https://homl.info/nmttuto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22dfc36-5a39-4279-9f04-abc893e7d8e9",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31836964-1b3d-48fd-b871-249e3dac825e",
   "metadata": {},
   "source": [
    "<b>11. Use one of the recent language models (e.g. BERT) to generate more convincing Shakespearan text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583c3a5-fad8-4093-9073-6b186ecb9c3f",
   "metadata": {},
   "source": [
    "My Answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OReilly)",
   "language": "python",
   "name": "oreilly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
