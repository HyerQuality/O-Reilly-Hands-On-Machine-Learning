{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fdeda9-c0d0-457c-9f87-d824de1474bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81946393-8dd7-40ae-bd4b-c682fa5ea292",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcd58a-ae82-4a8c-99af-d68a5c0fa87f",
   "metadata": {},
   "source": [
    "Training a deep DNN isn't a walk in the park. Here are some of the problems you could run into: <br>\n",
    "1. You many be faced with the trickey *vanishing gradients* problem or the related *exploding gradients* problem. This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training. Both of these problems make lower layers very hard to train. <br><br>\n",
    "2. You might not have enough training data for such a large network, or it might be too costly to label. <br><br>\n",
    "3. Training may be extremely slow. <br><br>\n",
    "4. A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if the yare too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc532cfb-cbf3-49c7-a4b0-e4315efafcab",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aef62a-9208-4e3c-8f14-e654da4448c3",
   "metadata": {},
   "source": [
    "Unfortunately, during backpropagation, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layers' connection weights virtually unchanged, and the training never converges to a good solution. We call this the *vanishing gradients* problem. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000's, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio. The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e, a normal distribution with a mean of 0 and a standard deviation of 1).\n",
    "\n",
    "In short, they showed that with the activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309fd460-a35c-454d-b410-af7e05630788",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b33a0-2858-46e9-adb0-7e68afc3e5df",
   "metadata": {},
   "source": [
    "In their paper, Glorot and Bengio propose that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don't want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both, but Glorot and Bengio proposed *Xavier initialization or Glorot initialization*\n",
    "\n",
    "Here's an analogy: if you set a microphone amplifier's knob too close to zero, people won't hear your voice, but if you set it too close to the max, your voice will be saturated and people won't understand what you're saying. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude as it came in.\n",
    "\n",
    "Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning. By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting **kernel_initialzier='he_uniform'** or **kernel_initializer='he_normal'** like below.\n",
    "\n",
    "If you want He initialization with a uniform distribution but based on $fan_{avg}$ rather than $fan_{in}$ you can use the VarianceScaling initializer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bda9b3-54cc-40b6-9def-09642233c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initialization</th>\n",
       "      <th>Activation Functions</th>\n",
       "      <th>$\\sigma^{2}$ (Normal)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Glorot</td>\n",
       "      <td>None, tanh, logistic, softmax</td>\n",
       "      <td>$\\frac{1}{fan_{avg}}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He</td>\n",
       "      <td>ReLU and variants</td>\n",
       "      <td>$\\frac{2}{fan_{in}}$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LeCun</td>\n",
       "      <td>SELU</td>\n",
       "      <td>$\\frac{1}{fan_{in}}$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Initialization           Activation Functions  $\\sigma^{2}$ (Normal)\n",
       "0         Glorot  None, tanh, logistic, softmax  $\\frac{1}{fan_{avg}}$\n",
       "1             He              ReLU and variants   $\\frac{2}{fan_{in}}$\n",
       "2          LeCun                           SELU   $\\frac{1}{fan_{in}}$"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_helper = pd.DataFrame(\n",
    "    columns=['Initialization', 'Activation Functions', '$\\sigma^{2}$ (Normal)'],\n",
    "    data=np.array([\n",
    "        ['Glorot', 'None, tanh, logistic, softmax', r'$\\frac{1}{fan_{avg}}$'],\n",
    "        ['He', 'ReLU and variants', r'$\\frac{2}{fan_{in}}$'],\n",
    "        ['LeCun', 'SELU', r'$\\frac{1}{fan_{in}}$']\n",
    "    ])\n",
    ")\n",
    "\n",
    "init_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7b0271-0ef2-4ba2-a968-82c2bb9ca69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# He Normal\n",
    "he_norm = tf.keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')\n",
    "\n",
    "# He using fan avg\n",
    "avg_init = tf.keras.initializers.VarianceScaling(\n",
    "    scale=2,\n",
    "    mode='fan_avg',\n",
    "    distribution='uniform'\n",
    ")\n",
    "he_avg_init = tf.keras.layers.Dense(10, activation='relu', kernel_initializer=avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bb8ac-2e7e-4a5b-a1ec-7e9e2fdec6db",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4acd510-1086-490c-8000-1c849444391c",
   "metadata": {},
   "source": [
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation functions. It turns out that other activations behave much better in deep neural networks-- in particular, the ReLU activation function. \n",
    "\n",
    "Unfortunately, the ReLU activiation function is not perfect. It suffers from a problem known as the *dying ReLUs*: during training, some neurons effectively \"die\", meaning they stop outputting anything other than 0. To solve this problem, you may want to use a variant of the ReLU function such as the *leaky ReLU*. The hyperparameter $\\alpha$ defines how much the function \"leaks\": it is the slope of the function for z < 0 and is typically set to 0.01.\n",
    "\n",
    "A 2015 paper compared several variants of the ReLU activiation function, and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting $\\alpha$ = 0.2 (a huge leak) seemed to result in better performance than $\\alpha$ = 0.01 (a small leak).\n",
    "\n",
    "*Randomized leaky ReLU (RReLU)*, where $\\alpha$ is selected at random, performed fairly well and seemed to act as a regularizer. Finally, the paper evaluated the *parametric leaky ReLU (PReLU)*, where $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). **PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.**\n",
    "\n",
    "Last but not least, **a 2015 paper proposed a new activation function called the *exponential linear unit (ELU)* that outperformed all the ReLU variants** in the authors' experiments: training time was reduced and the neural network performed better on the test set.\n",
    "\n",
    "The ELU activation function looks a lot like the ReLU function, with a few major differences: <br>\n",
    "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter. <br><br>\n",
    "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem. <br><br>\n",
    "3. If $\\alpha$ is equal to 1 then the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent since it does not bounce as much to the left and right of z = 0\n",
    "\n",
    "**The main drawback of the ELU activiation function is that it is slower to compute than the ReLU function and its variants** (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.\n",
    "\n",
    "Then, a 2017 paper introduced the *Scaled ELU (SELU)* activation function: as its named suggests, it is a scaled variant of the ELU activation function. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will *self-normalize*: the output of each layer will tend to preserve a mean of 0 and a standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. There are, however, a few conditions for self-normalization to happen: <br>\n",
    "1. The input features must be standardized (mean 0 and standard deviations 1) <br><br>\n",
    "2. Every hidden layer's weights must be initialized with LeCun normal initialization. In Keras, this means setting kernel_initializer='lecun_normal'<br><br>\n",
    "3. The network's architecture must be sequential. **Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks or networks with skip connections (i.e. connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.**<br><br>\n",
    "4. The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well.\n",
    "\n",
    "***So, which activation should you use for the hidden layers of your deep neural networks?*** Although your mileage will vary, in general **SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic**. \n",
    "\n",
    "If the network's architecture prevents its from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don't want to tweak yet another hyperparameter, you may use the default $\\alpha$ values used by Keras (e.g., 0.3 for leaky ReLU). If you ahve spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set. **That said, because ReLU is the most used activation function (by far), may libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is your priority, ReLU might still be the best choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c481da7-acc1-47d6-a9af-6e05c0b89d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of a model using LeakyReLU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Example of a model using PReLU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.PReLU(),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Examlpe of a model using SELU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, kernel_initializer='lecun_normal', activation='selu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22182b-462f-4e98-b733-0a6e68177a98",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb871a59-eb2a-4856-ae1c-86b27f10af6d",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn't guarantee that they won't come bak during training. *Batch Normalization (BN)* addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. **This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting.**\n",
    "\n",
    "In other words, the operation lets the model learn the optimal scale and mean of each of the layer's inputs. In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g. using StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
    "\n",
    "Unfortunatley, its not that simple. We may need to make predictions for individuals instances rather than for batches of instances. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable. \n",
    "\n",
    "One solution could be to wait until the end of training, then run the whole training set through the neural network and compute the mean and standard deviation of each input of the BN layer. These \"final\" input means and standard deviations could then be used instead of the batch input means and standard deviations when making predictions. **However, most implementations of Batch Normalization estimate these final statistics during training by using a moving average of the layer's input means and standard deviations. This is what Keras does automatically when you use the BatchNormalization layer.**\n",
    "\n",
    "Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all the deep neural networks they experiments with. The vanishing gradients problem was strongly reduced to the point that they could use saturating activation functions such as the tanh and even logistic activation function. The networks were also much less sensitive to the weight initialization. The authors were able to use much larger learning rates as well.\n",
    "\n",
    "Specifically they note that:\n",
    "    * Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result of ImageNet classification: reaching 4.9% top-5 validation error (4.8% test error), exceeding the accuracy of human raters.\n",
    "    \n",
    "Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer, reducing the need for other regularization techniques (such as dropout). \n",
    "\n",
    "Batch Normalization does, however, add some complexity to the model. Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. **Fortunatley, it's often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty.**\n",
    "\n",
    "You may find that training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance. All in all, *wall time* will usually be shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67f034-e592-4cc4-8bf6-6fd6d8965c44",
   "metadata": {},
   "source": [
    "#### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfa7ad-09f4-4551-b144-71d8d0caf496",
   "metadata": {},
   "source": [
    "As with most things with Keras, implementing Batch Normalization is simple and intuitive. Just add a BatchNormalization layer before or after each hidden layer's activation function, and optionally add a BN layer as well as the first layer in your model. The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after. You can experiment with this too to see which option works best on your dataset. To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Moreover, since a Batch Normalization layer includes one offset parameter per input, you can remove the bias term from the previous layer (just pass use_bias=False when creating it).\n",
    "\n",
    "The BatchNormalization class has quite a few hyperparameters you can tweak. The defaults will usually be fine, but you may occasionally need to tweak the momentum. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving averages; given a new value **v**. A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datsets and smaller mini-batches). Another important hyperparameter is axis. It defaults to -1, meaning that by default it will normalize the last axis. When the input batch is 2D (i.e., the batch shape is [batch size, features]), this means that each input feature will be normalized based on the mean and standard deviation computer across all the instances in the batch. If we move the first BN layer before the Flatten layer, then the input batches will be #D, with shape [batch size, height, width]; therefore, the BN layer will computer 28 means and 28 standard deviations (1 per column of pixels, computed across all instances in the batch and across all rows in the column), and it will normalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 sclare parameters and 28 shift parameters. If instead you still want to treat each of the 784 pixels independently, then you should set axis=[1, 2].\n",
    "\n",
    "BatchNormalization has become one of the most-used layers in deep neural networks, to the point that it is often omitted in the diagrams, as it is assumed that BN is added after every layer. But a recent paper may change this assumption: by using a novel *fixed-update* weight initialization technique. The authors managed to traing a very deep neural network (10,000 layers!) without BN, achieving state-of-the-art performance on complex image classification tasks. As this is bleeding-edge research, however, you may want to wait for additional research to confirm this finding before you drop Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3443829-1875-43f6-990c-14f8beed746b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_normalization/gamma:0', True), ('batch_normalization/beta:0', True), ('batch_normalization/moving_mean:0', False), ('batch_normalization/moving_variance:0', False)]\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 784)               3136      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271346 (1.04 MB)\n",
      "Trainable params: 268978 (1.03 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example applying BN after activation functions\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print([(var.name, var.trainable) for var in model.layers[1].variables])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d1eeab-edac-4030-9064-7606d840e7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_normalization_3/gamma:0', True), ('batch_normalization_3/beta:0', True), ('batch_normalization_3/moving_mean:0', False), ('batch_normalization_3/moving_variance:0', False)]\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 784)               3136      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 300)               1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               30000     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270946 (1.03 MB)\n",
      "Trainable params: 268578 (1.02 MB)\n",
      "Non-trainable params: 2368 (9.25 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example applying BN before activation functions\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('elu'),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('elu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print([(var.name, var.trainable) for var in model.layers[1].variables])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b87a45-fbb1-496d-8f12-1879f6376002",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940ba62-0ba6-4b36-b402-e444726df2ed",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. **This is called *Gradient Clipping*. This technique is most often used in recurrent neural networks**, as Batch Normalization is tricky to use in RNNs. For other types of networks, BN is usually sufficient.\n",
    "\n",
    "An optimizer with a clipvalue=1.0 will clip every component of the gradient vector to a value between -1.0 and 1.0. For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. In practice, this approach works well. If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. This will clip the whole gradient if its $l_{2}$ norm is greater than the threshold you picked.\n",
    "\n",
    "For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component. You can track the size of the gradients using TensorBoard.\n",
    "\n",
    "You may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4278ed13-541b-4e8a-b238-ef5bcc55f5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b3df1-3107-4720-9173-e3a67edd6c7e",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f7377-43d3-4095-939e-b0173dd87a5f",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one your are trying to tackle (more on this in Chapter 14) then reuse the lower layers of this network. This technique is called *transfer learning*. It will not only speed up training considerably, but also require significantly less training data.\n",
    "\n",
    "If the input pictures of your new task don't have the same size as the ones used in the original task, you will usually have to add a preprocessing step to resize them to the size expected by the original model. More generallyl, transfer learning will work best when the inputs have similar low-level features.\n",
    "\n",
    "The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "\n",
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers. The more simlar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer.\n",
    "\n",
    "Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won't modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. **The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.**\n",
    "\n",
    "If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563b506-d68b-42bb-b05c-289a38b0c59e",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97d9fe-aa2f-4924-b6b1-74a1ac55b2d9",
   "metadata": {},
   "source": [
    "Suppose someone built and trained a Keras model on that set and got reasonably good performance. Let's call this model A. You now want to tackle a different task. Your dataset is quite small; only 200 labeled images. When you train a new model for this task (call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it's a much easier task, you were hoping for me. So perhaps transfer learning can help. \n",
    "\n",
    "First, you need to load model A and create a new model based on that model's layers. When you train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to *clone* model_A before you reuse its layers. To do this, you clone model A's architecture with clone_model(), then copy its weights (since clone_model() does not clone the weights).\n",
    "\n",
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors, so there will be large error gradients that may wreck the reused weights. To avoid this, once approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer's trainable attribute to False and compile the model.\n",
    "\n",
    "**Note that you must always compile your model after you freeze or unfreeze layers.**\n",
    "\n",
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights.\n",
    "\n",
    "A good rule of thumb is when a paper looks too positive, you should be suspicious: perhaps the flashy new technique does not actually help much but the authors tried many variants and reported only the best results without mentioning how many failures they encountered on the way. In the example below, the author suggests the test accuracy is 99.25%! However, it turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks.\n",
    "\n",
    "In this case, the author specifically worked to find a set of conditions that led to model_A_on_B working extremely well. However, transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers). This concept will be revisited in chapter 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e90da8-2042-4bad-b21c-06c426c72504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example of transfer learning. Commented out because model A doesnt exist'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Example of transfer learning. Commented out because model A doesnt exist'''\n",
    "# model_A = tf.keras.load_model('my_model_A.h5')\n",
    "# model_A_clone = tf.keras.models.clone_model(model_A)\n",
    "# model_A_clone.set_weights(model_A.get_weights())\n",
    "# model_B_on_A = tf.keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "# model_B_on_A.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#     layer.trainable = False\n",
    "    \n",
    "# model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# history = model_B_on_A.fit(\n",
    "#     X_train_B,\n",
    "#     y_train_B,\n",
    "#     epochs=4,\n",
    "#     validation_data=(X_valid_B, y_valid_B)\n",
    "# )\n",
    "\n",
    "# for layer in model_B_on_A.layers[:-1]:\n",
    "#     layer.trainable = True\n",
    "    \n",
    "# optimizer = tf.keras.SGD(lr=1e-4)\n",
    "# model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# history = model_B_on_A.fit(\n",
    "#     X_train_B,\n",
    "#     y_train_B,\n",
    "#     epochs=16,\n",
    "#     validation_data=(X_valid_B, y_valid_B)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934961b-c958-4287-8166-2202c641218b",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561585f0-b29c-4dae-a3d8-88ff7f8fa897",
   "metadata": {},
   "source": [
    "Suppose you want to tackle a complex task for which you don't have much labeled training data, but unfortunately you cannot find a model trained on a similar task. You may still be able to perform *unsupervised pretraining*. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network. Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN's discriminator, add the output layer for your task on top, and fine-tune the final network using supervised learning.\n",
    "\n",
    "Unsupervised pretraining (today typically using autoencoders or GAN's rather than RBMs) is still a good option when you have a complex task to solve, no similar model you can reuse, and little labeled training data but plenty of unlabeled training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e37512-e6ce-40ac-916a-a265cb9f81a0",
   "metadata": {},
   "source": [
    "### Pretraining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0f696-f7e3-4f49-bdbf-c9a711eb6f7d",
   "metadata": {},
   "source": [
    "If you do not have much labeled training data, one last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network's lower layers will learn feature detectors that will likely be reusable by the second neural network.\n",
    "\n",
    "For example, if you want to build a system to recognize faces, you may only have a few pictures of each individual-- clearly not enough to train a good classifier. Gathering hundreds of pictures of each person would not be practical. You could, however, gather a lot of pictures of random people on the web and train a first neural network to detect whether or not two different pictures feature the same person. Such a network would learn good feature detectors for faces, so reusing its lower layers would allow you to train a good face classifier that uses little training data.\n",
    "\n",
    "For *natural language processing (NLP)* applications, you can download a corpus of millions of text documents and automatically generate labeled data from it. For example, you could randomly mask out some words and train a model to predict what the missing words are (e.g., it should predict that the missing word in the sentence \"What __ you saying?\" is probaby \"are\" or \"were\"). If you an train a model to reach a good performance on this task, then it will already know quite a lot about language, and you can certainly reuse it for your actual task and fine-tune it on your labeled data.\n",
    "\n",
    "*Self-supervised learning* is when you automatically generate the labels from the data itself, then you train a model on the resulting \"labeled\" dataset using supervised learning techniques. Since this approach requries no human labeled whatsoever, it is best classified as a form of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c08d43-ab8a-4bd2-afb9-8094c92d82ea",
   "metadata": {},
   "source": [
    "### Fast Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c091f-0728-4550-bb9c-49343449ad98",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training: <br>\n",
    "1. Applying a good initialization strategy for the connection weights.\n",
    "2. Using a good activation function.\n",
    "3. Using Batch Normalization\n",
    "4. Reusing parts of a pretrained network.\n",
    "    \n",
    "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068c53d-9bc4-41a0-a01c-cf84e5095b45",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad380e-fe4f-4ee1-8961-f6cdb8c62359",
   "metadata": {},
   "source": [
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the *momentum vector* **m** (multiplied by the learning rate $\\eta$), and it updates the weights by adding the momentum vector. In other words, the gradient is used for acceleration, not for speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9. This allows momentum optimization to escape from plateaus much faster than Gradient Descent.\n",
    "\n",
    "Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons it's good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.\n",
    "\n",
    "**The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee458e7-c927-4476-a936-902fe684c812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding momentum to the SGD optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b45c2-93b9-4ca3-83c7-3e45acd8909e",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556c246-b861-45dc-bd23-bf6651885286",
   "metadata": {},
   "source": [
    "The *Nezterov Accelerated Gradient (NAG)* method, also known as *Nesterov momentum optimization*, measures the gradient of the cost function not at the local position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta$ + $\\beta$**m**\n",
    "\n",
    "This small tweak works because in general the momentum vector will be pointing in the right direction, so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position. NAG is generally faster than regular momentum optimization. To use it, simply set nesterov=True when creating the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307d24f9-48ee-4a64-adc7-e315c3f7ebea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of Nezterov momentum\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4f337-a00e-4623-8355-fa6c3dba277b",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c5da8-d909-46ad-9114-1bc6d7b059f6",
   "metadata": {},
   "source": [
    "It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimium. The *AdaGrad* algorithm achieves this correction by scaling down the gradient vector along the steepest dimensions. In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an *adaptive learning rate.* One additional benefit is that it requires much less tuning of the learning rate hyperparameter $\\eta$.\n",
    "\n",
    "**AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6925dd-3441-4960-b1e9-449d5b852cc2",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05615fa7-349b-4c36-a627-dbcc1f3a9546",
   "metadata": {},
   "source": [
    "AdaGrad runs the risk of slowing down a bit too fast and never converging. The *RMSProp* algorithm fixes this by accumulating only gradients from the most iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay. It was the preferred optimization algorithm of many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064f5aa6-a1d6-44fa-a3d6-6f2c231f94b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example using RMSProp\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5e824-6b15-4c73-b7c5-9d5cfe3908cc",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0645c1-6949-4623-81fa-a110f9da6747",
   "metadata": {},
   "source": [
    "*Adam*, which stands for *adaptive moment estimation*, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps trach of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "**The momentum decay hyperparamter $\\beta_{1}$ is typically initalized to 0.9, while the scaling decay hyperparameter $\\beta_{2}$ is often initalized to 0.999.**\n",
    "\n",
    "Since Adam is an adaptive learning rate algorithm, it requires less tuning of the learning rate hyperparameter $\\eta$. You can often use the default value $\\eta$ = 0.001, making Adam even easier to use than Gradient Descent. \n",
    "\n",
    "Finally, two variants of Adam are worth mentioning:\n",
    "1. *AdaMax* <br>\n",
    "    Notice that in step 2 of Equation 11-8 on page 356, Adam accumulates the squares of the gradients in **s** (with greater weight for more recent weights). In step 5, if we ignore $\\epsilon$ and stesp 3 and 4 (which are technical details anyway), Adam scales down the parameter updates by the square root of **s**. In short, Adam scales down the parameter updates by the $l_{2}$ norm of the time-decayed gradients (recall that hte $l_{2}$ norm is the square root of the sum of squares). AdaMax, introduced in the same paper as Adam, replaces the $l_{2}$ norm with the $l_{\\infty}$ norm (a fancy way of saying the max). Specifically, it replaces step 2 in Equation 11-8 with the max of the time-decayed gradients. In practice, this can make AdaMax more stable than Adam, but it really depends on the dataset and in general Adam performs better. But AdaMax can be tried if there are problems with Adam <br>\n",
    "2. *Nadam* <br>\n",
    "    Nadam optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam. In his report introducing this technique, the researcher Timothy Dozat compares many different optimizers on various tasks and finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp.\n",
    "    \n",
    "**Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model's performance, try using play Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also, check out the latest research, because it's moving fast.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f567ed-47d7-4150-9a75-b2c626b51813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example implementing ADAM\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c902135c-eb17-44a5-a3f8-99bd4bf8788d",
   "metadata": {},
   "source": [
    "### Training Sparse Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a786397e-6333-403d-9a2b-54470b8d74af",
   "metadata": {},
   "source": [
    "All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead. To do so, apply a strong $l_{1}$ regularization during training as it pushes the optimizer to zero out as many weights as it can. If this is insufficient, check out the TensorFlow Model Optimization Toolkit (TF-MOT), which provides a pruning API capable of iteratively removing connections during training based on their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f8a9a2-9d62-4455-bb3d-f0aaa7070b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Convergence Speed</th>\n",
       "      <th>Convergence Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD(momentum=...)</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD(momentum=..., nesterov=True)</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>1 (stops too early)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RMSProp</td>\n",
       "      <td>3</td>\n",
       "      <td>2 or 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adam</td>\n",
       "      <td>3</td>\n",
       "      <td>2 or 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nadam</td>\n",
       "      <td>3</td>\n",
       "      <td>2 or 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaMax</td>\n",
       "      <td>3</td>\n",
       "      <td>2 or 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Class Convergence Speed  Convergence Quality\n",
       "0                               SGD                 1                    3\n",
       "1                 SGD(momentum=...)                 2                    3\n",
       "2  SGD(momentum=..., nesterov=True)                 2                    3\n",
       "3                           Adagrad                 3  1 (stops too early)\n",
       "4                           RMSProp                 3               2 or 3\n",
       "5                              Adam                 3               2 or 3\n",
       "6                             Nadam                 3               2 or 3\n",
       "7                            AdaMax                 3               2 or 3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of optimizer discussed. (1 is bad, 2 is average, 3 is good)\n",
    "summary = pd.DataFrame(\n",
    "    columns=['Class', 'Convergence Speed', 'Convergence Quality'],\n",
    "    data=np.array([\n",
    "        ['SGD', 1, 3],\n",
    "        ['SGD(momentum=...)', 2, 3],\n",
    "        ['SGD(momentum=..., nesterov=True)', 2, 3],\n",
    "        ['Adagrad', 3, '1 (stops too early)'],\n",
    "        ['RMSProp', 3, '2 or 3'],\n",
    "        ['Adam', 3, '2 or 3'],\n",
    "        ['Nadam', 3, '2 or 3'],\n",
    "        ['AdaMax', 3, '2 or 3'],\n",
    "    ])\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f596a988-b59a-497b-9361-0c6072b90c71",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f4f4c-35b2-4820-817f-5937561c7af9",
   "metadata": {},
   "source": [
    "If you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. It can also be beneficial to start with a low learning rate, increase it, then drop it again. These strategies are called *learning schedules*. **These are the most commonly used learning schedules:**\n",
    "\n",
    "1. ***Power scheduling***<br>\n",
    "    Set the learning rate to a function of the iteration number *t*: $\\large{\\eta}$(t) = $\\large{\\frac{\\eta_{0}}{(1 + \\frac{t}{s})^{c}}}$. The initial learning rate $\\eta_{0}$, the power *c* (typically set to 1), and the steps *s* are hyperparameters. The learning rate drops at each step. **After *s* steps, it is down to $\\large{\\frac{\\eta_{0}}{2}}$. After *s* more steps, it is down to $\\large{\\frac{\\eta_{0}}{3}}$, and so on.** As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning $\\eta_{0}$ and *s* (and possibly c).<br><br>\n",
    "2. ***Exponential scheduling***<br>\n",
    "    Set the learning rate to $\\large\\eta$(t) = $\\large\\eta_{0}0.1^{\\frac{t}{s}}$. The learning rate will gradually drop by a factor of 10 every *s* steps. **While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every *s* steps.**<br><br>\n",
    "3. ***Piecewise constant scheduling***<br>\n",
    "    Use a constant learning rate for a number of epochs then a smaller learning rate for another number of epochs, and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them.<br><br>\n",
    "4. ***Performance scheduling***<br>\n",
    "    Measure the validation error every *N* steps (just like for early stopping), and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.<br><br>\n",
    "5. ***1cycle scheduling***<br>\n",
    "    Contrary to the other approaches, *1cycle* starts by increasing the initial learning rate $\\eta_{0}$, growing linearly up to $\\eta_{1}$ halfway through training. Then it decreases the learning rate linearly down to $\\eta_{0}$ again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The maximum learning rate $\\eta_{1}$ is chosen using the same appraoch we used to find the optimal learning rate, and the initial learning rate $\\eta_{0}$ is chosen to be roughly 10 times lower. When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85 linearly), and then bring it back up to the maximum value (0.95) during the second half of training, finishing the last few epochs with that maximum value. **This approach was shown by the publishing author to be faster and reach better performance than other scheduling techniques.** For example, on the popular CIFAR10 image dataset, this approach reached 91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800 epochs through a standard approach.\n",
    "    \n",
    "When using momentum optimization to train deep neural networks for speech recognition both performance scheduling and exponential scheduling performed well, with exponential scheduling being preferred for being easier to tune and slightly faster. 1cycle still performed better than both, but is more involved to implement as it requires a custom callback class.\n",
    "\n",
    "The LearningRateScheduler will update the optimizer's learning_rate attribute at the beginning of each epoch. Updating the learning rate once per epoch is usually enough, but if you want it to be updated more often, for example at every step, you can always write your own callback. Updating the learning rate at every step makes sense if there are many steps per epoch. Alternatively, you can use the tf.keras.optimizers.schedules approach.\n",
    "\n",
    "The schedule function can optionally take the current learning rate as a second argument, which means the decay now starts at the beginning of epoch 0, instead of epoch 1. This implementation relies on the optimizer's initial learning rate, so make sure to set it properly.\n",
    "\n",
    "When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off, not problem. Things are not so simple if your schedule function uses the epoch argument, however: the epoch does not get saved and it gets reset to 0 every time you call the fit() method.\n",
    "\n",
    "For performance scheduling, use the ReduceLROnPlateau callback.\n",
    "\n",
    "Laslty, tf.keras offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in tf.keras.optimizers.schedules, the pass this learning rate to any optimizer. This approach updates the learning rate at each step rather than at each epoch. This is nice and simple, plus when you save the model, the learning rate and its schedule (including its state) get saved as well. This approach, however, is not part of the Keras API; it is specific to tf.keras.\n",
    "\n",
    "As for the 1cycle approach, the implementation poses no particular difficulty: just create a custom callback that modifies the learning rate at each iteration.\n",
    "\n",
    "**To sum up, exponential decay, performance scheduling, and 1cylce can considerably speed up convergence!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5639426f-b252-4844-81f6-f6d9c16269ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "decay is deprecated in the new Keras optimizer, please check the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example using power scheduling\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# decay is the inverse of s, c is assumed to be equal to 1\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Example using exponential scheduling with hardcoded eta not and s\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexponential_decay_fn\u001b[39m(epoch):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OReily-Machine-Learning\\lib\\site-packages\\keras\\src\\optimizers\\sgd.py:114\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, learning_rate, momentum, nesterov, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, name, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    113\u001b[0m ):\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjit_compile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_learning_rate(learning_rate)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m=\u001b[39m momentum\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OReily-Machine-Learning\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:1094\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m mesh \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmesh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;241m=\u001b[39m mesh\n\u001b[1;32m-> 1094\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjit_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor \u001b[38;5;241m=\u001b[39m dtensor_utils\u001b[38;5;241m.\u001b[39mrunning_with_dtensor_strategy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OReily-Machine-Learning\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:106\u001b[0m, in \u001b[0;36m_BaseOptimizer.__init__\u001b[1;34m(self, name, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, jit_compile, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iteration_variable()\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\OReily-Machine-Learning\\lib\\site-packages\\keras\\src\\optimizers\\optimizer.py:135\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m legacy_kwargs:\n\u001b[1;32m--> 135\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is deprecated in the new Keras optimizer, please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck the docstring for valid arguments, or use the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy optimizer, e.g., \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.keras.optimizers.legacy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid argument, kwargs should be empty \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: decay is deprecated in the new Keras optimizer, please check the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD."
     ]
    }
   ],
   "source": [
    "# Example using power scheduling\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, weight_decay=1e-4) # decay is the inverse of s, c is assumed to be equal to 1\n",
    "\n",
    "# Example using exponential scheduling with hardcoded eta not and s\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler]) 'commented out because no data is loaded or prepped'\n",
    "\n",
    "# Example using exponential scheduling with dynamic eta and s\n",
    "def exponential_decay(lr, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr=0.01, s=20)\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler]) 'commented out because no data is loaded or prepped'\n",
    "\n",
    "# Example using piecewise constant scheduling\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif 5 <= epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "        \n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler]) 'commented out because no data is loaded or prepped'\n",
    "\n",
    "# Example using performance scheduling\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "# history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler]) 'commented out because no data is loaded or prepped'\n",
    "\n",
    "# Example using tf.keras.optimizers.schedules\n",
    "# s = 20 * len(X_train) // 32 ' number of steps in 20 epochs (batch size = 32)' 'commented out because no data is loaded or prepped'\n",
    "# learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1) 'commented out because no data is loaded or prepped'\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate) 'commented out because no data is loaded or prepped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55576e5-fed3-4e3b-9535-8dd88b43508d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example to find optimum eta max\n",
    "K = tf.keras.backend\n",
    "\n",
    "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(\n",
    "        X, \n",
    "        y, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        callbacks=[exp_lr]\n",
    "    )\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f24227-1403-47fb-ae03-c3301f27dfd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "' From the chapters provided notebook'\n",
    "\n",
    "# Example implementing 1cycle\n",
    "K = tf.keras.backend\n",
    "\n",
    "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)\n",
    "\n",
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid_scaled, y_valid),\n",
    "    callbacks=[onecycle]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7138e9-7447-40e6-aa15-ff9c062d5cdd",
   "metadata": {},
   "source": [
    "### Avoid Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3c591-d4fd-4244-9d37-42a4ba246409",
   "metadata": {},
   "source": [
    "With thousands of parameters deep neural networks are prone to overfitting the training set. We need regularization. One of the best regularization techniques is early stopping. Moreover, Batch Normalization acts like a good regularizer too. Other popular techniques include $l_{1}$ and $l_{2}$ regularization, dropout, and max-norm regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59617f6d-c69a-4051-844a-5b663a9fcf50",
   "metadata": {},
   "source": [
    "### $l_{1}$ and $l_{2}$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa9dd5-f475-4a15-a14c-5e1ad16c8983",
   "metadata": {},
   "source": [
    "You can use $l_{2}$ regularization to constrain a neural network's connection weights, and/or $l_{1}$ regularization if you want a sparse model (with many weights equal to 0). The l2() function returns a regularizer that will b e called at each step during training to compute the regularization loss. This is then added to the final loss. As you might expect, you can just use tf.keras.regularizers.l1() if you $l_{1}$ regularization; if you want both $l_{1}$ and $l_{2}$ regularization, use tf.keras.regularizers.l1_l2().\n",
    "\n",
    "**Since you will typically want to apply the same regularizer to all layers in your network you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is to use Python's functools.partial() function, which lets you create a thin wrapper for any callable, with some default argument values, as shown below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e93d1a-613b-41ad-be51-101a2c3b7b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example applying l2 regularization\n",
    "layer = tf.keras.layers.Dense(\n",
    "    100,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "# Example using functools to refactor repetitive code\n",
    "from functools import partial\n",
    "\n",
    "RegularizeDense = partial(\n",
    "    tf.keras.layers.Dense,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizeDense(300),\n",
    "    RegularizeDense(100),\n",
    "    RegularizeDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc891e9f-af65-421f-bd9c-c747d15ea9ad",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cb40b-56ea-4c3b-b5c7-71aaa7533c19",
   "metadata": {},
   "source": [
    "*Dropout* is one of the most popular regularization techniques for deep neural networks. Even state-of-the-art neural networks get a 1-2% accuracy boost simply by adding dropout. It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability *p* of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step. **The hyperparameter *p* is called the *dropout rate*, and it is typically set between 10% and 50%: closer to 20-30% in recurrent neural nets, and closer to 40-50% in convolutional neural networks.** After training, neurons don't get dropped anymore.\n",
    "\n",
    "Think of it like this: \n",
    "\n",
    "Would a company perform better if its employees were told to toss a coin every morning to decide whether or not to go to work? Well, who knows, perhaps it would! The company would be forced to adapt its organization; it could not rely on any single person to work the coffee machine or perform any other critical tasks, so this expertise would have to be spread across several people. Employees would have to learn to cooperate with many of their coworkers, not just a handful of them. This company would become much more resilient. It's unclear if whether this idea would actually work for companies, but it certainly does for neural networks. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Once you have run 10,000 training steps, yo uhave essentially trained 10,000 different neural networks. The resulting neural network can be seen as an averaging ensemble of all these smaller networks. In general, you can usually apply dropout only to the neurons in the top 1-3 layers, excluding the output layer, of course.\n",
    "\n",
    "There is one small but important technical detail: we need to multiply each input connection weight by the *keep probability* (1 - p) after training. Alternatively, we can divide each neuron's output by the keep probability during training. This is a consequence of the fact that during testing, a neuron that was not dropped out will be connected to more inputs than if all neurons were present. This artifically increases the input weight assigned to that neuron.\n",
    "\n",
    "To implement dropout using Keras, you can use the keras.layers.Dropout layer. During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability. After training, it does nothing at all; it just passes the inputs to the next layer.\n",
    "\n",
    "**Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have simliar training and validation losses. So make sure you evaluate the training loss without dropout (e.g., after training).**\n",
    "\n",
    "**If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate for large layers, and reduce it for small ones. Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.**\n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.\n",
    "\n",
    "If you want to regularize a self-normalizing network based on the SELU activation function, you should use *alpha dropout*: this is a variant of dropout that preserves the mean and standard deviation of its inputs.\n",
    "\n",
    "Yarin Gal and Zoubin Ghaharamani published a paper in 2016 adding a few more good reasons to use dropout:\n",
    "1. First, the paper established a profound connection between dropout networks and approximate Bayesian inference, giving dropout a solid mathematical justification.\n",
    "2. Second, the authors introduced a powerful technique called *Monte Carlo Dropout*, which can boost the performance of any trained dropout model without having to retrain it or even modify it at all, provides a much better measure of the model's uncertainty, and is also amazingly simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2920d74-8b1f-44e4-8fd9-4800bf3598a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example implmenting dropout\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input([28, 28]),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab3b4f-23b1-4061-b791-03ba5e6ff3c0",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f1c51-078d-423a-a115-8dd204f18811",
   "metadata": {},
   "source": [
    "The full implementation is remarkable simple, as seen below, and boosts any dropout model without retraining it. In this example, we just make 100 predictions over the test set, setting training=True to ensure that the Dropout layer is active, and stack the predictions. Since dropout is active, all the predictions will be different. \n",
    "\n",
    "That all! Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a single prediction with dropout off. \n",
    "\n",
    "**The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. The higher it is, the more accurate the predictions and their uncertainty estimates will be. However, if you double it, inference time will also be doubled. Moreover, above a certain number of samples, you will notice little improvement. So your job is finding the right trade-off between latency and accuracy, depending on your application.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d471765-4339-43d5-b9f2-0fa20ce28df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing MC Dropout\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7569d2-ae31-40c4-b800-4543e16ca795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exploring MC Dropout results\n",
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15075e99-de3e-4c6e-92bf-7b831be2545b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.round(y_probas[:10, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25350863-2b74-4b16-97df-ab2d0e236d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2), np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88640413-2e61-4108-a988-3feda4599bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)\n",
    "\n",
    "''' \n",
    "Apparently there is quite a lot of variance in the probability estimates: if you were building a risk-senstive system (e.g. a medical or financial system),\n",
    "you should probably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a 99% confident prediction.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96537f41-afd8-4183-8488-f0165465b68f",
   "metadata": {},
   "source": [
    "**If your model contains other layers that behave in a special way during training (such as BatchNormalization layers), then you should not force traiing mode like we just did. Instead, you should replace the Dropout layers with the following MCDropout class.**\n",
    "\n",
    "Here, we just subclass the Dropout layer and override the call() method to force its training argument to True. Similarly, you could define an MCAlpha Dropout class by subclassing AlphaDropout instead. If you are creating am odel from scratch, it's just a matter of using MCDropout rather than Dropout. But if you have a model that was already trained using Dropout, you need to create a new model that's identical to the existing model except that it replaces the Dropout layers with MCDropout, then copy the existing model's weights to your new model.\n",
    "\n",
    "**In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during trianing, it also acts like a regularizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6785a-a396-46e9-9b56-2befcd697748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d21532-3cb4-49b3-90c0-ed2cd7dd8cea",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37eb529-5cc3-4061-b026-e0b57fc93ed9",
   "metadata": {},
   "source": [
    "Another regularization technique that is popular for neural networks is called *max-norm regularization*: for each neuron, it constrains the weights **w** of the incoming connections succh that $||w||_{2}$ $\\le$ r, where *r* is the max-norm hyperparameter and $||\\cdot||_{2}$ is the $l_{2}$ norm.\n",
    "\n",
    "**Reducing *r* increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the unstable gradients problems if you aren't using Batch Normalization**.\n",
    "\n",
    "To implement max-norm regulariation in Keras, set the kernel_constraint argument of each hidden layer to a max_norm() constraint with the appropriate max value, as shown below.\n",
    "\n",
    "**The max_norm() function has an axis argument that defaults to 0. A Dense layer usually has weights of shape [number of inputs, numer of neurons], so using axis=0 means that the max-norm constraint will apply independently to each neuron's weight vector. If you want to use max-norm with convolutional layers, make sure to set the max_norm() constraint's axis argument appropriately (usually axis=[0, 1, 2])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f481c8d-4c74-40fd-9157-72978ec7d8c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing max-norm regularization\n",
    "tf.keras.layers.Dense(\n",
    "    100,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(1.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828b248-91d1-4819-b16c-ca4f08d91616",
   "metadata": {},
   "source": [
    "### Summary and Practical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807843b-ce50-43d2-b62b-d5555d0a9538",
   "metadata": {},
   "source": [
    "The configuration in the first DataFrame work fine in most cases, however do not consider these defaults as hard rules! The second is a good default for a simple stack of desne layers. Don't forget to normalize the input features! You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you ahve a lot of labeled data for a similar task. \n",
    "\n",
    "While the previous guidelines should cover most cases, here are some exceptions:\n",
    "1. If you need a sparse model, you can use $l_{1}$ regularization (and optionally zero out the tiny weights after training). If you need an even sparser model, you can use the TensorFlow Model Optimization Toolkit. This will break self-normalization, so you should use the default configuration in this case.\n",
    "2. If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers in the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU. Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits. Again, check out TF-MOT\n",
    "3. If you are building a risk-sensitive application, or inference latency is not very important in your application, you can use MC Dropout to boost performance and get more reliable probability estimates, along with uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8870ef-a02b-4fa2-9e24-48cb6e09bea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default DNN configuration\n",
    "dnn_defaults = pd.DataFrame(\n",
    "    columns=['Hyperparameter', 'Default Value'],\n",
    "    data=np.array([\n",
    "        ['Kernel initializer', 'He initialization'],\n",
    "        ['Activation function', 'ELU'],\n",
    "        ['Normalization', 'None if shallow; Batch Norm if deep'],\n",
    "        ['Regularization', 'Early stopping (+ $l_{2}$ if needed)'],\n",
    "        ['Optimizer', 'Momentum optimization (or RMSProp or Nadam)'],\n",
    "        ['Learning rate cycle', '1cycle']\n",
    "    ])\n",
    ")\n",
    "\n",
    "dnn_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ad123-29fb-40cc-bc17-0ba808a97d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_norm_defaults = pd.DataFrame(\n",
    "    columns=['Hyperparameter', 'Default Value'],\n",
    "    data=np.array([\n",
    "        ['Kernel initializer', 'LeCun initialization'],\n",
    "        ['Activation function', 'SELU'],\n",
    "        ['Normalization', 'None'],\n",
    "        ['Regularization', 'Alpha dropout if needed)'],\n",
    "        ['Optimizer', 'Momentum optimization (or RMSProp or Nadam)'],\n",
    "        ['Learning rate cycle', '1cycle']\n",
    "    ])\n",
    ")\n",
    "\n",
    "self_norm_defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3f4eb-ea5e-49d4-a67f-a822cec20330",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e5df1-b095-434a-8a05-57cc935f37ff",
   "metadata": {},
   "source": [
    "**1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9eceee-fbe8-43c2-bc4a-fc3c87b0fdad",
   "metadata": {},
   "source": [
    "*My answer*\n",
    "\n",
    "Regardless of how the weights are assigned, they should never be assigned to the same value. If they are all the same value it makes backpropagation nearly impossible to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b69b32-fd2d-46d7-9af8-6bddac87076e",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12654a72-ea8f-4218-8cdd-ae9ef71dc922",
   "metadata": {},
   "source": [
    "**2. Is it OK to initialize the bias terms to 0?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d35f0d-d10e-4712-bdd6-3256523914a5",
   "metadata": {},
   "source": [
    "*My answer*\n",
    "\n",
    "Yes, biases can be initialized at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1431289-aa51-4236-a154-b1568e69fc02",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad0afb-dcdc-4ba9-8600-8135f37d4418",
   "metadata": {},
   "source": [
    "**3. Name three advantages of the SELU activation function over ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e70ad-a9e2-4d15-b845-e79864962ae6",
   "metadata": {},
   "source": [
    "*My answer*\n",
    "\n",
    "1. SELU self-normalizes\n",
    "2. It can process negative values\n",
    "3. Because it is a smooth function it can speed up gradient desecent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f36934-f913-4db3-93cb-d9b344e1c1d3",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a05bae-c072-48f4-bff2-9a846461377f",
   "metadata": {},
   "source": [
    "**4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09976990-0656-4c5d-b90f-2802045bc816",
   "metadata": {},
   "source": [
    "*My answer*\n",
    "\n",
    "SELU: You have a sequential model of Dense layers. Less valuable for CNNs, RNNs and Wide & Deep models, but can still provide value.\n",
    "<br>\n",
    "Leaky ReLU: \n",
    "<br>\n",
    "ReLU: Speed and simplicity are valued to a greater extent than maximizing performance.\n",
    "<br>\n",
    "Tanh:\n",
    "<br>\n",
    "Logistic:\n",
    "<br>\n",
    "Softmax:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ed5c3-2d8f-4c26-82e4-4d39dd0260e7",
   "metadata": {},
   "source": [
    "*Book answer*\n",
    "\n",
    "SELU: \n",
    "<br>\n",
    "Leaky ReLU: \n",
    "<br>\n",
    "ReLU: \n",
    "<br>\n",
    "Tanh:\n",
    "<br>\n",
    "Logistic:\n",
    "<br>\n",
    "Softmax:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f7df3-7df3-47b8-ac34-6c105a97df27",
   "metadata": {},
   "source": [
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g. 0.999999) when using an SGD optimizer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428347ee-ff21-4287-ae20-05fa65dc6220",
   "metadata": {},
   "source": [
    "*My answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ae193-1aca-47e4-8f38-2d3d01b452f2",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99004f61-d90c-492b-8781-4aeefffbb7bd",
   "metadata": {},
   "source": [
    "**6. Name three ways you can produce a sparse model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0188df7-6d7b-4213-afa1-b58cffb14410",
   "metadata": {},
   "source": [
    "*My answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefdf63-0d45-4d7c-b6b6-a2679797897e",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163b58c-444b-4b07-ba96-9cf6d0fe2fd6",
   "metadata": {},
   "source": [
    "**7. Does dropout slow down training? Does it slow down inference (i.e. making predictions on new instances)? What about MC Dropout?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0dd0e-c3e1-40af-a101-3bffaa737db7",
   "metadata": {},
   "source": [
    "*My answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366bbca5-1aec-475a-9893-240eeeda56f4",
   "metadata": {},
   "source": [
    "*Book answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5878877-23cf-4553-8c4a-be686a32dd09",
   "metadata": {},
   "source": [
    "**8. Practice training a deep neural network on the CIFAR10 image dataset:**\n",
    "1. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but its the point of this exercise). Use He initialization and the ELU activation function.\n",
    "2. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32x32 pixel color images (50k for training, 10k for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. **Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.**\n",
    "3. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "4. Try replacing Batch Normalization with SELU, an dmake the necessary adjustments to ensure the network self-normalizes (i.e. standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)\n",
    "5. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "6. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2f5de-6063-46fc-ac6e-9a155cc70f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18683e6-0597-450d-92ea-7460e0f23ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1bdcc-d262-4cb3-8bb6-b437cb9f26ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70341488-18ec-43d4-9061-02cf5ad43b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02372bb5-80bf-44b6-9a6a-f0b8e0e31bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037dc19-d56e-4c24-bbe4-1edd0c166b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9732d0-6e99-4403-9a1a-c79b9908a8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
